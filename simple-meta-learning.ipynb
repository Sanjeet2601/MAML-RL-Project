{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ba1ed61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 21:40:44.864073: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-12 21:40:44.864118: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-12 21:40:44.865725: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-12 21:40:44.873678: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-12 21:40:45.860137: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pywt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import talib\n",
    "from sklearn.metrics import f1_score\n",
    "import seaborn as sns\n",
    "import copy\n",
    "from torch.autograd import grad\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# writer = SummaryWriter('runs/port_opt_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08128f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install yfinance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3f38e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_data(index,start,end,freq):\n",
    "    data = yf.Ticker(index)\n",
    "    dat = data.history(interval = freq,start = start,end = end)\n",
    "    # snp.index = snp.index.map(lambda x: str(x).split(' ')[0])\n",
    "    dat['return'] = dat['Close'].pct_change().apply(lambda x:100*x)\n",
    "    dat =dat.dropna()\n",
    "    return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f944c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide_data('^GSPC','2001-01-01','2023-01-01','1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee329d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(index):\n",
    "    start,end,fre = '2001-01-01','2023-01-01','1d'\n",
    "    # snp = provide_data('RELIANCE.NS',start,end,fre)\n",
    "    snp = provide_data(index,start,end,fre)\n",
    "    snp_futures = provide_data('YM=F',start,end,fre)\n",
    "    def extract_macd(stock_prices):\n",
    "        macd, macdsignal, macdhist = talib.MACD(stock_prices, fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "        return macd,macdsignal\n",
    "    \n",
    "    res1,res2 = extract_macd(np.array(snp['Close'].values))\n",
    "    snp['Macd Signal'] = res2\n",
    "    snp['Macd'] = res1\n",
    "    # snp['Macd diff'] = res1-res2\n",
    "    snp['Simple MA'] = talib.SMA(snp['Close'],14)\n",
    "    snp['EMA'] = talib.EMA(snp['Close'], timeperiod = 14)\n",
    "    snp['upper_band'], snp['middle_band'], snp['lower_band'] = talib.BBANDS(snp['Close'], timeperiod =20)\n",
    "    snp['RSI'] = talib.RSI(snp['Close'],14) \n",
    "    snp['slowk'], snp['slowd'] = talib.STOCH(snp['High'], snp['Low'], snp['Close'], fastk_period=14, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0) \n",
    "    snp['diff_stoch'] = snp['slowk']-snp['slowd']\n",
    "    \n",
    "    # snp[['Close','Simple MA','EMA']][:300].plot(figsize=(15,15)),plt.show()\n",
    "    # snp[['Macd','Macd Signal']][:300].plot(figsize = (15,15)),plt.show()\n",
    "    # plt.figure(figsize=(15,15))\n",
    "    # plt.plot(res1[:300]-res2[:300])\n",
    "    # plt.show()\n",
    "    # snp[['Close','upper_band','middle_band','lower_band']][:300].plot(figsize=(15,15)),plt.show()\n",
    "    # snp['RSI'][:300].plot(figsize=(15,15)),plt.show()\n",
    "    # snp[['slowk','slowd']][:300].plot(figsize=(15,15)),plt.show()\n",
    "    \n",
    "    # snp.to_csv('stock_data.csv')\n",
    "    \n",
    "    cut_len = len(snp['Close'])\n",
    "    for col in snp.columns: \n",
    "        nan_indices = np.where(np.isnan(snp[col]))[0]\n",
    "    #     print(nan_indices)\n",
    "        # Remove NaN values from the array\n",
    "        arr_no_nan = np.delete(snp[col], nan_indices)\n",
    "        cut_len = int(min(cut_len,arr_no_nan.shape[0]))\n",
    "    cut_len\n",
    "    \n",
    "    snp = snp.drop(snp.index[range(len(snp['Close'])-cut_len)])\n",
    "    # print(snp.index[0],type(snp.index[0]))\n",
    "    remove_col = ['Volume','Dividends', 'Stock Splits','return']\n",
    "    col_len = len(snp.columns)-len(remove_col)\n",
    "    \n",
    "    snp = snp.drop(columns=remove_col)\n",
    "    snp\n",
    "    # autocorrelation_plot(snp['Close'])\n",
    "    result = adfuller(snp['Close'])\n",
    "    plt.plot(snp['Close'].values),plt.show()\n",
    "    print(result)\n",
    "    \n",
    "    \n",
    "    level=1\n",
    "    data_wt = snp['Close'][:int(pow(2,level))*int(len(snp['Close'])/int(pow(2,level)))]\n",
    "    cc = pywt.swt(data_wt,'db5',level = level,norm=True)\n",
    "    print(len(cc[0][1]),len(cc[0][0]),len(cc))\n",
    "    df = snp.iloc[-len(data_wt)+200:]\n",
    "    # print(df['Close'])\n",
    "    snp = df.copy()\n",
    "    snp['swt0'] = cc[0][0][200:]\n",
    "    \n",
    "    # snp['swt1'] = cc[1][0][200:]\n",
    "    # snp['swt2'] = cc[2][0][200:]\n",
    "    # snp['swt3'] = cc[3][0][200:]\n",
    "    # snp['swt4'] = cc[4][0][200:]\n",
    "    \n",
    "    snp['hf0'] = cc[0][1][200:]\n",
    "    \n",
    "    # snp['hf1'] = cc[1][1][200:]\n",
    "    # snp['hf2'] = cc[2][1][200:]\n",
    "    # snp['hf3'] = cc[3][1][200:]\n",
    "    # snp['hf4'] = cc[4][1][200:]\n",
    "    # snp['Close'] = cc[0][0][200:]a\n",
    "    \n",
    "    # print(snp['Close'],snp['swt'])\n",
    "    # plt.plot(df['Close'].values),plt.show()\n",
    "    # plt.plot(snp['Close']),plt.show()\n",
    "    snp = snp.drop(columns=['Open','High','Low'])\n",
    "    # print(snp.columns)\n",
    "    return snp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2097194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_data('IXIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c74d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_steps1 = 100\n",
    "t_steps2 = 10\n",
    "predict_steps = 1\n",
    "extra_days = 0\n",
    "batch_size = 32\n",
    "pd1 = 0.2\n",
    "pu1 = 1.2\n",
    "# pd1 = -1.1\n",
    "# pd2 = -0.3\n",
    "# pu1 = 0.2\n",
    "# pu2 = 0.7\n",
    "# pu3 = 1.3\n",
    "classes = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8512564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(\"matrix.txt\", np.array2string(, precision=2), delimiter=\"  \")\n",
    "# formatter = {'float': lambda x: f'{x:.2f}'}\n",
    "# print(np.array2string(np.corrcoef([np.array(snp[col].values) for col in snp.columns]), formatter=formatter, suppress_small=True))\n",
    "def data_transform(data,t_steps):    \n",
    "    def temp_function(data):\n",
    "#         return None\n",
    "        temp_data_list = []\n",
    "        for i in range(0,len(data)-(t_steps+predict_steps+extra_days)):\n",
    "            mi,mx = np.min(data[i:i+t_steps]),np.max(data[i:i+t_steps])\n",
    "            temp_data_list.append((data[i:i+t_steps]-mi)/(mx-mi))\n",
    "#             temp_data_list.append(data[i:i+t_steps])\n",
    "        temp_arr = np.array(temp_data_list,dtype = np.float32)\n",
    "        return temp_arr.reshape(temp_arr.shape[0],temp_arr.shape[1],-1)\n",
    "    dynamic_feat_list = [temp_function(data[col]) for col in data.columns[:] if col!='Close']\n",
    "    \n",
    "#     print(len(dynamic_feat_list))            \n",
    "    data_C = np.asarray(data['Close'],dtype = np.float32)\n",
    "    target_data_list = []\n",
    "    data_pred = []\n",
    "    labels = []\n",
    "    pst_arr = []\n",
    "    for i in range(0,len(data)-(t_steps+predict_steps+extra_days),1):\n",
    "        data_past = data_C[i:i+t_steps]\n",
    "#         print(data_C[i+t_steps])\n",
    "        # pred = np.mean(data_C[i+t_steps+extra_days:i+t_steps+extra_days+predict_steps])\n",
    "        pred = data_C[i+t_steps+predict_steps-1]\n",
    "#         print(pred)\n",
    "        data_pred.append(list([pred]))\n",
    "        std_past = np.std(data_past)\n",
    "#         past = np.mean(data_past)\n",
    "        past = data_past[-1]\n",
    "        pst_arr.append(past)\n",
    "#         pst_arr.append(data['swt'][i:i+t_steps][-1])\n",
    "#         if pred<pd1:\n",
    "#             id=0\n",
    "#         elif pred<pd2:\n",
    "#             id=1\n",
    "#         elif pred<=pu1:\n",
    "#             id=2\n",
    "#         elif pred<pu2:\n",
    "#             id=3\n",
    "#         elif pred<=pu3:\n",
    "#             id=4\n",
    "#         else:\n",
    "#             id=5\n",
    "        if pred<past:\n",
    "            id=0\n",
    "        else:\n",
    "            id=1\n",
    "        label = np.zeros((classes))\n",
    "        label[id]=1\n",
    "        labels.append(label)\n",
    "        mi,mx = np.min(data_past),np.max(data_past)\n",
    "        target_data_list.append((data_past-mi)/(mx-mi))\n",
    "#         target_data_list.append(data_past)\n",
    "    target_data_arr = np.array(target_data_list,dtype=np.float32)\n",
    "    target_data_arr = target_data_arr.reshape(target_data_arr.shape[0],target_data_arr.shape[1],-1)\n",
    "    final_dynamic_feat = np.concatenate(dynamic_feat_list,axis=2)\n",
    "#     print(final_dynamic_feat)\n",
    "    final_dataset = np.concatenate((final_dynamic_feat,target_data_arr),axis=2)\n",
    "    return final_dataset,np.array(data_pred),labels,np.array(pst_arr)\n",
    "\n",
    "# dat_tf,df_pr,dt_labels,df_past = data_transform(snp,snp_futures,t_steps1)\n",
    "# print(dat_tf.shape,dat_tf,len(dt_labels),np.count_nonzero(np.isnan(dat_tf)))\n",
    "# print(input_long.shape)\n",
    "\n",
    "# cnt = [0]*classes\n",
    "# for i in dt_labels:\n",
    "#     for j in range(classes):\n",
    "#         cnt[j]+=i[j]\n",
    "\n",
    "# cnt = cnt/sum(cnt)\n",
    "# # print(dt_labels)\n",
    "# print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33d9cfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class stock_dataset(Dataset):\n",
    "    def __init__(self,data,t_steps1):\n",
    "#         print(len(data),len(data2))\n",
    "        self.dat_long,self.pred_long,self.label_long,self.past_arr = data_transform(data,t_steps1)\n",
    "\n",
    "#         self.dat_short,self.pred_short,self.label_short = data_transform(data,data2,t_steps2)\n",
    "    def __len__(self):\n",
    "        return len(self.dat_long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.dat_long[idx][:t_steps1],self.label_long[idx],self.pred_long[idx],self.past_arr[idx]\n",
    "def prepare_dataloader(train_dataset,test_dataset):\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,batch_size = batch_size,shuffle = True,drop_last = True)\n",
    "    test_dataloader = DataLoader(test_dataset,batch_size = 1,shuffle = False,drop_last = True)\n",
    "    \n",
    "    # print(train_dataset[2],len(snp))\n",
    "    # print(test_dataset[-1])\n",
    "    return train_dataloader,test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9728f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_mat1():\n",
    "    y_pred,y_true = [],[]\n",
    "    with torch.no_grad():\n",
    "        for i,data in enumerate(test_dataloader):\n",
    "            inp,labels,pred,past = data[0],data[1],data[2],data[3]\n",
    "            out = model(inp.repeat(batch_size,1,1))[0]\n",
    "            temp = [0]*classes\n",
    "            y_pred.append(torch.argmax(out))\n",
    "            y_true.append(torch.argmax(labels))\n",
    "        \n",
    "        print(len(y_true),len(y_pred))\n",
    "        print((len(y_pred)-sum([abs(x-y) for x,y in zip(y_true,y_pred)]))/len(y_pred))\n",
    "        conf_mat = confusion_matrix(y_true,y_pred,normalize='pred')\n",
    "        print(conf_mat)\n",
    "        # print(np.trace(np.array(conf_mat))/classes)\n",
    "        print(f1_score(y_true,y_pred))\n",
    "def confusion_mat2():\n",
    "    y_pred,y_true = [],[]\n",
    "    with torch.no_grad():\n",
    "        for i,data in enumerate(test_dataloader):\n",
    "            inp,labels,pred = data[0],data[1],data[2]\n",
    "            # out = model(inp.repeat(batch_size,1,1))[0]\n",
    "            encoder_outputs, (hidden, cell) = encoder(inp.repeat(1,1,1))\n",
    "          # input = trg[:-1]\n",
    "            out,reg,wts = decoder( hidden, cell, encoder_outputs)\n",
    "            temp = [0]*classes\n",
    "#             temp[torch.argmax(out)]=1\n",
    "            y_pred.append(torch.argmax(out))\n",
    "            y_true.append(torch.argmax(labels))\n",
    "            if i%100==0:\n",
    "                # print(wts)\n",
    "                sns.heatmap(wts[0])\n",
    "                plt.show()\n",
    "        y_true,y_pred = np.array(y_true),np.array(y_pred)\n",
    "\n",
    "        print(len(y_true),len(y_pred))\n",
    "        print((len(y_pred)-sum([abs(x-y) for x,y in zip(y_true,y_pred)]))/len(y_pred))\n",
    "        conf_mat = confusion_matrix(y_true,y_pred,normalize='pred')\n",
    "        print(conf_mat)\n",
    "        # print(np.trace(np.array(conf_mat))/classes)\n",
    "        print(f1_score(y_true,y_pred))\n",
    "        \n",
    "class stock_lstm2(torch.nn.Module):\n",
    "    def __init__(self,input_dim,output_dim,batch_size,t_steps1,t_steps2,classes):\n",
    "        super(stock_lstm,self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.in_dim = input_dim\n",
    "        self.out_dim = output_dim\n",
    "        self.nlayers = 6\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size = input_dim,hidden_size = output_dim,num_layers = self.nlayers,batch_first = True)\n",
    "        self.lstm2 = nn.LSTM(input_size = input_dim,hidden_size = output_dim,num_layers = self.nlayers,batch_first = True)\n",
    "\n",
    "        self.ln_long = nn.LayerNorm((t_steps1,self.in_dim))\n",
    "        self.ln_short = nn.LayerNorm((t_steps2,self.in_dim))\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.soft = nn.Softmax()\n",
    "\n",
    "        self.fc1_long = nn.Linear(t_steps*self.in_dim,t_steps*self.in_dim//2)\n",
    "        self.fc2_long = nn.Linear(t_steps*self.in_dim//2,classes)\n",
    "        self.fc1_short = nn.Linear(t_steps2*self.in_dim,t_steps2*self.in_dim//2)\n",
    "        self.fc2_short = nn.Linear(t_steps2*self.in_dim//2,classes)\n",
    "        self.fc3 = nn.Linear(2*classes,classes)\n",
    "\n",
    "    def forward(self,input_long,input_short):\n",
    "        input_long = input_long.view(self.batch_size,-1,self.in_dim)\n",
    "        input_short = input_short.view(self.batch_size,-1,self.in_dim)\n",
    "        h_0_long = input_long[:][:,0].view(1,self.batch_size,-1).repeat(self.nlayers,1,1)\n",
    "        h_0_short = input_short[:][:,0].view(1,self.batch_size,-1).repeat(self.nlayers,1,1)\n",
    "        input_long = self.ln_long(input_long)\n",
    "        input_short = self.ln_short(input_short)\n",
    "\n",
    "#         lstm_out,_ = self.lstm1(input_series,(h_0,torch.zeros(h_0.shape)))\n",
    "        lstm_long,_ = self.lstm1(input_long)\n",
    "        lstm_long = self.ln_long(lstm_long)\n",
    "        lstm_long = self.tanh(lstm_long)\n",
    "        lstm_long = lstm_long.view(self.batch_size,-1)\n",
    "\n",
    "        lstm_short,_ = self.lstm1(input_short)\n",
    "        lstm_short = self.ln_short(lstm_short)\n",
    "        lstm_short = self.tanh(lstm_short)\n",
    "        lstm_short = lstm_short.view(self.batch_size,-1)\n",
    "\n",
    "        out_long = self.fc1_long(lstm_long)\n",
    "        out_long = self.sig(out_long)\n",
    "        out_long = self.fc2_long(out_long)\n",
    "        out_long = self.soft(out_long)\n",
    "\n",
    "        out_short = self.fc1(lstm_short)\n",
    "        out_short = self.sig(out_short)\n",
    "        out_short = self.fc2(out_short)\n",
    "        out_short = self.soft(out_short)\n",
    "\n",
    "        out = self.fc3(np.hstack(out_long,out_short))\n",
    "        out = self.soft(out)\n",
    "#         print(input_series[0],lstm_out[0],out_pre[0])\n",
    "#         out_final = self.act2(out_pre)\n",
    "#         out_final = self.fc2(out_pre)\n",
    "        return out\n",
    "class stock_lstm(torch.nn.Module):\n",
    "    def __init__(self,input_dim,d_model,n_layers,batch_size,t_steps,classes):\n",
    "        super(stock_lstm,self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.in_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.nlayers = n_layers\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size = input_dim,hidden_size = d_model,num_layers = self.nlayers,batch_first = True)\n",
    "\n",
    "        # self.input_ln = nn.LayerNorm((t_steps,self.in_dim))\n",
    "        self.lstm_ln = nn.LayerNorm((t_steps,self.d_model))\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.soft = nn.Softmax()\n",
    "\n",
    "        self.fc1_long = nn.Linear(t_steps*self.d_model,t_steps*self.d_model//2)\n",
    "        self.fc2_long = nn.Linear(t_steps*self.d_model//2,t_steps*self.d_model//4)\n",
    "        self.fc3_long = nn.Linear(t_steps*self.d_model//4,classes)\n",
    "\n",
    "    def forward(self,input_long):\n",
    "#         print(input_long.shape,input_long[0])\n",
    "#         input_long = np.transpose(input_long,(0,2,1))\n",
    "#         h_0_long = input_long[:][:,0].view(1,self.batch_size,-1).repeat(self.nlayers,1,1)\n",
    "#         input_long = self.input_ln(input_long)\n",
    "#         print(torch.mean(input_long,dim=(1,2)))\n",
    "        lstm_long,_ = self.lstm1(input_long)\n",
    "        lstm_long = self.lstm_ln(lstm_long)\n",
    "        lstm_long = self.tanh(lstm_long)\n",
    "        lstm_long = lstm_long.reshape(lstm_long.shape[0],-1)\n",
    "\n",
    "        out_long = self.fc1_long(lstm_long)\n",
    "        out_long = self.sig(out_long)\n",
    "        out_long = self.fc2_long(out_long)\n",
    "        # out_long = self.soft(out_long)\n",
    "        out_long = self.sig(out_long)\n",
    "        out_long = self.fc3_long(out_long)\n",
    "        out  = self.soft(out_long)\n",
    "\n",
    "        return out\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, t_steps):\n",
    "        super().__init__()\n",
    "        self.d_model = hid_dim\n",
    "        self.t_steps= t_steps\n",
    "        self.input_dim = input_dim\n",
    "        self.nlayers = n_layers\n",
    "        self.rnn = nn.LSTM(input_dim, hid_dim, n_layers, dropout=0.5,batch_first = True)\n",
    "        self.rnncell = nn.LSTMCell(input_dim,hid_dim)\n",
    "#         self.rnn2 = nn.LSTM(input_dim,hid_dim,n_layers,dropout=0.2,batch_first=True)\n",
    "        self.ln = nn.LayerNorm((t_steps,input_dim))\n",
    "        self.lno = nn.LayerNorm((t_steps,hid_dim))\n",
    "        self.Q = nn.Linear(hid_dim,hid_dim)\n",
    "        self.K = nn.Linear(hid_dim,hid_dim)\n",
    "        self.Q2 = nn.Linear(input_dim,input_dim)\n",
    "        self.K2 = nn.Linear(hid_dim,input_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "    def feature_attn(self,inp,h):\n",
    "        inp = inp[:,None,:]\n",
    "        h = h[:,None,:]\n",
    "        Q2 = inp\n",
    "        K2 = self.K2(h)\n",
    "        \n",
    "        wts = F.softmax(torch.matmul(torch.transpose(Q2,1,2),K2),dim=-1)\n",
    "#         print(Q2.shape,K2.shape,wts.shape)\n",
    "        out = torch.matmul(inp,wts.transpose(1,2))\n",
    "        out = out[:,0,:]\n",
    "#         print(out.shape)\n",
    "        return out,wts\n",
    "    def attn(self,Q,K):\n",
    "        # Q = self.Q(Qs)\n",
    "        # K = self.K(Ks)\n",
    "        wts = F.softmax(torch.matmul(Q,torch.transpose(K,1,2)),dim=-1)\n",
    "        output = torch.matmul(wts,Q)\n",
    "        return output,wts\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # inp_mi = torch.tensor(torch.min(src,dim=1)[0])[:,None,:]\n",
    "        # inp_mx = torch.tensor(torch.max(src,dim=1)[0])[:,None,:]\n",
    "        # src = (src -inp_mi)/(inp_mx-inp_mi)\n",
    "        # src = self.ln(src)\n",
    "#         h0 = np.sum(src[:,0,:],axis=)\n",
    "        \n",
    "#         h0 = torch.zeros((src.shape[0],self.d_model))\n",
    "#         c0=torch.zeros((src.shape[0],self.d_model))\n",
    "#         output = []\n",
    "#         # cells = []\n",
    "#         for i in range(self.t_steps):\n",
    "#             inp = src[:,i,:]\n",
    "#             inp,wtsf = self.feature_attn(inp,h0)\n",
    "# #             inp = self.ln(inp)\n",
    "#             h0,c0 = self.rnncell(inp,(h0,c0))\n",
    "#             h0,c0 = self.relu(h0),self.relu(c0)\n",
    "# #             print(h0.shape)\n",
    "#             output.append(h0)\n",
    "#             # cells.append(c0)\n",
    "#         output = torch.stack(output,dim=1)\n",
    "#         # cells = torch.stack(cells,dim=1)\n",
    "#         output,wts = self.attn(output,output)\n",
    "#         output = self.lno(output)\n",
    "#         # cells,wts = self.attn(cells,output)\n",
    "#         hidden,cell = output[:,-1,:],c0\n",
    "        # return outputs,(hidden, cell),wtsf\n",
    "        \n",
    "#         hidden,cell = h0[:,None,:],c0[:,None,:]\n",
    "\n",
    "\n",
    "        # Q2 = self.Q2(src)\n",
    "        # K2 = self.K2(src)\n",
    "        # wts2 = F.softmax(torch.matmul(torch.transpose(Q2,1,2),K2),dim=-1)\n",
    "        # src2 = torch.matmul(wts2,src.transpose(1,2)).transpose(1,2)\n",
    "        # src2 = feature_attn(src,)\n",
    "        # src2 = self.ln(src2)\n",
    "        outputs, (hidden, cell) = self.rnn(src)\n",
    "        outputs = self.tanh(outputs)\n",
    "        hidden = hidden.transpose(0,1)\n",
    "        cell = cell.transpose(0,1)\n",
    "        hidden = hidden[:,0,:]\n",
    "        cell = cell[:,0,:]\n",
    "        # Q = self.Q(outputs)\n",
    "        # K = self.K(outputs)\n",
    "        # wts = F.softmax(torch.matmul(Q,torch.transpose(K,1,2)),dim=-1)\n",
    "        # output = torch.matmul(wts,outputs)\n",
    "\n",
    "#         print(output.shape,wts.shape,hidden.shape)\n",
    "        return outputs,(hidden, cell)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim,hid_dim, n_layers, p_steps,t_steps):\n",
    "        super().__init__()\n",
    "        # self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.psteps = p_steps\n",
    "#         self.rnn = nn.LSTM(hid_dim , hid_dim, n_layers, dropout=0.5)\n",
    "        self.ln = nn.LayerNorm((t_steps,hid_dim))\n",
    "        self.rnncell = nn.LSTMCell(hid_dim,hid_dim)\n",
    "        self.out = nn.Linear(hid_dim, output_dim)\n",
    "        self.Qc = nn.Linear(hid_dim,hid_dim)\n",
    "        self.Kc = nn.Linear(hid_dim,hid_dim)\n",
    "        self.Qs = nn.Linear(hid_dim,hid_dim)\n",
    "        self.Ks = nn.Linear(hid_dim,hid_dim)\n",
    "        # self.fc1 = nn.Linear(n_layers*hid_dim,hid_dim)\n",
    "        self.fc1 = nn.Linear(hid_dim,output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.soft = nn.Softmax()\n",
    "        self.logsoft = nn.LogSoftmax()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(hid_dim,hid_dim//2)\n",
    "        self.fc3 = nn.Linear(hid_dim//2,output_dim)\n",
    "        self.rfc = nn.Linear(hid_dim,hid_dim//2)\n",
    "        self.rfc2 = nn.Linear(hid_dim//2,hid_dim//4)\n",
    "        self.rfc3 = nn.Linear(hid_dim//4,1)\n",
    "    def cross_attn(self,Qs,Ks):\n",
    "        if len(Ks.shape)<3:\n",
    "            Ks = Ks[:,None,:]\n",
    "        Q = Qs\n",
    "        K = Ks\n",
    "        wts = F.softmax(torch.matmul(Q,torch.transpose(K,1,2)),dim=-2)\n",
    "        output = torch.matmul(wts.transpose(1,2),Qs)\n",
    "        output = output[:,0,:]\n",
    "        return output,wts\n",
    "    def self_attn(self,Qs,Ks):\n",
    "        if len(Ks.shape)<3:\n",
    "            Ks = Ks[:,None,:]\n",
    "        Q = Qs\n",
    "        K = Ks\n",
    "        wts = F.softmax(torch.matmul(Q,torch.transpose(K,1,2)),dim=-2)\n",
    "        output = torch.matmul(wts.transpose(1,2),Qs)\n",
    "        output = output[:,0,:]\n",
    "        return output,wts\n",
    "    def forward(self, hidden, cell, encoder_outputs):\n",
    "        # print(encoder_outputs.shape,hidden.transpose(1,0).shape)\n",
    "        # query = self.Q(encoder_outputs.reshape(encoder_outputs.shape[0],-1))\n",
    "        # query = query.reshape(query.shape[0],-1,query.shape[1]).transpose(2,1)\n",
    "        # query = query.transpose(2,1)\n",
    "#         hidden = hidden.transpose(1,0)\n",
    "#         scores = torch.matmul(hidden,query)\n",
    "#         attention_weights = torch.softmax(scores,dim=1)\n",
    "#         value = torch.mul(hidden,attention_weights)\n",
    "#         out = self.fc1(value.reshape(value.shape[0],-1))\n",
    "#         prediction = self.soft(out)\n",
    "#         print(encoder_outputs.shape,hidden.transpose(1,0).shape)\n",
    "#         encoder_outputs = self.Q(encoder_outputs)\n",
    "#         scores = torch.matmul(hidden.transpose(1,0),encoder_outputs.transpose(2,1))\n",
    "#         attention_weights = torch.softmax(scores,dim=-1)\n",
    "# #         print(scores.shape,attention_weights.shape,attention_weights)\n",
    "#         value = torch.matmul(attention_weights,encoder_outputs)\n",
    "#         value = self.V(value)\n",
    "#         value = value.reshape(value.shape[0],-1)\n",
    "#         out = self.sig(value)\n",
    "#         out = self.fc1(out)\n",
    "#         out = self.sig(out)\n",
    "#         out = self.fc2(out)\n",
    "#         prediction = self.soft(out)\n",
    "\n",
    "        \n",
    "        encoder_outputs = encoder_outputs\n",
    "        h0,c0 = hidden,cell\n",
    "        a1,wts = self.cross_attn(encoder_outputs,h0)\n",
    "        \n",
    "        # h0 = h0[:,0,:]\n",
    "        # c0 = c0[:,0,:]\n",
    "        # print(h0.shape)\n",
    "        h0,c0 = self.rnncell(h0,(a1,c0))\n",
    "        h0,c0 = self.tanh(h0),self.tanh(c0)\n",
    "        hiddens = h0[:,None,:]\n",
    "        for i in range(self.psteps-1):\n",
    "            a1,_ = self.cross_attn(encoder_outputs,h0)\n",
    "            a2,_ = self.cross_attn(hiddens,h0)\n",
    "            h0,c0 = self.rnncell(h0,(a1+a2,c0))\n",
    "            h0,c0 = self.relu(h0),self.relu(c0)\n",
    "            hiddens = torch.cat((hiddens,h0[:,None,:]),1)\n",
    "        out = self.fc1(h0)\n",
    "        # out = self.sig(out)\n",
    "        # out = self.fc2(out)\n",
    "        # out = self.sig(out)\n",
    "        # out = self.fc3(out)\n",
    "        prediction = self.soft(out)\n",
    "        reg = self.rfc(h0)\n",
    "        reg = self.sig(reg)\n",
    "        reg = self.rfc2(reg)\n",
    "        reg = self.sig(reg)\n",
    "        reg = self.rfc3(reg)\n",
    "        reg = self.tanh(reg)\n",
    "        \n",
    "        return prediction,reg,wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17c29fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_steps1 = 20\n",
    "predict_steps = 1\n",
    "batch_size = 64\n",
    "classes = 2\n",
    "stock = '^IRX'\n",
    "\n",
    "# import gspc_train_20_5.pt\n",
    "def fetch(stock,split,t_steps,p_steps):\n",
    "    file = '{}_{}_{}_{}.pt'.format(stock,split,t_steps,p_steps)\n",
    "    return torch.load(file)\n",
    "def prepare_task(stock,t_steps1,predict_steps,classes):\n",
    "    try:\n",
    "        nme = re.sub(r'[^a-zA-Z]', '', stock).lower()\n",
    "        train_dataset = fetch(nme,'train',t_steps1,predict_steps)\n",
    "        test_dataset = fetch(nme,'test',t_steps1,predict_steps)\n",
    "        \n",
    "    except:\n",
    "        print('file not found . starting to prepare')\n",
    "        index = prepare_data(stock)\n",
    "        train_len = int(0.85*len(index))\n",
    "        test_len = len(index)-train_len\n",
    "        \n",
    "        train_dataset = stock_dataset(index[:train_len],t_steps1)\n",
    "        test_dataset = stock_dataset(index[-(len(index)-train_len):],t_steps1)\n",
    "        nme = re.sub(r'[^a-zA-Z]', '', stock).lower()\n",
    "        torch.save(train_dataset,'./{}_train_{}_{}.pt'.format(nme,t_steps1,predict_steps))\n",
    "        torch.save(test_dataset,'./{}_test_{}_{}.pt'.format(nme,t_steps1,predict_steps))\n",
    "    in_dim = train_dataset.dat_long.shape[2]\n",
    "    out_dim = in_dim\n",
    "    cnt = [0]*classes\n",
    "    for i in train_dataset.label_long:\n",
    "        for j in range(classes):\n",
    "            cnt[j]+=i[j]\n",
    "    cnt/=sum(cnt)\n",
    "    print(cnt,'cnt')\n",
    "    \n",
    "    train_dataloader,test_dataloader = prepare_dataloader(train_dataset,test_dataset)\n",
    "    return train_dataloader,test_dataloader,cnt\n",
    "# print(cnt)\n",
    "# confusion_mat2()\n",
    "# confusion_mat1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f233d5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.45896521 0.54103479] cnt\n",
      "[0.42563644 0.57436356] cnt\n",
      "[0.45606601 0.54393399] cnt\n",
      "[0.46742005 0.53257995] cnt\n",
      "[0.4775419 0.5224581] cnt\n"
     ]
    }
   ],
   "source": [
    "task1 = prepare_task('^GSPC',t_steps1,predict_steps,classes)\n",
    "task2 = prepare_task('^IRX',t_steps1,predict_steps,classes)\n",
    "task3 = prepare_task('^IXIC',t_steps1,predict_steps,classes)\n",
    "task4 = prepare_task('DJI',t_steps1,predict_steps,classes)\n",
    "task5 = prepare_task('CL=F',t_steps1,predict_steps,classes)\n",
    "in_dim = task1[0].dataset.dat_long.shape[2]\n",
    "out_dim=in_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62bdc656",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:48:40.295572Z",
     "iopub.status.busy": "2024-03-12T09:48:40.295328Z",
     "iopub.status.idle": "2024-03-12T09:48:40.299795Z",
     "shell.execute_reply": "2024-03-12T09:48:40.298901Z",
     "shell.execute_reply.started": "2024-03-12T09:48:40.295552Z"
    }
   },
   "outputs": [],
   "source": [
    "# in_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "644fcd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_layers = 2\n",
    "d_model = 32\n",
    "encoder = Encoder(in_dim, d_model, n_layers,t_steps1)\n",
    "decoder = Decoder(classes, d_model, n_layers, predict_steps,t_steps1)\n",
    "\n",
    "# Define the optimizer and criterion\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "400474de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext tensorboard\n",
    "# dataiter = iter(train_dataloader)\n",
    "# src,trg,pred,past = next(dataiter)\n",
    "# print(src,src.shape)\n",
    "# out,(hid,cel) = encoder(src)\n",
    "# writer.add_graph(encoder,src)\n",
    "# writer.add_graph(decoder,(hid,cel,out))\n",
    "# writer.close()\n",
    "# writer.flush()\n",
    " \n",
    "# %tensorboard --logdir=runs\n",
    "# next(iter(task1[0]))\n",
    "# for k,p in model.named_parameters():\n",
    "#     print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46158e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 2\n",
    "d_model = 16\n",
    "model = stock_lstm(in_dim,d_model,n_layers,batch_size,t_steps1,classes)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5747a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chkpt = torch.load('stock_lstm_meta_12345_0shots.pt')\n",
    "# model.load_state_dict(chkpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92b2abe6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def diffout():\n",
    "def model_test(test_dataloader,model):\n",
    "    y_pred,y_true = [],[]\n",
    "    for i,test_data in enumerate(test_dataloader):\n",
    "        src=test_data[0]\n",
    "        trg = test_data[1]\n",
    "        pred=test_data[2][:,0]\n",
    "        past=test_data[3]\n",
    "        output = model(src)\n",
    "#         if i%50==0:\n",
    "#             print(criterion1(output,trg))\n",
    "        y_pred.append(torch.argmax(output))\n",
    "        y_true.append(torch.argmax(trg))\n",
    "    print(f1_score(y_true,y_pred))\n",
    "    \n",
    "def meta_test(model,task_list,n_shots=0):\n",
    "    \n",
    "    for task in task_list:\n",
    "        criterion1 = nn.CrossEntropyLoss(weight = torch.tensor(task[2]))\n",
    "        tmodel = copy.deepcopy(model)\n",
    "        optimizer = torch.optim.Adam(tmodel.parameters())\n",
    "        test_dataloader =  task[1]\n",
    "        itr = iter(test_dataloader)\n",
    "        for _ in range(n_shots):\n",
    "            test_data = next(itr)\n",
    "            src=test_data[0]\n",
    "            trg = test_data[1]\n",
    "            pred=test_data[2][:,0]\n",
    "            past=test_data[3]\n",
    "            output = tmodel(src)\n",
    "            loss = criterion1(output,trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        tmodel.eval()\n",
    "        model_test(test_dataloader,tmodel)\n",
    "\n",
    "\n",
    "def meta_train(epochs,task_list,n_shots=0):\n",
    "    loss_list = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()))\n",
    "    # optimizer = torch.optim.Adam(model.parameters())\n",
    "    # criterion1 = nn.NLLLoss(weight=torch.tensor(cnt))\n",
    "    # criterion2 = nn.MSELoss()\n",
    "    matches,length = 0,0\n",
    "    inner_lr = 0.01\n",
    "    meta_lr=0.001\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        grads = []\n",
    "        for task in task_list:\n",
    "            \n",
    "            train_dataloader = task[0]\n",
    "            criterion1 = nn.CrossEntropyLoss(weight = torch.tensor(task[2]))\n",
    "            itr = iter(train_dataloader)\n",
    "            fmodel = stock_lstm(in_dim,d_model,n_layers,batch_size,t_steps1,classes)\n",
    "            fmodel.load_state_dict(model.state_dict())\n",
    "            fast_wts = dict((name,param) for (name,param) in fmodel.named_parameters())\n",
    "            loss=0\n",
    "            # print('fast_wts',fast_wts[0])\n",
    "            for _ in range(n_shots):\n",
    "                print('ho gaya')\n",
    "                train_data = next(itr)\n",
    "                src = train_data[0]\n",
    "                trg = train_data[1]\n",
    "                pred = train_data[2][:,0]\n",
    "                past = train_data[3]\n",
    "                \n",
    "                # encoder_outputs, (hidden, cell) = fenc(src)\n",
    "                # output,reg,wts = fdec( hidden, cell, encoder_outputs)\n",
    "                output = fmodel(src)\n",
    "                # reg = reg[:,0]\n",
    "                ls = criterion1(output,trg)\n",
    "                gd = grad(ls,fmodel.parameters())\n",
    "                # foptimizer.step()\n",
    "                fast_wts = dict((name, param - inner_lr * g) for ((name, param), g) in zip(fast_wts.items(), gd))\n",
    "                fmodel.load_state_dict(fast_wts)\n",
    "\n",
    "            # iid=0\n",
    "            # print(len(fast_wts))\n",
    "            # for i,p in enumerate(fmodel.parameters()):\n",
    "            #     p=fast_wts[i]\n",
    "            #     # sz = np.prod(list(p.shape))\n",
    "            #     # # print(iid,sz)\n",
    "            #     # print('fast weights',fast_wts[iid:iid+sz])\n",
    "            #     # p.data = torch.tensor(fast_wts[iid:iid+sz]).view_as(p)\n",
    "            #     # iid+=sz\n",
    "            for i,train_data in enumerate(train_dataloader):\n",
    "                src = train_data[0]\n",
    "                # print(i,src[0].shape,src[0])\n",
    "                # break\n",
    "                trg = train_data[1]\n",
    "                pred = train_data[2][:,0]\n",
    "                past = train_data[3]\n",
    "                # optimizer.zero_grad()\n",
    "                # encoder_outputs, (hidden, cell) = encoder(src)\n",
    "    \n",
    "                # # input = trg[:-1]\n",
    "                # output,reg,wts = decoder( hidden, cell, encoder_outputs)\n",
    "                # reg = reg[:,0]\n",
    "                output = fmodel(src)\n",
    "                \n",
    "                loss += criterion1(output, trg)/len(task_list)\n",
    "                # print(grad(loss,model.parameters(),retain_graph=True,allow_unused=True))\n",
    "                # 'kn;\n",
    "                # break\n",
    "                # print(reg.shape)\n",
    "                # l2 = criterion2(pred,past*reg)/(batch_size*pow(10,torch.log10(past[-1])))\n",
    "                # loss = l2+l1\n",
    "                # loss = l1\n",
    "                \n",
    "                # length+=batch_size\n",
    "                if i%8000==0:\n",
    "                    # print(epoch,l2,\"out >>>> \",output[:1],\"\\nlabel >>>> \",train_data[1][:1])\n",
    "                    # print(epoch,loss,past[-1],past[-1]*(1+reg[-1]),l2,output[:1],train_data[1][:1],l1)\n",
    "                    print(loss,output.shape,trg.shape,output[0,:],'>>>>>>>>>>>>>>>>.',trg[0,:])\n",
    "                    # if epoch%20==0:\n",
    "                    #     sns.heatmap(wts[0].detach().numpy())\n",
    "                    #     plt.show()\n",
    "            g = torch.autograd.grad(loss, fmodel.parameters(), create_graph=True)\n",
    "            meta_grads = {name:g for ((name, _), g) in zip(fmodel.named_parameters(), g)}\n",
    "            grads.append(meta_grads)\n",
    "                # loss_list.append(loss.detach().numpy())\n",
    "        output = model(src)\n",
    "        loss = criterion(output,trg)\n",
    "        loss.backward(retain_graph=True)\n",
    "        # print(grads)\n",
    "        gradients = {k: sum(d[k] for d in grads) for k in grads[0].keys()}\n",
    "        hooks = []\n",
    "        for(k,v) in model.named_parameters():\n",
    "            def get_closure():\n",
    "                key = k\n",
    "                def replace_grad(grad):\n",
    "                    return gradients[key]\n",
    "                return replace_grad\n",
    "            hooks.append(v.register_hook(get_closure()))\n",
    "        torch.optim.Adam(model.parameters(), lr=meta_lr).zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.optim.Adam(model.parameters(), lr=meta_lr).step()\n",
    "\n",
    "        # Remove the hooks before next training phase\n",
    "        for h in hooks:\n",
    "            h.remove()\n",
    "    for task in task_list:\n",
    "        test_dataloader = task[1]\n",
    "        for i,test_data in enumerate(test_dataloader):\n",
    "            src=test_data[0]\n",
    "            trg = test_data[1]\n",
    "            pred=test_data[2][:,0]\n",
    "            past=test_data[3]\n",
    "            # encoder_outputs, (hidden, cell) = encoder(src)\n",
    "\n",
    "            # # input = trg[:-1]\n",
    "            # output,reg,wts = decoder( hidden, cell, encoder_outputs)\n",
    "            output = model(src)\n",
    "            if i%50==0:\n",
    "                print(criterion1(output,trg))\n",
    "            # optimizer.zero_grad()\n",
    "                \n",
    "    #             break\n",
    "    # print('train_acc = ',matches/length)\n",
    "    return loss_list\n",
    "n_shots = 0\n",
    "task_list = [task1,task2,task3,task4,task5]\n",
    "# loss_list = meta_train(500,task_list,n_shots=n_shots)\n",
    "# confusion_mat2()\n",
    "# writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b3d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'state_dict':model.state_dict()},'stock_lstm_meta_12345_shots.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da6adb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:48:19.039790Z",
     "iopub.status.busy": "2024-03-12T09:48:19.039499Z",
     "iopub.status.idle": "2024-03-12T09:48:19.062572Z",
     "shell.execute_reply": "2024-03-12T09:48:19.061394Z",
     "shell.execute_reply.started": "2024-03-12T09:48:19.039769Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chkpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/working/stock_lstm_meta_1234_1shots.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(chkpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "chkpt = torch.load('/kaggle/working/stock_lstm_meta_1234_1shots.pt')\n",
    "model.load_state_dict(chkpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35bffa5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:55:34.120209Z",
     "iopub.status.busy": "2024-03-12T09:55:34.119888Z",
     "iopub.status.idle": "2024-03-12T09:55:36.094955Z",
     "shell.execute_reply": "2024-03-12T09:55:36.094058Z",
     "shell.execute_reply.started": "2024-03-12T09:55:34.120183Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8289611752360966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6054644808743169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8380355276907002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8327796234772978\n"
     ]
    }
   ],
   "source": [
    "meta_test(model,task_list,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "83d4d5cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T09:56:44.166606Z",
     "iopub.status.busy": "2024-03-11T09:56:44.166073Z",
     "iopub.status.idle": "2024-03-11T09:56:46.551806Z",
     "shell.execute_reply": "2024-03-11T09:56:46.550908Z",
     "shell.execute_reply.started": "2024-03-11T09:56:44.166581Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7211961301671065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7347972972972971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7425569176882661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7226415094339623\n"
     ]
    }
   ],
   "source": [
    "for task in task_list:\n",
    "    y_pred,y_true = [],[]\n",
    "    test_dataloader = task[1]\n",
    "    for i,test_data in enumerate(test_dataloader):\n",
    "        src=test_data[0]\n",
    "        trg = test_data[1]\n",
    "        pred=test_data[2][:,0]\n",
    "        past=test_data[3]\n",
    "        output = model(src)\n",
    "#         if i%50==0:\n",
    "#             print(criterion1(output,trg))\n",
    "        y_pred.append(torch.argmax(output))\n",
    "        y_true.append(torch.argmax(trg))\n",
    "    print(f1_score(y_true,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c17bbf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train2(epochs):\n",
    "    loss_list = []\n",
    "    criterion1 = nn.CrossEntropyLoss(weight=torch.tensor(cnt))\n",
    "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()))\n",
    "    # criterion1 = nn.NLLLoss(weight=torch.tensor(cnt))\n",
    "    criterion2 = nn.MSELoss()\n",
    "    matches,length = 0,0\n",
    "    for epoch in range(epochs):\n",
    "        for i,train_data in enumerate(train_dataloader):\n",
    "            src = train_data[0]\n",
    "            trg = train_data[1]\n",
    "            pred = train_data[2][:,0]\n",
    "            past = train_data[3]\n",
    "            optimizer.zero_grad()\n",
    "            encoder_outputs, (hidden, cell) = encoder(src)\n",
    "\n",
    "            # input = trg[:-1]\n",
    "            output,reg,wts = decoder( hidden, cell, encoder_outputs)\n",
    "            reg = reg[:,0]\n",
    "            # output_dim = output.shape[-1]\n",
    "            # output = output.view(-1, output_dim)\n",
    "            # trg = trg[1:].view(-1)\n",
    "            \n",
    "            # print(output.shape,trg.shape,pred.shape,past.shape,reg.shape)\n",
    "            # print(trg)\n",
    "            # trg = torch.argmax(trg,dim=1).float()\n",
    "            # trg = trg.float()\n",
    "            # print(trg[0],type(trg[0].float()))\n",
    "            l1 = criterion1(output, trg)\n",
    "            # print(reg.shape)\n",
    "            l2 = criterion2(pred,past*reg)/(batch_size*pow(10,torch.log10(past[-1])))\n",
    "            loss = l2+l1\n",
    "            # writer.add_scalar(\"Loss1/train\", l1, i+epoch*len(train_dataloader))\n",
    "            # writer.add_scalar(\"Loss2/train\", l2, i+epoch*len(train_dataloader))\n",
    "            # matches+=batch_size-sum(np.argmax(output.detach().numpy(),axis=1)-np.argmax(trg.numpy(),axis=1))\n",
    "            length+=batch_size\n",
    "            if i%8000==0:\n",
    "                # print(epoch,l2,\"out >>>> \",output[:1],\"\\nlabel >>>> \",train_data[1][:1])\n",
    "                print(epoch,loss,past[-1],past[-1]*(1+reg[-1]),l2,output[:1],train_data[1][:1],l1)\n",
    "                if epoch%20==0:\n",
    "                    sns.heatmap(wts[0].detach().numpy())\n",
    "                    plt.show()\n",
    "\n",
    "\n",
    "            loss_list.append(loss.detach().numpy())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "#             break\n",
    "    print('train_acc = ',matches/length)\n",
    "    return loss_list\n",
    "loss_list = train2(200)\n",
    "confusion_mat2()\n",
    "# writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb25bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_list[-20000:])\n",
    "# torch.save({\n",
    "#             'enc_state_dict': encoder.state_dict(),\n",
    "#             'dec_state_dict': decoder.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "\n",
    "\n",
    "#             }, 'stock_classifier_enc_dec.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f762c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 2\n",
    "d_model = 16\n",
    "model = stock_lstm(in_dim,d_model,n_layers,batch_size,t_steps1,classes)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# try:\n",
    "#     checkpoint = torch.load('stock_classifier_lstm.pt')\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# except:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "676834b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhss20/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.3419, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.4107, 0.5893]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "1 tensor(0.3537, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.2912, 0.7088]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "2 tensor(0.3332, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.3349, 0.6651]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "3 tensor(0.3459, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.3528, 0.6472]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "4 tensor(0.3470, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.3611, 0.6389]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "5 tensor(0.3435, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.3035, 0.6965]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "6 tensor(0.2222, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.0215, 0.9785]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "7 tensor(0.2134, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.9735, 0.0265]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "8 tensor(0.2381, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.0142, 0.9858]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "9 tensor(0.2336, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.6306, 0.3694]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "10 tensor(0.2093, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.8004, 0.1996]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "11 tensor(0.2620, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.0050, 0.9950]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "12 tensor(0.2287, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.0020, 0.9980]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "13 tensor(0.2453, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.0012, 0.9988]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "14 tensor(0.2074, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.0012, 0.9988]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "15 tensor(0.2171, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.9988, 0.0012]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "16 tensor(0.2021, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.9980, 0.0020]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "17 tensor(0.2299, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.0131, 0.9869]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "18 tensor(0.2076, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9915e-01, 8.4599e-04]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "19 tensor(0.1924, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.5603, 0.4397]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "20 tensor(0.2245, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[4.9631e-04, 9.9950e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "21 tensor(0.1839, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.9989, 0.0011]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "22 tensor(0.2082, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9958e-01, 4.1828e-04]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "23 tensor(0.1922, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.7311e-04, 9.9973e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "24 tensor(0.1902, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.9981, 0.0019]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "25 tensor(0.2006, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.0228e-04, 9.9980e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "26 tensor(0.2162, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.3928e-04, 9.9976e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "27 tensor(0.2219, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.8806e-04, 9.9981e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "28 tensor(0.1853, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.9901, 0.0099]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "29 tensor(0.2092, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9980e-01, 2.0159e-04]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "30 tensor(0.1924, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.8864, 0.1136]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "31 tensor(0.2198, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.7256, 0.2744]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "32 tensor(0.1804, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.0066, 0.9934]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "33 tensor(0.1992, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.9971, 0.0029]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "34 tensor(0.2181, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[8.3335e-05, 9.9992e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "35 tensor(0.2076, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9992e-01, 7.7927e-05]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "36 tensor(0.1976, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9972e-01, 2.7744e-04]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "37 tensor(0.1968, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9950e-01, 4.9533e-04]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "38 tensor(0.2244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.0017, 0.9983]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "39 tensor(0.1822, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9998e-01, 2.2215e-05]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "40 tensor(0.1754, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.4968e-04, 9.9985e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "41 tensor(0.1898, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9981e-01, 1.8527e-04]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "42 tensor(0.2010, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[3.6004e-04, 9.9964e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "43 tensor(0.2101, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9999e-01, 1.1828e-05]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "44 tensor(0.1986, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[5.8343e-05, 9.9994e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "45 tensor(0.1894, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.3661, 0.6339]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "46 tensor(0.1976, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.9795, 0.0205]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "47 tensor(0.1830, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9998e-01, 2.4490e-05]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "48 tensor(0.2097, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9997e-01, 2.6778e-05]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "49 tensor(0.2023, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[3.4883e-04, 9.9965e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "50 tensor(0.2054, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[4.8283e-05, 9.9995e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "51 tensor(0.1918, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[4.7631e-05, 9.9995e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "52 tensor(0.1964, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[6.2697e-05, 9.9994e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "53 tensor(0.1833, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[5.1475e-06, 9.9999e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "54 tensor(0.1882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[8.6021e-05, 9.9991e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "55 tensor(0.1981, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9999e-01, 7.2616e-06]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "56 tensor(0.1883, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.1612e-05, 9.9999e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "57 tensor(0.1938, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.7720e-05, 9.9998e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "58 tensor(0.1815, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.0016, 0.9984]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "59 tensor(0.1665, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.9930e-06, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "60 tensor(0.2071, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.0834e-05, 9.9998e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "61 tensor(0.1747, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.8952e-06, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "62 tensor(0.1915, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.9714, 0.0286]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "63 tensor(0.1658, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.1165e-06, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "64 tensor(0.1837, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.0346e-07]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "65 tensor(0.1729, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5806e-04, 9.9974e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "66 tensor(0.1816, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[5.7996e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "67 tensor(0.1680, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 2.4948e-08]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "68 tensor(0.1766, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[5.5136e-05, 9.9994e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "69 tensor(0.1694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[8.9612e-06, 9.9999e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "70 tensor(0.2042, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9998e-01, 2.0442e-05]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "71 tensor(0.1735, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.8842e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "72 tensor(0.1737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.9890, 0.0110]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "73 tensor(0.1756, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.6637e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "74 tensor(0.2086, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.5916e-08]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "75 tensor(0.1660, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0431e-05, 9.9999e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "76 tensor(0.1871, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.7765e-08]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "77 tensor(0.1570, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9911e-01, 8.9218e-04]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "78 tensor(0.2031, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 2.1832e-09]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "79 tensor(0.1827, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5685e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "80 tensor(0.1663, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0074e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "81 tensor(0.1714, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.2353e-06]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "82 tensor(0.1791, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.4417e-06, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "83 tensor(0.2075, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9999e-01, 6.7981e-06]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "84 tensor(0.1728, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[4.7277e-08, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "85 tensor(0.1860, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[7.2890e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "86 tensor(0.1607, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.7585e-05, 9.9997e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "87 tensor(0.2073, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 7.7326e-09]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "88 tensor(0.1706, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9998e-01, 2.0544e-05]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "89 tensor(0.1637, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.6260e-10]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "90 tensor(0.1724, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 8.6523e-09]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "91 tensor(0.1817, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[4.8633e-08, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "92 tensor(0.1575, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[5.3005e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "93 tensor(0.1944, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 8.8536e-11]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "94 tensor(0.1659, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.7731e-08, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "95 tensor(0.1775, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[3.0938e-06, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "96 tensor(0.2003, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[4.7037e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "97 tensor(0.1794, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 3.6155e-09]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "98 tensor(0.1803, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[6.4273e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "99 tensor(0.1903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[6.1375e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "100 tensor(0.1775, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 2.0903e-10]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "101 tensor(0.1798, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.6669e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "102 tensor(0.1587, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[8.1929e-06, 9.9999e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "103 tensor(0.1671, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.9389e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "104 tensor(0.1661, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.2467e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "105 tensor(0.2055, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.1907e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "106 tensor(0.1878, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.6525e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "107 tensor(0.1735, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.1081e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "108 tensor(0.1807, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9983e-01, 1.7356e-04]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "109 tensor(0.1709, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.0605e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "110 tensor(0.1893, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.3822, 0.6178]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "111 tensor(0.1817, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.8548e-09]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "112 tensor(0.1736, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[3.9293e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "113 tensor(0.1719, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[8.7451e-08, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "114 tensor(0.1900, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 2.2818e-11]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "115 tensor(0.1710, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.7175e-08]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "116 tensor(0.1722, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 2.3581e-12]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "117 tensor(0.1724, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[6.3809e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "118 tensor(0.1791, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.2937e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "119 tensor(0.1730, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[3.4754e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "120 tensor(0.1659, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.4433e-05, 9.9999e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "121 tensor(0.1798, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 2.4824e-07]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "122 tensor(0.1656, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9999e-01, 1.3804e-05]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "123 tensor(0.1710, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[4.7041e-08, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "124 tensor(0.1754, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[8.8093e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "125 tensor(0.1783, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[4.1464e-10, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "126 tensor(0.1650, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9996e-01, 3.8348e-05]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "127 tensor(0.1593, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9995e-01, 5.1450e-05]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "128 tensor(0.1722, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.9295e-10, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "129 tensor(0.1892, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[6.4941e-08, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "130 tensor(0.1830, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 4.7310e-12]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "131 tensor(0.1718, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 4.3691e-10]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "132 tensor(0.1641, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.0821e-10, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "133 tensor(0.1764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.4948e-04, 9.9985e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "134 tensor(0.1805, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.3338e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "135 tensor(0.1769, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.3869e-13]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "136 tensor(0.1714, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5295e-05, 9.9997e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "137 tensor(0.1981, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.6610e-06, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "138 tensor(0.1679, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.0387e-13]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "139 tensor(0.1692, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9991e-01, 9.0287e-05]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "140 tensor(0.1777, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.0796e-07]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "141 tensor(0.1692, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[7.8513e-11, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "142 tensor(0.1722, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.1810e-05, 9.9998e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "143 tensor(0.1782, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.1779e-12]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "144 tensor(0.1834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.9955e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "145 tensor(0.1666, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 5.5229e-14]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "146 tensor(0.1714, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 2.1388e-06]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "147 tensor(0.1588, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.5652e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "148 tensor(0.1656, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[8.6473e-11, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "149 tensor(0.1672, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[7.1645e-11, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "150 tensor(0.1635, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.0197e-13]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "151 tensor(0.1729, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 7.7454e-13]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "152 tensor(0.1778, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.2114e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "153 tensor(0.1731, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 3.3320e-08]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "154 tensor(0.1654, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 6.1294e-14]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "155 tensor(0.1726, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.2262e-10, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "156 tensor(0.1562, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[6.4913e-10, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "157 tensor(0.1874, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[3.8505e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "158 tensor(0.1798, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.9162e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "159 tensor(0.1574, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 3.3119e-06]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "160 tensor(0.1679, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 2.1499e-14]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "161 tensor(0.1634, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 2.7925e-10]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "162 tensor(0.1658, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.2062e-06, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "163 tensor(0.1687, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.9046e-10, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "164 tensor(0.1642, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[8.1173e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "165 tensor(0.1582, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[7.1409e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "166 tensor(0.1578, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[6.5047e-10, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "167 tensor(0.1802, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.8862e-11, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "168 tensor(0.1715, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 7.7370e-13]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "169 tensor(0.1750, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[5.8186e-05, 9.9994e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "170 tensor(0.1817, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.0350e-13]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "171 tensor(0.1679, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.1480e-12]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "172 tensor(0.1663, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 9.3313e-15]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "173 tensor(0.1847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.1198e-12]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "174 tensor(0.1876, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[3.6964e-11, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "175 tensor(0.1665, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 5.5779e-10]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "176 tensor(0.1669, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 2.7398e-14]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "177 tensor(0.1891, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.9429e-11]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "178 tensor(0.1735, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 2.6689e-14]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "179 tensor(0.1679, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.9267e-11, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "180 tensor(0.1598, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[5.3113e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "181 tensor(0.1654, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[4.2016e-10, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "182 tensor(0.1727, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 4.6689e-09]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "183 tensor(0.1612, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[7.1962e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "184 tensor(0.1646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 7.2532e-15]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "185 tensor(0.1679, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.0319e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "186 tensor(0.1662, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.2981e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "187 tensor(0.1642, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 4.6075e-13]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "188 tensor(0.1654, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.1863e-11]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "189 tensor(0.1658, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[4.1731e-11, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "190 tensor(0.1710, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[6.6311e-05, 9.9993e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "191 tensor(0.1961, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.1365e-06, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "192 tensor(0.1806, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9903e-01, 9.6537e-04]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "193 tensor(0.1675, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 4.0836e-15]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "194 tensor(0.1590, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.1664e-12]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "195 tensor(0.1546, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.1486e-14]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "196 tensor(0.1598, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5181e-11, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "197 tensor(0.1550, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 4.6795e-09]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "198 tensor(0.1575, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 5.6275e-10]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "199 tensor(0.1643, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[4.2240e-06, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "train_acc =  0.9699787946428572\n",
      "0 tensor(0.5013, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[3.8254e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "1 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4859e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "2 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4422e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "3 tensor(0.3043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4411e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "4 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5190e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "5 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4458e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "6 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4503e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "7 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4423e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "8 tensor(0.3931, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4411e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "9 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4686e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "10 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4823e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "11 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "12 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4402e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "13 tensor(0.3931, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4463e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "14 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4449e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "15 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5367e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "16 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4384e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "17 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "18 tensor(0.3635, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4420e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "19 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4623e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "20 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "21 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "22 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4769e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "23 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "24 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "25 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "26 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4471e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "27 tensor(0.3635, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "28 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "29 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "30 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.6039e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "31 tensor(0.3754, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4426e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "32 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4487e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "33 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4409e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "34 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4559e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "35 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "36 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4485e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "37 tensor(0.3102, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "38 tensor(0.3991, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4437e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "39 tensor(0.3102, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.6044e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "40 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "41 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4426e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "42 tensor(0.3043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "43 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4421e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "44 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4512e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "45 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "46 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "47 tensor(0.3635, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4500e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "48 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4384e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "49 tensor(0.3813, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4406e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "50 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4436e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "51 tensor(0.3754, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4414e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "52 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4473e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "53 tensor(0.3221, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4401e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "54 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4428e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "55 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "56 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4399e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "57 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "58 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4446e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "59 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "60 tensor(0.3754, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4472e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "61 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4406e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "62 tensor(0.3813, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4444e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "63 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5714e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "64 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4569e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "65 tensor(0.3043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4444e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "66 tensor(0.3635, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4395e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "67 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "68 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "69 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "70 tensor(0.3221, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "71 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4404e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "72 tensor(0.3635, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5009e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "73 tensor(0.3221, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4437e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "74 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4426e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "75 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4437e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "76 tensor(0.2806, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4559e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "77 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4436e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "78 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4451e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "79 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "80 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5022e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "81 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "82 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4474e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "83 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "84 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4517e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "85 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4407e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "86 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "87 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4414e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "88 tensor(0.3635, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4406e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "89 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "90 tensor(0.3221, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4568e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "91 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4401e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "92 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4437e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "93 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4417e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "94 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4404e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "95 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4422e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "96 tensor(0.3221, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4413e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "97 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4423e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "98 tensor(0.2747, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "99 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4416e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "100 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4405e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "101 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4424e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "102 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4385e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "103 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4574e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "104 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4660e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "105 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "106 tensor(0.3635, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "107 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "108 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4489e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "109 tensor(0.3754, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4399e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "110 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4713e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "111 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4484e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "112 tensor(0.4050, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "113 tensor(0.3754, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4413e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "114 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4392e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "115 tensor(0.3813, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4399e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "116 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5842e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "117 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4648e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "118 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4402e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "119 tensor(0.3221, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4629e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "120 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4425e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "121 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4384e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "122 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4384e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "123 tensor(0.3102, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4591e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "124 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4801e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "125 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4386e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "126 tensor(0.3635, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4392e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "127 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4426e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "128 tensor(0.3754, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4408e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "129 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5588e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "130 tensor(0.3872, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4414e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "131 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4496e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "132 tensor(0.3102, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4415e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "133 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "134 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4715e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "135 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4493e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "136 tensor(0.3102, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4406e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "137 tensor(0.3813, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4455e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "138 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4453e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "139 tensor(0.3043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4386e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "140 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "141 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4444e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "142 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "143 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "144 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "145 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "146 tensor(0.3754, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4406e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "147 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4494e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "148 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4744e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "149 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "150 tensor(0.3102, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4455e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "151 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4471e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "152 tensor(0.3102, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "153 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4404e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "154 tensor(0.3754, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4528e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "155 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4456e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "156 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "157 tensor(0.3102, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4384e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "158 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4633e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "159 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "160 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4443e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "161 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4417e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "162 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4449e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "163 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4402e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "164 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4769e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "165 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "166 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4547e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "167 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "168 tensor(0.3754, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4421e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "169 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4431e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "170 tensor(0.4109, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4386e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "171 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4681e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "172 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4426e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "173 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "174 tensor(0.3102, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4408e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "175 tensor(0.3043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4392e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "176 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "177 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4385e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "178 tensor(0.3635, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4600e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "179 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "180 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4419e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "181 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4421e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "182 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4395e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "183 tensor(0.3813, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4406e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "184 tensor(0.3102, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "185 tensor(0.3635, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4667e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "186 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4395e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "187 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "188 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4421e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "189 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4496e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "190 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4484e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "191 tensor(0.3931, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4406e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "192 tensor(0.3221, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.7668e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "193 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4466e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "194 tensor(0.3043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4478e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "195 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4518e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "196 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4721e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "197 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4441e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "198 tensor(0.3102, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4446e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "199 tensor(0.3043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "train_acc =  0.5750939764492754\n",
      "0 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4509e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "1 tensor(0.3378, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4421e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "2 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4461e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "3 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "4 tensor(0.4114, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4463e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "5 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4466e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "6 tensor(0.4114, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4442e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "7 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "8 tensor(0.4382, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4435e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "9 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4471e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "10 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "11 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4799e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "12 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "13 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "14 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "15 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5352e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "16 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4519e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "17 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4408e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "18 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "19 tensor(0.3914, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4512e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "20 tensor(0.4048, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4401e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "21 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4414e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "22 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4619e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "23 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4442e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "24 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "25 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4552e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "26 tensor(0.3378, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[3.1239e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "27 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4815e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "28 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4473e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "29 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "30 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4439e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "31 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4457e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "32 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "33 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4449e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "34 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4416e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "35 tensor(0.3177, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "36 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4972e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "37 tensor(0.4048, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4423e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "38 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4616e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "39 tensor(0.3981, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "40 tensor(0.4048, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4476e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "41 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4443e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "42 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4429e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "43 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4484e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "44 tensor(0.3378, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4564e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "45 tensor(0.4181, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4461e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "46 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4587e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "47 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "48 tensor(0.3378, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4424e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "49 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4468e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "50 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "51 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4423e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "52 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "53 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4568e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "54 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4510e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "55 tensor(0.4315, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4434e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "56 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4415e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "57 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "58 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4419e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "59 tensor(0.3914, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "60 tensor(0.4449, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4401e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "61 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4446e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "62 tensor(0.3914, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4444e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "63 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4384e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "64 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4414e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "65 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4455e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "66 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4429e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "67 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "68 tensor(0.3177, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4409e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "69 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4405e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "70 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4423e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "71 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4432e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "72 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4415e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "73 tensor(0.3311, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4423e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "74 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "75 tensor(0.3914, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4429e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "76 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4723e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "77 tensor(0.3311, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4445e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "78 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4453e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "79 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "80 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "81 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4420e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "82 tensor(0.4114, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4874e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "83 tensor(0.3244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "84 tensor(0.3110, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "85 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "86 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4425e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "87 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4416e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "88 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4402e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "89 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "90 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4449e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "91 tensor(0.4048, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4444e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "92 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4435e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "93 tensor(0.3981, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4413e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "94 tensor(0.4048, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "95 tensor(0.4248, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4566e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "96 tensor(0.3378, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4732e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "97 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4463e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "98 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4495e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "99 tensor(0.3244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "100 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4407e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "101 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4484e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "102 tensor(0.4048, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "103 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4472e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "104 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4744e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "105 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4509e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "106 tensor(0.3244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4419e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "107 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "108 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5042e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "109 tensor(0.3914, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4498e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "110 tensor(0.3311, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "111 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4386e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "112 tensor(0.3110, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4501e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "113 tensor(0.3311, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4449e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "114 tensor(0.3914, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "115 tensor(0.4048, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4385e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "116 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4637e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "117 tensor(0.3311, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "118 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4395e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "119 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "120 tensor(0.3378, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4395e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "121 tensor(0.4114, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4406e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "122 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "123 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4405e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "124 tensor(0.4114, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4487e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "125 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4384e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "126 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "127 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4413e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "128 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "129 tensor(0.3244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "130 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "131 tensor(0.3914, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4445e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "132 tensor(0.4048, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4433e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "133 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4869e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "134 tensor(0.4382, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "135 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4418e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "136 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4418e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "137 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4493e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "138 tensor(0.2909, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4546e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "139 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5862e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "140 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "141 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "142 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5198e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "143 tensor(0.3914, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4399e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "144 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "145 tensor(0.3311, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4426e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "146 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4447e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "147 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4471e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "148 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4430e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "149 tensor(0.2842, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "150 tensor(0.3311, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4643e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "151 tensor(0.3311, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4464e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "152 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5652e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "153 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4520e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "154 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "155 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4439e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "156 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4405e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "157 tensor(0.3244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4415e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "158 tensor(0.3981, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4401e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "159 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4446e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "160 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4412e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "161 tensor(0.3914, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4436e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "162 tensor(0.3311, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4395e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "163 tensor(0.3378, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4538e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "164 tensor(0.3378, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4436e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "165 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4416e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "166 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4493e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "167 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4455e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "168 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.9828e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "169 tensor(0.3244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4404e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "170 tensor(0.3311, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "171 tensor(0.3914, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "172 tensor(0.3914, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4411e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "173 tensor(0.3914, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5099e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "174 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4408e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "175 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4407e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "176 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "177 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "178 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "179 tensor(0.4114, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4726e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "180 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4640e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "181 tensor(0.3244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "182 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4402e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "183 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4763e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "184 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4395e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "185 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "186 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "187 tensor(0.3378, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4430e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "188 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4420e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "189 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4401e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "190 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4445e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "191 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4494e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "192 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4444e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "193 tensor(0.4114, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4552e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "194 tensor(0.3981, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4585e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "195 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "196 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "197 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4415e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "198 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.6514e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "199 tensor(0.2842, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "train_acc =  0.5439185267857143\n",
      "0 tensor(0.4113, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4384e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "1 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4406e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "2 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4402e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "3 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4384e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "4 tensor(0.4322, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4451e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "5 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4462e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "6 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "7 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4655e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "8 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5164e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "9 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "10 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4485e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "11 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4445e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "12 tensor(0.4392, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4445e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "13 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4475e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "14 tensor(0.4322, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4414e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "15 tensor(0.3205, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4438e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "16 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4408e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "17 tensor(0.4253, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4695e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "18 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4459e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "19 tensor(0.4043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4409e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "20 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4450e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "21 tensor(0.3205, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "22 tensor(0.4043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "23 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4527e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "24 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4405e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "25 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4652e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "26 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4493e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "27 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4383e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "28 tensor(0.4113, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4478e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "29 tensor(0.3205, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "30 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4818e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "31 tensor(0.3205, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "32 tensor(0.4183, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "33 tensor(0.3275, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4417e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "34 tensor(0.4043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "35 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4424e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "36 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "37 tensor(0.4322, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4537e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "38 tensor(0.4113, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4405e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "39 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4780e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "40 tensor(0.3205, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4414e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "41 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4404e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "42 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "43 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4486e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "44 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4414e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "45 tensor(0.3414, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "46 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4418e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "47 tensor(0.3484, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "48 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4498e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "49 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "50 tensor(0.3345, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4454e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "51 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4608e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "52 tensor(0.3414, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4423e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "53 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "54 tensor(0.3484, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4407e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "55 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4386e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "56 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4425e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "57 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4618e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "58 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4487e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "59 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4509e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "60 tensor(0.3345, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4419e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "61 tensor(0.4043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4432e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "62 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4535e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "63 tensor(0.3275, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4392e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "64 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4601e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "65 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4457e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "66 tensor(0.3135, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "67 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[4.0846e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "68 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "69 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4604e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "70 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4435e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "71 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "72 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4399e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "73 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4499e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "74 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4427e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "75 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4434e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "76 tensor(0.3205, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4411e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "77 tensor(0.4043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4439e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "78 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4668e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "79 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4417e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "80 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "81 tensor(0.3484, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4413e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "82 tensor(0.4113, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4386e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "83 tensor(0.4322, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4567e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "84 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "85 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4463e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "86 tensor(0.3414, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4408e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "87 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4594e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "88 tensor(0.4183, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4430e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "89 tensor(0.3484, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4442e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "90 tensor(0.4043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4468e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "91 tensor(0.4322, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "92 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4609e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "93 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4444e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "94 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "95 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "96 tensor(0.3484, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4424e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "97 tensor(0.4043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4464e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "98 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4413e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "99 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4416e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "100 tensor(0.4392, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4416e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "101 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4425e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "102 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.9754e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "103 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4411e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "104 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "105 tensor(0.4183, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4426e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "106 tensor(0.4253, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4429e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "107 tensor(0.4322, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4434e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "108 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "109 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4446e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "110 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4405e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "111 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4420e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "112 tensor(0.4113, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "113 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "114 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "115 tensor(0.3345, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4936e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "116 tensor(0.3484, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4407e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "117 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4696e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "118 tensor(0.3414, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "119 tensor(0.4183, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4630e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "120 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4416e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "121 tensor(0.3414, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4392e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "122 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4438e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "123 tensor(0.4113, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4468e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "124 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4589e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "125 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4572e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "126 tensor(0.4322, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4430e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "127 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "128 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4414e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "129 tensor(0.3275, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4395e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "130 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4449e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "131 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4386e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "132 tensor(0.3135, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4399e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "133 tensor(0.3484, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4601e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "134 tensor(0.3414, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "135 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "136 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4778e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "137 tensor(0.3484, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4399e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "138 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "139 tensor(0.3414, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "140 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4443e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "141 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "142 tensor(0.4392, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4486e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "143 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4407e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "144 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4460e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "145 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4467e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "146 tensor(0.4113, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4500e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "147 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4401e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "148 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4402e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "149 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4421e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "150 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "151 tensor(0.4113, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "152 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4579e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "153 tensor(0.4043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4450e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "154 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4462e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "155 tensor(0.4253, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4545e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "156 tensor(0.3205, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4422e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "157 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4404e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "158 tensor(0.4183, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4412e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "159 tensor(0.3345, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "160 tensor(0.3484, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4405e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "161 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4438e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "162 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4451e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "163 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4478e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "164 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "165 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4438e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "166 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4417e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "167 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4395e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "168 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4402e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "169 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4717e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "170 tensor(0.4043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4672e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "171 tensor(0.4183, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4402e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "172 tensor(0.3414, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4441e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "173 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "174 tensor(0.4253, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4419e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "175 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4604e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "176 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4404e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "177 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4447e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "178 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4754e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "179 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4452e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "180 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4643e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "181 tensor(0.3484, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "182 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4392e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "183 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "184 tensor(0.3275, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4690e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "185 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4545e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "186 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "187 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4392e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "188 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4624e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "189 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "190 tensor(0.4043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4444e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "191 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4448e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "192 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "193 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4451e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "194 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4416e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "195 tensor(0.4322, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4479e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "196 tensor(0.4462, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4411e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "197 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4474e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "198 tensor(0.4322, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "199 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4405e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "train_acc =  0.532489013671875\n",
      "0 tensor(0.4099, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "1 tensor(0.4171, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.6360e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "2 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4401e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "3 tensor(0.4461, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4421e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "4 tensor(0.3447, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "5 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "6 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "7 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4456e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "8 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "9 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4407e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "10 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4614e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "11 tensor(0.3375, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4442e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "12 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "13 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "14 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "15 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "16 tensor(0.4099, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "17 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4450e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "18 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4415e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "19 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "20 tensor(0.4026, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4451e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "21 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4600e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "22 tensor(0.3520, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "23 tensor(0.4244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "24 tensor(0.3447, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "25 tensor(0.3230, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4385e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "26 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4477e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "27 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4408e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "28 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4406e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "29 tensor(0.4244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "30 tensor(0.3520, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4399e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "31 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4413e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "32 tensor(0.4461, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4433e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "33 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4487e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "34 tensor(0.3447, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "35 tensor(0.3592, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "36 tensor(0.3447, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5218e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "37 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "38 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4437e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "39 tensor(0.4099, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4414e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "40 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4433e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "41 tensor(0.4389, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "42 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "43 tensor(0.3592, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4409e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "44 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4458e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "45 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4392e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "46 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5294e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "47 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "48 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4440e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "49 tensor(0.4533, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4386e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "50 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4844e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "51 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "52 tensor(0.4026, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4438e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "53 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4527e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "54 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4461e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "55 tensor(0.4316, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "56 tensor(0.4461, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4578e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "57 tensor(0.3592, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "58 tensor(0.3230, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4414e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "59 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "60 tensor(0.4244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4992e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "61 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "62 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4392e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "63 tensor(0.4099, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4462e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "64 tensor(0.3302, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4420e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "65 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4711e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "66 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4516e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "67 tensor(0.3302, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "68 tensor(0.4099, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4455e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "69 tensor(0.3375, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4417e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "70 tensor(0.4099, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4501e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "71 tensor(0.4171, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4409e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "72 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4701e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "73 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4453e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "74 tensor(0.4026, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "75 tensor(0.3230, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4429e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "76 tensor(0.3157, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4407e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "77 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "78 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "79 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "80 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "81 tensor(0.4171, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4392e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "82 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "83 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4404e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "84 tensor(0.4244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "85 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4386e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "86 tensor(0.4389, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4419e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "87 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4421e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "88 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4513e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "89 tensor(0.4171, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "90 tensor(0.4026, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4414e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "91 tensor(0.4171, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4456e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "92 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4420e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "93 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "94 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "95 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4678e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "96 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4418e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "97 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4602e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "98 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4482e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "99 tensor(0.4171, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4485e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "100 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "101 tensor(0.4026, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4420e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "102 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4425e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "103 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4413e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "104 tensor(0.4171, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "105 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4496e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "106 tensor(0.3592, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5167e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "107 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4444e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "108 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4401e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "109 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4419e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "110 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5009e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "111 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4454e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "112 tensor(0.4026, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4724e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "113 tensor(0.4171, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4401e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "114 tensor(0.4751, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4435e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "115 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4443e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "116 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "117 tensor(0.3592, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4423e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "118 tensor(0.4244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4472e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "119 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4402e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "120 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "121 tensor(0.3592, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4793e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "122 tensor(0.4026, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4415e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "123 tensor(0.3447, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4724e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "124 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4545e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "125 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "126 tensor(0.4171, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4453e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "127 tensor(0.4099, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4424e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "128 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4515e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "129 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "130 tensor(0.3157, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4414e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "131 tensor(0.3447, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "132 tensor(0.4461, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "133 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4553e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "134 tensor(0.3375, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4412e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "135 tensor(0.4171, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4443e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "136 tensor(0.4099, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4417e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "137 tensor(0.3520, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "138 tensor(0.4171, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4489e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "139 tensor(0.4099, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4402e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "140 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4416e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "141 tensor(0.3520, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "142 tensor(0.4026, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4402e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "143 tensor(0.3302, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4435e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "144 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4490e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "145 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4649e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "146 tensor(0.3230, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4674e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "147 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "148 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "149 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "150 tensor(0.3375, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "151 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4406e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "152 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5528e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "153 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4411e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "154 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4507e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "155 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4442e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "156 tensor(0.4461, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4405e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "157 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5862e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "158 tensor(0.4244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4607e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "159 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "160 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4413e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "161 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "162 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4456e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "163 tensor(0.3447, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4434e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "164 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "165 tensor(0.3447, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4570e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "166 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4622e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "167 tensor(0.4316, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "168 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4406e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "169 tensor(0.3592, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4759e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "170 tensor(0.3447, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4489e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "171 tensor(0.4244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "172 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4392e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "173 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "174 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4619e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "175 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "176 tensor(0.3592, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4392e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "177 tensor(0.4026, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4668e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "178 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4401e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "179 tensor(0.3520, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4468e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "180 tensor(0.3447, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4490e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "181 tensor(0.3375, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4436e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "182 tensor(0.4099, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "183 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4522e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "184 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4392e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "185 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5562e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "186 tensor(0.3520, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4404e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "187 tensor(0.3592, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4443e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "188 tensor(0.3447, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4417e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "189 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "190 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4770e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "191 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4532e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "192 tensor(0.3592, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4540e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "193 tensor(0.4171, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4780e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "194 tensor(0.3447, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4418e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "195 tensor(0.3302, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "196 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4631e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "197 tensor(0.3592, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5577e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "198 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "199 tensor(0.3375, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "train_acc =  0.5224467844202898\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "\n",
    "\n",
    "def train(epochs,tsk):\n",
    "    matches,length = 0,0\n",
    "    train_dataloader = tsk[0]\n",
    "    criterion = nn.CrossEntropyLoss(weight = torch.tensor(tsk[2]))\n",
    "    for epoch in range(epochs):\n",
    "        for i,train_data in enumerate(train_dataloader):\n",
    "            model.zero_grad()\n",
    "#             in_mean = torch.mean(train_data[0],dim=1,keepdim=True)\n",
    "#             in_std = torch.std(train_data[0],dim=1,keepdim=True)\n",
    "            inp,exp_out = train_data[0],train_data[1]\n",
    "#             print(exp_out)\n",
    "#             return\n",
    "            out = model(inp)\n",
    "            loss = criterion(out,exp_out)\n",
    "            matches+=batch_size-sum(np.argmax(out.detach().numpy(),axis=1)-np.argmax(exp_out.numpy(),axis=1))\n",
    "            length+=batch_size\n",
    "#             return\n",
    "            if i%8000==0:\n",
    "#                 model()\n",
    "                print(epoch,loss,\"out >>>> \",out[:1],\"\\nlabel >>>> \",train_data[1][:1])\n",
    "#                 break\n",
    "#                 return\n",
    "            \n",
    "            loss_list.append(loss.detach().numpy())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    print('train_acc = ',matches/length)\n",
    "for tsk in task_list:\n",
    "    train(200,tsk)\n",
    "# confusion_mat1()\n",
    "# torch.save({\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "\n",
    "            \n",
    "#             }, 'stock_classifier_lstm.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ae77217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x72dc3df4dea0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "682a1aaf-f7b2-47f9-b33b-e34742edcf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhss20/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6925675675675677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhss20/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7424857839155159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhss20/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7078464106844742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhss20/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6963636363636364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhss20/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.714879467996675\n"
     ]
    }
   ],
   "source": [
    "def model_test(test_dataloader,model):\n",
    "    y_pred,y_true = [],[]\n",
    "    for i,test_data in enumerate(test_dataloader):\n",
    "        src=test_data[0]\n",
    "        trg = test_data[1]\n",
    "        pred=test_data[2][:,0]\n",
    "        past=test_data[3]\n",
    "        output = model(src)\n",
    "#         if i%50==0:\n",
    "#             print(criterion1(output,trg))\n",
    "        y_pred.append(torch.argmax(output))\n",
    "        y_true.append(torch.argmax(trg))\n",
    "    print(f1_score(y_true,y_pred))\n",
    "    \n",
    "def meta_test(model,task_list,n_shots=0):\n",
    "    \n",
    "    for task in task_list:\n",
    "        criterion1 = nn.CrossEntropyLoss(weight = torch.tensor(task[2]))\n",
    "        tmodel = copy.deepcopy(model)\n",
    "        optimizer = torch.optim.Adam(tmodel.parameters())\n",
    "        test_dataloader =  task[1]\n",
    "        itr = iter(test_dataloader)\n",
    "        for _ in range(n_shots):\n",
    "            test_data = next(itr)\n",
    "            src=test_data[0]\n",
    "            trg = test_data[1]\n",
    "            pred=test_data[2][:,0]\n",
    "            past=test_data[3]\n",
    "            output = tmodel(src)\n",
    "            loss = criterion1(output,trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        tmodel.eval()\n",
    "        model_test(test_dataloader,tmodel)\n",
    "meta_test(model,task_list,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a114805",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test1(t_steps,i1=0,i2=1e9):\n",
    "    colr = ['red','green']\n",
    "#     print()\n",
    "    i2 = int(min(i2,test_len-t_steps-extra_days))\n",
    "#     plt.plot(range(i1,i2),snp['Close'][train_len+i1+t_steps:train_len+i2+t_steps],color ='black',linewidth=0.5)\n",
    "    with torch.no_grad():\n",
    "        for i,data in enumerate(test_dataloader):\n",
    "            if(i<i1 or i>i2):\n",
    "                continue\n",
    "            inp,labels,pred,past = data[0],data[1],data[2],data[3]\n",
    "#             print(pred)\n",
    "            out = model(inp.repeat(batch_size,1,1))[0]\n",
    "#             print(inp[0,:,-1][-1])\n",
    "#             past,std_past = torch.mean(inp[0,:,-1][-1]),torch.std(inp[0,:,-1])\n",
    "            idx = torch.argmax(out)\n",
    "#             print(idx)\n",
    "#             return\n",
    "            plt.scatter(i,past,color='black',s=2,alpha=0.3)\n",
    "#             plt.scatter(i,snp['swt'][i+train_len+t_steps1-1],color='blue',s=2,alpha=0.5)\n",
    "            if(idx==0):\n",
    "                plt.fill_between(\n",
    "                    range(i,i+predict_steps),\n",
    "                    past-(10*pd1)*np.sqrt(past), \n",
    "                    past, \n",
    "                    alpha=0.5,\n",
    "                    color=colr[idx],\n",
    "                    interpolate=True,\n",
    "                    label=\"+/- 1-std\",\n",
    "                )\n",
    "            if(idx==1):\n",
    "                plt.fill_between(\n",
    "                    range(i,i+predict_steps),\n",
    "                    past, \n",
    "                    past+5*pu1*np.sqrt(past), \n",
    "                    alpha=0.5,\n",
    "                    color=colr[idx],\n",
    "                    interpolate=True,\n",
    "                    label=\"+/- 1-std\",\n",
    "                )\n",
    "            if(idx==2):\n",
    "                plt.fill_between(\n",
    "                    range(i,i+predict_steps),\n",
    "                    past+pu1*std_past, \n",
    "                    past+2*pu1*std_past, \n",
    "                    alpha=0.5,\n",
    "                    color=colr[idx],\n",
    "                    interpolate=True,\n",
    "                    label=\"+/- 1-std\",\n",
    "                )\n",
    "#             if(idx==3):\n",
    "#                 plt.fill_between(\n",
    "#                     range(i,i+predict_steps),\n",
    "#                     past+par_k2*par_k*std_past, \n",
    "#                     past+(par_k2+1)*par_k*std_past, \n",
    "#                     alpha=0.5,\n",
    "#                     color=colr[idx],\n",
    "#                     interpolate=True,\n",
    "#                     label=\"+/- 1-std\",\n",
    "#                 )\n",
    "#             if(i==200):\n",
    "#                 break\n",
    "def random_confusion():\n",
    "    y_pred,y_true = [],[]\n",
    "    colr = ['red','green']\n",
    "    for i,data in enumerate(test_dataloader):\n",
    "        inp,labels,_,past = data[0],data[1],data[2],data[3]\n",
    "        # out = model(inp.repeat(batch_size,1,1))[0]\n",
    "      # input = trg[:-1]\n",
    "#         past,std_past = torch.mean(inp[0,:,-1]),torch.std(inp[0,:,-1])\n",
    "#         past,std_past = torch.mean(inp[0,:,-1][-1]),torch.std(inp[0,:,-1])\n",
    "        pred = snp['swt'][i+train_len+t_steps1-1]\n",
    "        if pred<past:\n",
    "            idx=0\n",
    "        else:\n",
    "            idx=1\n",
    "        pred_label = torch.tensor(idx)\n",
    "#         pred_label[id]=1\n",
    "        y_pred.append(pred_label)\n",
    "#             temp[torch.argmax(out)]=1\n",
    "        y_true.append(torch.argmax(labels))\n",
    "    conf_mat = confusion_matrix(y_true,y_pred,normalize='pred')\n",
    "    print(conf_mat)\n",
    "    print(np.trace(np.array(conf_mat))/classes)\n",
    "    \n",
    "def random_confusion_return():\n",
    "    y_pred,y_true = [],[]\n",
    "    for i,data in enumerate(test_dataloader):\n",
    "        inp,labels,pred = data[0],data[1],data[2]\n",
    "        # out = model(inp.repeat(batch_size,1,1))[0]\n",
    "      # input = trg[:-1]\n",
    "#         past = talib.EMA(inp[0,:,-1].numpy().astype(np.float64),timeperiod = len(inp[0,:,-1]))[-1]\n",
    "        past = torch.mean(inp[0,:,-1][-3:])\n",
    "        pred = 100*(torch.mean(inp[0,:,-1][-1])-past)/past\n",
    "        \n",
    "        if pred<pd1:\n",
    "            id=0\n",
    "        elif pred<pd2:\n",
    "            id=1\n",
    "        elif pred<=pu1:\n",
    "            id=2\n",
    "        elif pred<pu2:\n",
    "            id=3\n",
    "        elif pred<=pu3:\n",
    "            id=4\n",
    "        else:\n",
    "            id=5\n",
    "        pred_label = torch.tensor(np.random.randint(0,6,1))[0]\n",
    "#         pred_label = torch.tensor(id)\n",
    "#         print(pred_label)\n",
    "#         pred_label[id]=1\n",
    "        y_pred.append(pred_label)\n",
    "#             temp[torch.argmax(out)]=1\n",
    "        y_true.append(torch.argmax(labels))\n",
    "#         y_true,y_pred = np.array(y_true),np.array(y_pred)\n",
    "\n",
    "#     print(y_true,y_pred)\n",
    "    conf_mat = confusion_matrix(y_true,y_pred,normalize='pred')\n",
    "    print(conf_mat)\n",
    "    print(np.trace(np.array(conf_mat))/classes)\n",
    "\n",
    "def test2(t_steps,i1=0,i2=1e9):\n",
    "    colr = ['red','green']\n",
    "#     colr = ['darkred','indianred','orange','yellowgreen','lime','darkgreen']\n",
    "#     print()\n",
    "    i2 = int(min(i2,test_len-t_steps-extra_days))\n",
    "    profit = 0\n",
    "    g_cnt = 0\n",
    "    toler = 0.75\n",
    "    ry_cnt = 0\n",
    "    gy_cnt = 0\n",
    "    r_cnt = 0\n",
    "    investment = 0\n",
    "#     prev_trade = 0\n",
    "    list_trade = []\n",
    "    cnt_trade = 0\n",
    "    profit_trade = 0\n",
    "    prev = -1\n",
    "    entry = 0\n",
    "    plt.figure(figsize=(15,15))\n",
    "#     plt.plot(range(i1,i2),snp['Close'][train_len+i1+t_steps-1+extra_days:train_len+i2+t_steps-1+extra_days],color ='black',linewidth=0.5)\n",
    "    with torch.no_grad():\n",
    "        for i,data in enumerate(test_dataloader):\n",
    "            if(i<i1 or i>i2):\n",
    "                continue\n",
    "            inp,labels,pred,past = data[0],data[1],data[2],data[3]\n",
    "            encoder_outputs, (hidden, cell) = encoder(inp.repeat(batch_size,1,1))\n",
    "            out = decoder( hidden, cell, encoder_outputs)[0]\n",
    "            \n",
    "            plt.scatter(i,past,color='black',s=2,alpha=0.3)\n",
    "#             plt.scatter(i,snp['swt'][i+train_len+t_steps1-1],color='blue',s=2,alpha=0.5)\n",
    "            print(past)\n",
    "            idx = torch.argmax(out)\n",
    "\n",
    "            if(idx==0):\n",
    "                plt.fill_between(\n",
    "                    range(i,i+predict_steps),\n",
    "                    past-(10*pd1)*np.sqrt(past), \n",
    "                    past, \n",
    "                    alpha=0.5,\n",
    "                    color=colr[idx],\n",
    "                    interpolate=True,\n",
    "                    label=\"+/- 1-std\",\n",
    "                )\n",
    "            if(idx==1):\n",
    "                plt.fill_between(\n",
    "                    range(i,i+predict_steps),\n",
    "                    past, \n",
    "                    past+5*pu1*np.sqrt(past), \n",
    "                    alpha=0.5,\n",
    "                    color=colr[idx],\n",
    "                    interpolate=True,\n",
    "                    label=\"+/- 1-std\",\n",
    "                )\n",
    "            if(idx==2):\n",
    "                plt.fill_between(\n",
    "                    range(i,i+predict_steps),\n",
    "                    past+pu1*std_past, \n",
    "                    past+2*pu1*std_past, \n",
    "                    alpha=0.5,\n",
    "                    color=colr[idx],\n",
    "                    interpolate=True,\n",
    "                    label=\"+/- 1-std\",\n",
    "                )\n",
    "#             if(idx==0):\n",
    "#                 plt.fill_between(\n",
    "#                     range(i,i+predict_steps),\n",
    "#                     past*(1+2*pd1),\n",
    "#                     past*(1+pd1),\n",
    "#                     alpha=0.8,\n",
    "#                     color=colr[idx],\n",
    "#                     interpolate=True,\n",
    "#                     label=\"+/- 1-std\",\n",
    "#                 )\n",
    "#             if(idx==1):\n",
    "#                 plt.fill_between(\n",
    "#                     range(i,i+predict_steps),\n",
    "#                     past*(1+pd1),\n",
    "#                     past*(1+pd2),\n",
    "#                     alpha=0.8,\n",
    "#                     color=colr[idx],\n",
    "#                     interpolate=True,\n",
    "#                     label=\"+/- 1-std\",\n",
    "#                 )\n",
    "#             if(idx==2):\n",
    "#                 plt.fill_between(\n",
    "#                     range(i,i+predict_steps),\n",
    "#                     past*(1+pd2),\n",
    "#                     past*(1+pu1),\n",
    "#                     alpha=0.8,\n",
    "#                     color=colr[idx],\n",
    "#                     interpolate=True,\n",
    "#                     label=\"+/- 1-std\",\n",
    "#                 )\n",
    "#             if(idx==3):\n",
    "#                 plt.fill_between(\n",
    "#                     range(i,i+predict_steps),\n",
    "#                     past*(1+pu1),\n",
    "#                     past*(1+pu2),\n",
    "#                     alpha=0.8,\n",
    "#                     color=colr[idx],\n",
    "#                     interpolate=True,\n",
    "#                     label=\"+/- 1-std\",\n",
    "#                 )\n",
    "#             if(idx==4):\n",
    "#                 plt.fill_between(\n",
    "#                     range(i,i+predict_steps),\n",
    "#                     past*(1+pu2),\n",
    "#                     past*(1+pu3),\n",
    "#                     alpha=0.8,\n",
    "#                     color=colr[idx],\n",
    "#                     interpolate=True,\n",
    "#                     label=\"+/- 1-std\",\n",
    "#                 )\n",
    "#             if(idx==5):\n",
    "#                 plt.fill_between(\n",
    "#                     range(i,i+predict_steps),\n",
    "#                     past*(1+pu3),\n",
    "#                     past*(1+pu3*2),\n",
    "#                     alpha=0.8,\n",
    "#                     color=colr[idx],\n",
    "#                     interpolate=True,\n",
    "#                     label=\"+/- 1-std\",\n",
    "#                 )\n",
    "            prev=idx\n",
    "# test2(t_steps1,700,1200)\n",
    "confusion_mat2()\n",
    "# random_confusion()\n",
    "confusion_mat1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f801e887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4573290,
     "sourceId": 7808675,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
