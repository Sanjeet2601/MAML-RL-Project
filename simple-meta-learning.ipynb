{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ba1ed61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 11:00:28.335089: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-21 11:00:28.335205: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-21 11:00:28.441999: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-21 11:00:28.655294: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-21 11:00:30.161239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pywt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import talib\n",
    "from sklearn.metrics import f1_score\n",
    "import seaborn as sns\n",
    "import copy\n",
    "from torch.autograd import grad\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# writer = SummaryWriter('runs/port_opt_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08128f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install yfinance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3f38e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_data(index,start,end,freq):\n",
    "    data = yf.Ticker(index)\n",
    "    dat = data.history(interval = freq,start = start,end = end)\n",
    "    # snp.index = snp.index.map(lambda x: str(x).split(' ')[0])\n",
    "    dat['return'] = dat['Close'].pct_change().apply(lambda x:100*x)\n",
    "    dat =dat.dropna()\n",
    "    return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f944c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide_data('^GSPC','2001-01-01','2023-01-01','1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee329d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(index):\n",
    "    start,end,fre = '2001-01-01','2023-01-01','1d'\n",
    "    # snp = provide_data('RELIANCE.NS',start,end,fre)\n",
    "    snp = provide_data(index,start,end,fre)\n",
    "    snp_futures = provide_data('YM=F',start,end,fre)\n",
    "    def extract_macd(stock_prices):\n",
    "        macd, macdsignal, macdhist = talib.MACD(stock_prices, fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "        return macd,macdsignal\n",
    "    \n",
    "    res1,res2 = extract_macd(np.array(snp['Close'].values))\n",
    "    snp['Macd Signal'] = res2\n",
    "    snp['Macd'] = res1\n",
    "    # snp['Macd diff'] = res1-res2\n",
    "    snp['Simple MA'] = talib.SMA(snp['Close'],14)\n",
    "    snp['EMA'] = talib.EMA(snp['Close'], timeperiod = 14)\n",
    "    snp['upper_band'], snp['middle_band'], snp['lower_band'] = talib.BBANDS(snp['Close'], timeperiod =20)\n",
    "    snp['RSI'] = talib.RSI(snp['Close'],14) \n",
    "    snp['slowk'], snp['slowd'] = talib.STOCH(snp['High'], snp['Low'], snp['Close'], fastk_period=14, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0) \n",
    "    snp['diff_stoch'] = snp['slowk']-snp['slowd']\n",
    "    \n",
    "    # snp[['Close','Simple MA','EMA']][:300].plot(figsize=(15,15)),plt.show()\n",
    "    # snp[['Macd','Macd Signal']][:300].plot(figsize = (15,15)),plt.show()\n",
    "    # plt.figure(figsize=(15,15))\n",
    "    # plt.plot(res1[:300]-res2[:300])\n",
    "    # plt.show()\n",
    "    # snp[['Close','upper_band','middle_band','lower_band']][:300].plot(figsize=(15,15)),plt.show()\n",
    "    # snp['RSI'][:300].plot(figsize=(15,15)),plt.show()\n",
    "    # snp[['slowk','slowd']][:300].plot(figsize=(15,15)),plt.show()\n",
    "    \n",
    "    # snp.to_csv('stock_data.csv')\n",
    "    \n",
    "    cut_len = len(snp['Close'])\n",
    "    for col in snp.columns: \n",
    "        nan_indices = np.where(np.isnan(snp[col]))[0]\n",
    "    #     print(nan_indices)\n",
    "        # Remove NaN values from the array\n",
    "        arr_no_nan = np.delete(snp[col], nan_indices)\n",
    "        cut_len = int(min(cut_len,arr_no_nan.shape[0]))\n",
    "    cut_len\n",
    "    \n",
    "    snp = snp.drop(snp.index[range(len(snp['Close'])-cut_len)])\n",
    "    # print(snp.index[0],type(snp.index[0]))\n",
    "    remove_col = ['Volume','Dividends', 'Stock Splits','return']\n",
    "    col_len = len(snp.columns)-len(remove_col)\n",
    "    \n",
    "    snp = snp.drop(columns=remove_col)\n",
    "    snp\n",
    "    # autocorrelation_plot(snp['Close'])\n",
    "    result = adfuller(snp['Close'])\n",
    "    plt.plot(snp['Close'].values),plt.show()\n",
    "    print(result)\n",
    "    \n",
    "    \n",
    "    level=1\n",
    "    data_wt = snp['Close'][:int(pow(2,level))*int(len(snp['Close'])/int(pow(2,level)))]\n",
    "    cc = pywt.swt(data_wt,'db5',level = level,norm=True)\n",
    "    print(len(cc[0][1]),len(cc[0][0]),len(cc))\n",
    "    df = snp.iloc[-len(data_wt)+200:]\n",
    "    # print(df['Close'])\n",
    "    snp = df.copy()\n",
    "    snp['swt0'] = cc[0][0][200:]\n",
    "    \n",
    "    # snp['swt1'] = cc[1][0][200:]\n",
    "    # snp['swt2'] = cc[2][0][200:]\n",
    "    # snp['swt3'] = cc[3][0][200:]\n",
    "    # snp['swt4'] = cc[4][0][200:]\n",
    "    \n",
    "    snp['hf0'] = cc[0][1][200:]\n",
    "    \n",
    "    # snp['hf1'] = cc[1][1][200:]\n",
    "    # snp['hf2'] = cc[2][1][200:]\n",
    "    # snp['hf3'] = cc[3][1][200:]\n",
    "    # snp['hf4'] = cc[4][1][200:]\n",
    "    # snp['Close'] = cc[0][0][200:]a\n",
    "    \n",
    "    # print(snp['Close'],snp['swt'])\n",
    "    # plt.plot(df['Close'].values),plt.show()\n",
    "    # plt.plot(snp['Close']),plt.show()\n",
    "    snp = snp.drop(columns=['Open','High','Low'])\n",
    "    # print(snp.columns)\n",
    "    return snp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2097194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_data('IXIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c74d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_steps1 = 100\n",
    "t_steps2 = 10\n",
    "predict_steps = 1\n",
    "extra_days = 0\n",
    "batch_size = 32\n",
    "pd1 = 0.2\n",
    "pu1 = 1.2\n",
    "# pd1 = -1.1\n",
    "# pd2 = -0.3\n",
    "# pu1 = 0.2\n",
    "# pu2 = 0.7\n",
    "# pu3 = 1.3\n",
    "classes = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8512564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(\"matrix.txt\", np.array2string(, precision=2), delimiter=\"  \")\n",
    "# formatter = {'float': lambda x: f'{x:.2f}'}\n",
    "# print(np.array2string(np.corrcoef([np.array(snp[col].values) for col in snp.columns]), formatter=formatter, suppress_small=True))\n",
    "def data_transform(data,t_steps):    \n",
    "    def temp_function(data):\n",
    "#         return None\n",
    "        temp_data_list = []\n",
    "        for i in range(0,len(data)-(t_steps+predict_steps+extra_days)):\n",
    "            mi,mx = np.min(data[i:i+t_steps]),np.max(data[i:i+t_steps])\n",
    "            temp_data_list.append((data[i:i+t_steps]-mi)/(mx-mi))\n",
    "#             temp_data_list.append(data[i:i+t_steps])\n",
    "        temp_arr = np.array(temp_data_list,dtype = np.float32)\n",
    "        return temp_arr.reshape(temp_arr.shape[0],temp_arr.shape[1],-1)\n",
    "    dynamic_feat_list = [temp_function(data[col]) for col in data.columns[:] if col!='Close']\n",
    "    \n",
    "#     print(len(dynamic_feat_list))            \n",
    "    data_C = np.asarray(data['Close'],dtype = np.float32)\n",
    "    target_data_list = []\n",
    "    data_pred = []\n",
    "    labels = []\n",
    "    pst_arr = []\n",
    "    for i in range(0,len(data)-(t_steps+predict_steps+extra_days),1):\n",
    "        data_past = data_C[i:i+t_steps]\n",
    "#         print(data_C[i+t_steps])\n",
    "        # pred = np.mean(data_C[i+t_steps+extra_days:i+t_steps+extra_days+predict_steps])\n",
    "        pred = data_C[i+t_steps+predict_steps-1]\n",
    "#         print(pred)\n",
    "        data_pred.append(list([pred]))\n",
    "        std_past = np.std(data_past)\n",
    "#         past = np.mean(data_past)\n",
    "        past = data_past[-1]\n",
    "        pst_arr.append(past)\n",
    "#         pst_arr.append(data['swt'][i:i+t_steps][-1])\n",
    "#         if pred<pd1:\n",
    "#             id=0\n",
    "#         elif pred<pd2:\n",
    "#             id=1\n",
    "#         elif pred<=pu1:\n",
    "#             id=2\n",
    "#         elif pred<pu2:\n",
    "#             id=3\n",
    "#         elif pred<=pu3:\n",
    "#             id=4\n",
    "#         else:\n",
    "#             id=5\n",
    "        if pred<past:\n",
    "            id=0\n",
    "        else:\n",
    "            id=1\n",
    "        label = np.zeros((classes))\n",
    "        label[id]=1\n",
    "        labels.append(label)\n",
    "        mi,mx = np.min(data_past),np.max(data_past)\n",
    "        target_data_list.append((data_past-mi)/(mx-mi))\n",
    "#         target_data_list.append(data_past)\n",
    "    target_data_arr = np.array(target_data_list,dtype=np.float32)\n",
    "    target_data_arr = target_data_arr.reshape(target_data_arr.shape[0],target_data_arr.shape[1],-1)\n",
    "    final_dynamic_feat = np.concatenate(dynamic_feat_list,axis=2)\n",
    "#     print(final_dynamic_feat)\n",
    "    final_dataset = np.concatenate((final_dynamic_feat,target_data_arr),axis=2)\n",
    "    return final_dataset,np.array(data_pred),labels,np.array(pst_arr)\n",
    "\n",
    "# dat_tf,df_pr,dt_labels,df_past = data_transform(snp,snp_futures,t_steps1)\n",
    "# print(dat_tf.shape,dat_tf,len(dt_labels),np.count_nonzero(np.isnan(dat_tf)))\n",
    "# print(input_long.shape)\n",
    "\n",
    "# cnt = [0]*classes\n",
    "# for i in dt_labels:\n",
    "#     for j in range(classes):\n",
    "#         cnt[j]+=i[j]\n",
    "\n",
    "# cnt = cnt/sum(cnt)\n",
    "# # print(dt_labels)\n",
    "# print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33d9cfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class stock_dataset(Dataset):\n",
    "    def __init__(self,data,t_steps1):\n",
    "#         print(len(data),len(data2))\n",
    "        self.dat_long,self.pred_long,self.label_long,self.past_arr = data_transform(data,t_steps1)\n",
    "\n",
    "#         self.dat_short,self.pred_short,self.label_short = data_transform(data,data2,t_steps2)\n",
    "    def __len__(self):\n",
    "        return len(self.dat_long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.dat_long[idx][:t_steps1],self.label_long[idx],self.pred_long[idx],self.past_arr[idx]\n",
    "def prepare_dataloader(train_dataset,test_dataset):\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,batch_size = batch_size,shuffle = True,drop_last = True)\n",
    "    test_dataloader = DataLoader(test_dataset,batch_size = 1,shuffle = False,drop_last = True)\n",
    "    \n",
    "    # print(train_dataset[2],len(snp))\n",
    "    # print(test_dataset[-1])\n",
    "    return train_dataloader,test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9728f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_mat1():\n",
    "    y_pred,y_true = [],[]\n",
    "    with torch.no_grad():\n",
    "        for i,data in enumerate(test_dataloader):\n",
    "            inp,labels,pred,past = data[0],data[1],data[2],data[3]\n",
    "            out = model(inp.repeat(batch_size,1,1))[0]\n",
    "            temp = [0]*classes\n",
    "            y_pred.append(torch.argmax(out))\n",
    "            y_true.append(torch.argmax(labels))\n",
    "        \n",
    "        print(len(y_true),len(y_pred))\n",
    "        print((len(y_pred)-sum([abs(x-y) for x,y in zip(y_true,y_pred)]))/len(y_pred))\n",
    "        conf_mat = confusion_matrix(y_true,y_pred,normalize='pred')\n",
    "        print(conf_mat)\n",
    "        # print(np.trace(np.array(conf_mat))/classes)\n",
    "        print(f1_score(y_true,y_pred))\n",
    "def confusion_mat2():\n",
    "    y_pred,y_true = [],[]\n",
    "    with torch.no_grad():\n",
    "        for i,data in enumerate(test_dataloader):\n",
    "            inp,labels,pred = data[0],data[1],data[2]\n",
    "            # out = model(inp.repeat(batch_size,1,1))[0]\n",
    "            encoder_outputs, (hidden, cell) = encoder(inp.repeat(1,1,1))\n",
    "          # input = trg[:-1]\n",
    "            out,reg,wts = decoder( hidden, cell, encoder_outputs)\n",
    "            temp = [0]*classes\n",
    "#             temp[torch.argmax(out)]=1\n",
    "            y_pred.append(torch.argmax(out))\n",
    "            y_true.append(torch.argmax(labels))\n",
    "            if i%100==0:\n",
    "                # print(wts)\n",
    "                sns.heatmap(wts[0])\n",
    "                plt.show()\n",
    "        y_true,y_pred = np.array(y_true),np.array(y_pred)\n",
    "\n",
    "        print(len(y_true),len(y_pred))\n",
    "        print((len(y_pred)-sum([abs(x-y) for x,y in zip(y_true,y_pred)]))/len(y_pred))\n",
    "        conf_mat = confusion_matrix(y_true,y_pred,normalize='pred')\n",
    "        print(conf_mat)\n",
    "        # print(np.trace(np.array(conf_mat))/classes)\n",
    "        print(f1_score(y_true,y_pred))\n",
    "        \n",
    "class stock_lstm2(torch.nn.Module):\n",
    "    def __init__(self,input_dim,output_dim,batch_size,t_steps1,t_steps2,classes):\n",
    "        super(stock_lstm,self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.in_dim = input_dim\n",
    "        self.out_dim = output_dim\n",
    "        self.nlayers = 6\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size = input_dim,hidden_size = output_dim,num_layers = self.nlayers,batch_first = True)\n",
    "        self.lstm2 = nn.LSTM(input_size = input_dim,hidden_size = output_dim,num_layers = self.nlayers,batch_first = True)\n",
    "\n",
    "        self.ln_long = nn.LayerNorm((t_steps1,self.in_dim))\n",
    "        self.ln_short = nn.LayerNorm((t_steps2,self.in_dim))\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.soft = nn.Softmax()\n",
    "\n",
    "        self.fc1_long = nn.Linear(t_steps*self.in_dim,t_steps*self.in_dim//2)\n",
    "        self.fc2_long = nn.Linear(t_steps*self.in_dim//2,classes)\n",
    "        self.fc1_short = nn.Linear(t_steps2*self.in_dim,t_steps2*self.in_dim//2)\n",
    "        self.fc2_short = nn.Linear(t_steps2*self.in_dim//2,classes)\n",
    "        self.fc3 = nn.Linear(2*classes,classes)\n",
    "\n",
    "    def forward(self,input_long,input_short):\n",
    "        input_long = input_long.view(self.batch_size,-1,self.in_dim)\n",
    "        input_short = input_short.view(self.batch_size,-1,self.in_dim)\n",
    "        h_0_long = input_long[:][:,0].view(1,self.batch_size,-1).repeat(self.nlayers,1,1)\n",
    "        h_0_short = input_short[:][:,0].view(1,self.batch_size,-1).repeat(self.nlayers,1,1)\n",
    "        input_long = self.ln_long(input_long)\n",
    "        input_short = self.ln_short(input_short)\n",
    "\n",
    "#         lstm_out,_ = self.lstm1(input_series,(h_0,torch.zeros(h_0.shape)))\n",
    "        lstm_long,_ = self.lstm1(input_long)\n",
    "        lstm_long = self.ln_long(lstm_long)\n",
    "        lstm_long = self.tanh(lstm_long)\n",
    "        lstm_long = lstm_long.view(self.batch_size,-1)\n",
    "\n",
    "        lstm_short,_ = self.lstm1(input_short)\n",
    "        lstm_short = self.ln_short(lstm_short)\n",
    "        lstm_short = self.tanh(lstm_short)\n",
    "        lstm_short = lstm_short.view(self.batch_size,-1)\n",
    "\n",
    "        out_long = self.fc1_long(lstm_long)\n",
    "        out_long = self.sig(out_long)\n",
    "        out_long = self.fc2_long(out_long)\n",
    "        out_long = self.soft(out_long)\n",
    "\n",
    "        out_short = self.fc1(lstm_short)\n",
    "        out_short = self.sig(out_short)\n",
    "        out_short = self.fc2(out_short)\n",
    "        out_short = self.soft(out_short)\n",
    "\n",
    "        out = self.fc3(np.hstack(out_long,out_short))\n",
    "        out = self.soft(out)\n",
    "#         print(input_series[0],lstm_out[0],out_pre[0])\n",
    "#         out_final = self.act2(out_pre)\n",
    "#         out_final = self.fc2(out_pre)\n",
    "        return out\n",
    "class stock_lstm(torch.nn.Module):\n",
    "    def __init__(self,input_dim,d_model,n_layers,batch_size,t_steps,classes):\n",
    "        super(stock_lstm,self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.in_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.nlayers = n_layers\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size = input_dim,hidden_size = d_model,num_layers = self.nlayers,batch_first = True)\n",
    "\n",
    "        # self.input_ln = nn.LayerNorm((t_steps,self.in_dim))\n",
    "        self.lstm_ln = nn.LayerNorm((t_steps,self.d_model))\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.soft = nn.Softmax()\n",
    "\n",
    "        self.fc1_long = nn.Linear(t_steps*self.d_model,t_steps*self.d_model//2)\n",
    "        self.fc2_long = nn.Linear(t_steps*self.d_model//2,t_steps*self.d_model//4)\n",
    "        self.fc3_long = nn.Linear(t_steps*self.d_model//4,classes)\n",
    "\n",
    "    def forward(self,input_long):\n",
    "#         print(input_long.shape,input_long[0])\n",
    "#         input_long = np.transpose(input_long,(0,2,1))\n",
    "#         h_0_long = input_long[:][:,0].view(1,self.batch_size,-1).repeat(self.nlayers,1,1)\n",
    "#         input_long = self.input_ln(input_long)\n",
    "#         print(torch.mean(input_long,dim=(1,2)))\n",
    "        lstm_long,_ = self.lstm1(input_long)\n",
    "        lstm_long = self.lstm_ln(lstm_long)\n",
    "        lstm_long = self.tanh(lstm_long)\n",
    "        lstm_long = lstm_long.reshape(lstm_long.shape[0],-1)\n",
    "\n",
    "        out_long = self.fc1_long(lstm_long)\n",
    "        out_long = self.sig(out_long)\n",
    "        out_long = self.fc2_long(out_long)\n",
    "        # out_long = self.soft(out_long)\n",
    "        out_long = self.sig(out_long)\n",
    "        out_long = self.fc3_long(out_long)\n",
    "        out  = self.soft(out_long)\n",
    "\n",
    "        return out\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, t_steps):\n",
    "        super().__init__()\n",
    "        self.d_model = hid_dim\n",
    "        self.t_steps= t_steps\n",
    "        self.input_dim = input_dim\n",
    "        self.nlayers = n_layers\n",
    "        self.rnn = nn.LSTM(input_dim, hid_dim, n_layers, dropout=0.5,batch_first = True)\n",
    "        self.rnncell = nn.LSTMCell(input_dim,hid_dim)\n",
    "#         self.rnn2 = nn.LSTM(input_dim,hid_dim,n_layers,dropout=0.2,batch_first=True)\n",
    "        self.ln = nn.LayerNorm((t_steps,input_dim))\n",
    "        self.lno = nn.LayerNorm((t_steps,hid_dim))\n",
    "        self.Q = nn.Linear(hid_dim,hid_dim)\n",
    "        self.K = nn.Linear(hid_dim,hid_dim)\n",
    "        self.Q2 = nn.Linear(input_dim,input_dim)\n",
    "        self.K2 = nn.Linear(hid_dim,input_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "    def feature_attn(self,inp,h):\n",
    "        inp = inp[:,None,:]\n",
    "        h = h[:,None,:]\n",
    "        Q2 = inp\n",
    "        K2 = self.K2(h)\n",
    "        \n",
    "        wts = F.softmax(torch.matmul(torch.transpose(Q2,1,2),K2),dim=-1)\n",
    "#         print(Q2.shape,K2.shape,wts.shape)\n",
    "        out = torch.matmul(inp,wts.transpose(1,2))\n",
    "        out = out[:,0,:]\n",
    "#         print(out.shape)\n",
    "        return out,wts\n",
    "    def attn(self,Q,K):\n",
    "        # Q = self.Q(Qs)\n",
    "        # K = self.K(Ks)\n",
    "        wts = F.softmax(torch.matmul(Q,torch.transpose(K,1,2)),dim=-1)\n",
    "        output = torch.matmul(wts,Q)\n",
    "        return output,wts\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # inp_mi = torch.tensor(torch.min(src,dim=1)[0])[:,None,:]\n",
    "        # inp_mx = torch.tensor(torch.max(src,dim=1)[0])[:,None,:]\n",
    "        # src = (src -inp_mi)/(inp_mx-inp_mi)\n",
    "        # src = self.ln(src)\n",
    "#         h0 = np.sum(src[:,0,:],axis=)\n",
    "        \n",
    "#         h0 = torch.zeros((src.shape[0],self.d_model))\n",
    "#         c0=torch.zeros((src.shape[0],self.d_model))\n",
    "#         output = []\n",
    "#         # cells = []\n",
    "#         for i in range(self.t_steps):\n",
    "#             inp = src[:,i,:]\n",
    "#             inp,wtsf = self.feature_attn(inp,h0)\n",
    "# #             inp = self.ln(inp)\n",
    "#             h0,c0 = self.rnncell(inp,(h0,c0))\n",
    "#             h0,c0 = self.relu(h0),self.relu(c0)\n",
    "# #             print(h0.shape)\n",
    "#             output.append(h0)\n",
    "#             # cells.append(c0)\n",
    "#         output = torch.stack(output,dim=1)\n",
    "#         # cells = torch.stack(cells,dim=1)\n",
    "#         output,wts = self.attn(output,output)\n",
    "#         output = self.lno(output)\n",
    "#         # cells,wts = self.attn(cells,output)\n",
    "#         hidden,cell = output[:,-1,:],c0\n",
    "        # return outputs,(hidden, cell),wtsf\n",
    "        \n",
    "#         hidden,cell = h0[:,None,:],c0[:,None,:]\n",
    "\n",
    "\n",
    "        # Q2 = self.Q2(src)\n",
    "        # K2 = self.K2(src)\n",
    "        # wts2 = F.softmax(torch.matmul(torch.transpose(Q2,1,2),K2),dim=-1)\n",
    "        # src2 = torch.matmul(wts2,src.transpose(1,2)).transpose(1,2)\n",
    "        # src2 = feature_attn(src,)\n",
    "        # src2 = self.ln(src2)\n",
    "        outputs, (hidden, cell) = self.rnn(src)\n",
    "        outputs = self.tanh(outputs)\n",
    "        hidden = hidden.transpose(0,1)\n",
    "        cell = cell.transpose(0,1)\n",
    "        hidden = hidden[:,0,:]\n",
    "        cell = cell[:,0,:]\n",
    "        # Q = self.Q(outputs)\n",
    "        # K = self.K(outputs)\n",
    "        # wts = F.softmax(torch.matmul(Q,torch.transpose(K,1,2)),dim=-1)\n",
    "        # output = torch.matmul(wts,outputs)\n",
    "\n",
    "#         print(output.shape,wts.shape,hidden.shape)\n",
    "        return outputs,(hidden, cell)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim,hid_dim, n_layers, p_steps,t_steps):\n",
    "        super().__init__()\n",
    "        # self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.psteps = p_steps\n",
    "#         self.rnn = nn.LSTM(hid_dim , hid_dim, n_layers, dropout=0.5)\n",
    "        self.ln = nn.LayerNorm((t_steps,hid_dim))\n",
    "        self.rnncell = nn.LSTMCell(hid_dim,hid_dim)\n",
    "        self.out = nn.Linear(hid_dim, output_dim)\n",
    "        self.Qc = nn.Linear(hid_dim,hid_dim)\n",
    "        self.Kc = nn.Linear(hid_dim,hid_dim)\n",
    "        self.Qs = nn.Linear(hid_dim,hid_dim)\n",
    "        self.Ks = nn.Linear(hid_dim,hid_dim)\n",
    "        # self.fc1 = nn.Linear(n_layers*hid_dim,hid_dim)\n",
    "        self.fc1 = nn.Linear(hid_dim,output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.soft = nn.Softmax()\n",
    "        self.logsoft = nn.LogSoftmax()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(hid_dim,hid_dim//2)\n",
    "        self.fc3 = nn.Linear(hid_dim//2,output_dim)\n",
    "        self.rfc = nn.Linear(hid_dim,hid_dim//2)\n",
    "        self.rfc2 = nn.Linear(hid_dim//2,hid_dim//4)\n",
    "        self.rfc3 = nn.Linear(hid_dim//4,1)\n",
    "    def cross_attn(self,Qs,Ks):\n",
    "        if len(Ks.shape)<3:\n",
    "            Ks = Ks[:,None,:]\n",
    "        Q = Qs\n",
    "        K = Ks\n",
    "        wts = F.softmax(torch.matmul(Q,torch.transpose(K,1,2)),dim=-2)\n",
    "        output = torch.matmul(wts.transpose(1,2),Qs)\n",
    "        output = output[:,0,:]\n",
    "        return output,wts\n",
    "    def self_attn(self,Qs,Ks):\n",
    "        if len(Ks.shape)<3:\n",
    "            Ks = Ks[:,None,:]\n",
    "        Q = Qs\n",
    "        K = Ks\n",
    "        wts = F.softmax(torch.matmul(Q,torch.transpose(K,1,2)),dim=-2)\n",
    "        output = torch.matmul(wts.transpose(1,2),Qs)\n",
    "        output = output[:,0,:]\n",
    "        return output,wts\n",
    "    def forward(self, hidden, cell, encoder_outputs):\n",
    "        # print(encoder_outputs.shape,hidden.transpose(1,0).shape)\n",
    "        # query = self.Q(encoder_outputs.reshape(encoder_outputs.shape[0],-1))\n",
    "        # query = query.reshape(query.shape[0],-1,query.shape[1]).transpose(2,1)\n",
    "        # query = query.transpose(2,1)\n",
    "#         hidden = hidden.transpose(1,0)\n",
    "#         scores = torch.matmul(hidden,query)\n",
    "#         attention_weights = torch.softmax(scores,dim=1)\n",
    "#         value = torch.mul(hidden,attention_weights)\n",
    "#         out = self.fc1(value.reshape(value.shape[0],-1))\n",
    "#         prediction = self.soft(out)\n",
    "#         print(encoder_outputs.shape,hidden.transpose(1,0).shape)\n",
    "#         encoder_outputs = self.Q(encoder_outputs)\n",
    "#         scores = torch.matmul(hidden.transpose(1,0),encoder_outputs.transpose(2,1))\n",
    "#         attention_weights = torch.softmax(scores,dim=-1)\n",
    "# #         print(scores.shape,attention_weights.shape,attention_weights)\n",
    "#         value = torch.matmul(attention_weights,encoder_outputs)\n",
    "#         value = self.V(value)\n",
    "#         value = value.reshape(value.shape[0],-1)\n",
    "#         out = self.sig(value)\n",
    "#         out = self.fc1(out)\n",
    "#         out = self.sig(out)\n",
    "#         out = self.fc2(out)\n",
    "#         prediction = self.soft(out)\n",
    "\n",
    "        \n",
    "        encoder_outputs = encoder_outputs\n",
    "        h0,c0 = hidden,cell\n",
    "        a1,wts = self.cross_attn(encoder_outputs,h0)\n",
    "        \n",
    "        # h0 = h0[:,0,:]\n",
    "        # c0 = c0[:,0,:]\n",
    "        # print(h0.shape)\n",
    "        h0,c0 = self.rnncell(h0,(a1,c0))\n",
    "        h0,c0 = self.tanh(h0),self.tanh(c0)\n",
    "        hiddens = h0[:,None,:]\n",
    "        for i in range(self.psteps-1):\n",
    "            a1,_ = self.cross_attn(encoder_outputs,h0)\n",
    "            a2,_ = self.cross_attn(hiddens,h0)\n",
    "            h0,c0 = self.rnncell(h0,(a1+a2,c0))\n",
    "            h0,c0 = self.relu(h0),self.relu(c0)\n",
    "            hiddens = torch.cat((hiddens,h0[:,None,:]),1)\n",
    "        out = self.fc1(h0)\n",
    "        # out = self.sig(out)\n",
    "        # out = self.fc2(out)\n",
    "        # out = self.sig(out)\n",
    "        # out = self.fc3(out)\n",
    "        prediction = self.soft(out)\n",
    "        reg = self.rfc(h0)\n",
    "        reg = self.sig(reg)\n",
    "        reg = self.rfc2(reg)\n",
    "        reg = self.sig(reg)\n",
    "        reg = self.rfc3(reg)\n",
    "        reg = self.tanh(reg)\n",
    "        \n",
    "        return prediction,reg,wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17c29fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_steps1 = 20\n",
    "predict_steps = 1\n",
    "batch_size = 64\n",
    "classes = 2\n",
    "stock = '^IRX'\n",
    "\n",
    "# import gspc_train_20_5.pt\n",
    "def fetch(stock,split,t_steps,p_steps):\n",
    "    file = '{}_{}_{}_{}.pt'.format(stock,split,t_steps,p_steps)\n",
    "    return torch.load(file)\n",
    "def prepare_task(stock,t_steps1,predict_steps,classes):\n",
    "    try:\n",
    "        nme = re.sub(r'[^a-zA-Z]', '', stock).lower()\n",
    "        train_dataset = fetch(nme,'train',t_steps1,predict_steps)\n",
    "        test_dataset = fetch(nme,'test',t_steps1,predict_steps)\n",
    "        \n",
    "    except:\n",
    "        print('file not found . starting to prepare')\n",
    "        index = prepare_data(stock)\n",
    "        train_len = int(0.85*len(index))\n",
    "        test_len = len(index)-train_len\n",
    "        \n",
    "        train_dataset = stock_dataset(index[:train_len],t_steps1)\n",
    "        test_dataset = stock_dataset(index[-(len(index)-train_len):],t_steps1)\n",
    "        nme = re.sub(r'[^a-zA-Z]', '', stock).lower()\n",
    "        torch.save(train_dataset,'./{}_train_{}_{}.pt'.format(nme,t_steps1,predict_steps))\n",
    "        torch.save(test_dataset,'./{}_test_{}_{}.pt'.format(nme,t_steps1,predict_steps))\n",
    "    in_dim = train_dataset.dat_long.shape[2]\n",
    "    out_dim = in_dim\n",
    "    cnt = [0]*classes\n",
    "    for i in train_dataset.label_long:\n",
    "        for j in range(classes):\n",
    "            cnt[j]+=i[j]\n",
    "    cnt/=sum(cnt)\n",
    "    print(cnt,'cnt')\n",
    "    \n",
    "    train_dataloader,test_dataloader = prepare_dataloader(train_dataset,test_dataset)\n",
    "    return train_dataloader,test_dataloader,cnt\n",
    "# print(cnt)\n",
    "# confusion_mat2()\n",
    "# confusion_mat1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f233d5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file not found . starting to prepare\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.32851090080687073, 0.9786153954937146, 27, 5473, {'1%': -3.4315453900435435, '5%': -2.8620682431852114, '10%': -2.56705118276025}, 50508.99996352711)\n",
      "5500 5500 1\n",
      "[0.45896521 0.54103479] cnt\n",
      "file not found . starting to prepare\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "^IRX: No price data found, symbol may be delisted (1d 2001-01-01 -> 2023-01-01)\n"
     ]
    }
   ],
   "source": [
    "task1 = prepare_task('^GSPC',t_steps1,predict_steps,classes)\n",
    "task2 = prepare_task('^IRX',t_steps1,predict_steps,classes)\n",
    "task3 = prepare_task('^IXIC',t_steps1,predict_steps,classes)\n",
    "task4 = prepare_task('DJI',t_steps1,predict_steps,classes)\n",
    "task5 = prepare_task('CL=F',t_steps1,predict_steps,classes)\n",
    "in_dim = task1[0].dataset.dat_long.shape[2]\n",
    "out_dim=in_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62bdc656",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:48:40.295572Z",
     "iopub.status.busy": "2024-03-12T09:48:40.295328Z",
     "iopub.status.idle": "2024-03-12T09:48:40.299795Z",
     "shell.execute_reply": "2024-03-12T09:48:40.298901Z",
     "shell.execute_reply.started": "2024-03-12T09:48:40.295552Z"
    }
   },
   "outputs": [],
   "source": [
    "# in_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "644fcd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_layers = 2\n",
    "d_model = 32\n",
    "encoder = Encoder(in_dim, d_model, n_layers,t_steps1)\n",
    "decoder = Decoder(classes, d_model, n_layers, predict_steps,t_steps1)\n",
    "\n",
    "# Define the optimizer and criterion\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "400474de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext tensorboard\n",
    "# dataiter = iter(train_dataloader)\n",
    "# src,trg,pred,past = next(dataiter)\n",
    "# print(src,src.shape)\n",
    "# out,(hid,cel) = encoder(src)\n",
    "# writer.add_graph(encoder,src)\n",
    "# writer.add_graph(decoder,(hid,cel,out))\n",
    "# writer.close()\n",
    "# writer.flush()\n",
    " \n",
    "# %tensorboard --logdir=runs\n",
    "# next(iter(task1[0]))\n",
    "# for k,p in model.named_parameters():\n",
    "#     print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46158e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 2\n",
    "d_model = 16\n",
    "model = stock_lstm(in_dim,d_model,n_layers,batch_size,t_steps1,classes)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5747a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chkpt = torch.load('stock_lstm_meta_12345_0shots.pt')\n",
    "# model.load_state_dict(chkpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92b2abe6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def diffout():\n",
    "def model_test(test_dataloader,model):\n",
    "    y_pred,y_true = [],[]\n",
    "    for i,test_data in enumerate(test_dataloader):\n",
    "        src=test_data[0]\n",
    "        trg = test_data[1]\n",
    "        pred=test_data[2][:,0]\n",
    "        past=test_data[3]\n",
    "        output = model(src)\n",
    "#         if i%50==0:\n",
    "#             print(criterion1(output,trg))\n",
    "        y_pred.append(torch.argmax(output))\n",
    "        y_true.append(torch.argmax(trg))\n",
    "    print(f1_score(y_true,y_pred))\n",
    "    \n",
    "def meta_test(model,task_list,n_shots=0):\n",
    "    \n",
    "    for task in task_list:\n",
    "        criterion1 = nn.CrossEntropyLoss(weight = torch.tensor(task[2]))\n",
    "        tmodel = copy.deepcopy(model)\n",
    "        optimizer = torch.optim.Adam(tmodel.parameters())\n",
    "        test_dataloader =  task[1]\n",
    "        itr = iter(test_dataloader)\n",
    "        for _ in range(n_shots):\n",
    "            test_data = next(itr)\n",
    "            src=test_data[0]\n",
    "            trg = test_data[1]\n",
    "            pred=test_data[2][:,0]\n",
    "            past=test_data[3]\n",
    "            output = tmodel(src)\n",
    "            loss = criterion1(output,trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        tmodel.eval()\n",
    "        model_test(test_dataloader,tmodel)\n",
    "\n",
    "\n",
    "def meta_train(epochs,task_list,n_shots=0):\n",
    "    loss_list = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()))\n",
    "    # optimizer = torch.optim.Adam(model.parameters())\n",
    "    # criterion1 = nn.NLLLoss(weight=torch.tensor(cnt))\n",
    "    # criterion2 = nn.MSELoss()\n",
    "    matches,length = 0,0\n",
    "    inner_lr = 0.01\n",
    "    meta_lr=0.001\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        grads = []\n",
    "        for task in task_list:\n",
    "            \n",
    "            train_dataloader = task[0]\n",
    "            criterion1 = nn.CrossEntropyLoss(weight = torch.tensor(task[2]))\n",
    "            itr = iter(train_dataloader)\n",
    "            fmodel = stock_lstm(in_dim,d_model,n_layers,batch_size,t_steps1,classes)\n",
    "            fmodel.load_state_dict(model.state_dict())\n",
    "            fast_wts = dict((name,param) for (name,param) in fmodel.named_parameters())\n",
    "            loss=0\n",
    "            # print('fast_wts',fast_wts[0])\n",
    "            for _ in range(n_shots):\n",
    "                print('ho gaya')\n",
    "                train_data = next(itr)\n",
    "                src = train_data[0]\n",
    "                trg = train_data[1]\n",
    "                pred = train_data[2][:,0]\n",
    "                past = train_data[3]\n",
    "                \n",
    "                # encoder_outputs, (hidden, cell) = fenc(src)\n",
    "                # output,reg,wts = fdec( hidden, cell, encoder_outputs)\n",
    "                output = fmodel(src)\n",
    "                # reg = reg[:,0]\n",
    "                ls = criterion1(output,trg)\n",
    "                gd = grad(ls,fmodel.parameters())\n",
    "                # foptimizer.step()\n",
    "                fast_wts = dict((name, param - inner_lr * g) for ((name, param), g) in zip(fast_wts.items(), gd))\n",
    "                fmodel.load_state_dict(fast_wts)\n",
    "\n",
    "            # iid=0\n",
    "            # print(len(fast_wts))\n",
    "            # for i,p in enumerate(fmodel.parameters()):\n",
    "            #     p=fast_wts[i]\n",
    "            #     # sz = np.prod(list(p.shape))\n",
    "            #     # # print(iid,sz)\n",
    "            #     # print('fast weights',fast_wts[iid:iid+sz])\n",
    "            #     # p.data = torch.tensor(fast_wts[iid:iid+sz]).view_as(p)\n",
    "            #     # iid+=sz\n",
    "            for i,train_data in enumerate(train_dataloader):\n",
    "                src = train_data[0]\n",
    "                # print(i,src[0].shape,src[0])\n",
    "                # break\n",
    "                trg = train_data[1]\n",
    "                pred = train_data[2][:,0]\n",
    "                past = train_data[3]\n",
    "                # optimizer.zero_grad()\n",
    "                # encoder_outputs, (hidden, cell) = encoder(src)\n",
    "    \n",
    "                # # input = trg[:-1]\n",
    "                # output,reg,wts = decoder( hidden, cell, encoder_outputs)\n",
    "                # reg = reg[:,0]\n",
    "                output = fmodel(src)\n",
    "                \n",
    "                loss += criterion1(output, trg)/len(task_list)\n",
    "                # print(grad(loss,model.parameters(),retain_graph=True,allow_unused=True))\n",
    "                # 'kn;\n",
    "                # break\n",
    "                # print(reg.shape)\n",
    "                # l2 = criterion2(pred,past*reg)/(batch_size*pow(10,torch.log10(past[-1])))\n",
    "                # loss = l2+l1\n",
    "                # loss = l1\n",
    "                \n",
    "                # length+=batch_size\n",
    "                if i%8000==0:\n",
    "                    # print(epoch,l2,\"out >>>> \",output[:1],\"\\nlabel >>>> \",train_data[1][:1])\n",
    "                    # print(epoch,loss,past[-1],past[-1]*(1+reg[-1]),l2,output[:1],train_data[1][:1],l1)\n",
    "                    print(loss,output.shape,trg.shape,output[0,:],'>>>>>>>>>>>>>>>>.',trg[0,:])\n",
    "                    # if epoch%20==0:\n",
    "                    #     sns.heatmap(wts[0].detach().numpy())\n",
    "                    #     plt.show()\n",
    "            g = torch.autograd.grad(loss, fmodel.parameters(), create_graph=True)\n",
    "            meta_grads = {name:g for ((name, _), g) in zip(fmodel.named_parameters(), g)}\n",
    "            grads.append(meta_grads)\n",
    "                # loss_list.append(loss.detach().numpy())\n",
    "        output = model(src)\n",
    "        loss = criterion(output,trg)\n",
    "        loss.backward(retain_graph=True)\n",
    "        # print(grads)\n",
    "        gradients = {k: sum(d[k] for d in grads) for k in grads[0].keys()}\n",
    "        hooks = []\n",
    "        for(k,v) in model.named_parameters():\n",
    "            def get_closure():\n",
    "                key = k\n",
    "                def replace_grad(grad):\n",
    "                    return gradients[key]\n",
    "                return replace_grad\n",
    "            hooks.append(v.register_hook(get_closure()))\n",
    "        torch.optim.Adam(model.parameters(), lr=meta_lr).zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.optim.Adam(model.parameters(), lr=meta_lr).step()\n",
    "\n",
    "        # Remove the hooks before next training phase\n",
    "        for h in hooks:\n",
    "            h.remove()\n",
    "    for task in task_list:\n",
    "        test_dataloader = task[1]\n",
    "        for i,test_data in enumerate(test_dataloader):\n",
    "            src=test_data[0]\n",
    "            trg = test_data[1]\n",
    "            pred=test_data[2][:,0]\n",
    "            past=test_data[3]\n",
    "            # encoder_outputs, (hidden, cell) = encoder(src)\n",
    "\n",
    "            # # input = trg[:-1]\n",
    "            # output,reg,wts = decoder( hidden, cell, encoder_outputs)\n",
    "            output = model(src)\n",
    "            if i%50==0:\n",
    "                print(criterion1(output,trg))\n",
    "            # optimizer.zero_grad()\n",
    "                \n",
    "    #             break\n",
    "    # print('train_acc = ',matches/length)\n",
    "    return loss_list\n",
    "n_shots = 0\n",
    "task_list = [task1,task2,task3,task4,task5]\n",
    "# loss_list = meta_train(500,task_list,n_shots=n_shots)\n",
    "# confusion_mat2()\n",
    "# writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b3d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'state_dict':model.state_dict()},'stock_lstm_meta_12345_shots.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da6adb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:48:19.039790Z",
     "iopub.status.busy": "2024-03-12T09:48:19.039499Z",
     "iopub.status.idle": "2024-03-12T09:48:19.062572Z",
     "shell.execute_reply": "2024-03-12T09:48:19.061394Z",
     "shell.execute_reply.started": "2024-03-12T09:48:19.039769Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chkpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/working/stock_lstm_meta_1234_1shots.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(chkpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "chkpt = torch.load('/kaggle/working/stock_lstm_meta_1234_1shots.pt')\n",
    "model.load_state_dict(chkpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35bffa5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T09:55:34.120209Z",
     "iopub.status.busy": "2024-03-12T09:55:34.119888Z",
     "iopub.status.idle": "2024-03-12T09:55:36.094955Z",
     "shell.execute_reply": "2024-03-12T09:55:36.094058Z",
     "shell.execute_reply.started": "2024-03-12T09:55:34.120183Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8289611752360966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6054644808743169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8380355276907002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8327796234772978\n"
     ]
    }
   ],
   "source": [
    "meta_test(model,task_list,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "83d4d5cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-11T09:56:44.166606Z",
     "iopub.status.busy": "2024-03-11T09:56:44.166073Z",
     "iopub.status.idle": "2024-03-11T09:56:46.551806Z",
     "shell.execute_reply": "2024-03-11T09:56:46.550908Z",
     "shell.execute_reply.started": "2024-03-11T09:56:44.166581Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7211961301671065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7347972972972971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7425569176882661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7226415094339623\n"
     ]
    }
   ],
   "source": [
    "for task in task_list:\n",
    "    y_pred,y_true = [],[]\n",
    "    test_dataloader = task[1]\n",
    "    for i,test_data in enumerate(test_dataloader):\n",
    "        src=test_data[0]\n",
    "        trg = test_data[1]\n",
    "        pred=test_data[2][:,0]\n",
    "        past=test_data[3]\n",
    "        output = model(src)\n",
    "#         if i%50==0:\n",
    "#             print(criterion1(output,trg))\n",
    "        y_pred.append(torch.argmax(output))\n",
    "        y_true.append(torch.argmax(trg))\n",
    "    print(f1_score(y_true,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c17bbf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train2(epochs):\n",
    "    loss_list = []\n",
    "    criterion1 = nn.CrossEntropyLoss(weight=torch.tensor(cnt))\n",
    "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()))\n",
    "    # criterion1 = nn.NLLLoss(weight=torch.tensor(cnt))\n",
    "    criterion2 = nn.MSELoss()\n",
    "    matches,length = 0,0\n",
    "    for epoch in range(epochs):\n",
    "        for i,train_data in enumerate(train_dataloader):\n",
    "            src = train_data[0]\n",
    "            trg = train_data[1]\n",
    "            pred = train_data[2][:,0]\n",
    "            past = train_data[3]\n",
    "            optimizer.zero_grad()\n",
    "            encoder_outputs, (hidden, cell) = encoder(src)\n",
    "\n",
    "            # input = trg[:-1]\n",
    "            output,reg,wts = decoder( hidden, cell, encoder_outputs)\n",
    "            reg = reg[:,0]\n",
    "            # output_dim = output.shape[-1]\n",
    "            # output = output.view(-1, output_dim)\n",
    "            # trg = trg[1:].view(-1)\n",
    "            \n",
    "            # print(output.shape,trg.shape,pred.shape,past.shape,reg.shape)\n",
    "            # print(trg)\n",
    "            # trg = torch.argmax(trg,dim=1).float()\n",
    "            # trg = trg.float()\n",
    "            # print(trg[0],type(trg[0].float()))\n",
    "            l1 = criterion1(output, trg)\n",
    "            # print(reg.shape)\n",
    "            l2 = criterion2(pred,past*reg)/(batch_size*pow(10,torch.log10(past[-1])))\n",
    "            loss = l2+l1\n",
    "            # writer.add_scalar(\"Loss1/train\", l1, i+epoch*len(train_dataloader))\n",
    "            # writer.add_scalar(\"Loss2/train\", l2, i+epoch*len(train_dataloader))\n",
    "            # matches+=batch_size-sum(np.argmax(output.detach().numpy(),axis=1)-np.argmax(trg.numpy(),axis=1))\n",
    "            length+=batch_size\n",
    "            if i%8000==0:\n",
    "                # print(epoch,l2,\"out >>>> \",output[:1],\"\\nlabel >>>> \",train_data[1][:1])\n",
    "                print(epoch,loss,past[-1],past[-1]*(1+reg[-1]),l2,output[:1],train_data[1][:1],l1)\n",
    "                if epoch%20==0:\n",
    "                    sns.heatmap(wts[0].detach().numpy())\n",
    "                    plt.show()\n",
    "\n",
    "\n",
    "            loss_list.append(loss.detach().numpy())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "#             break\n",
    "    print('train_acc = ',matches/length)\n",
    "    return loss_list\n",
    "loss_list = train2(200)\n",
    "confusion_mat2()\n",
    "# writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb25bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_list[-20000:])\n",
    "# torch.save({\n",
    "#             'enc_state_dict': encoder.state_dict(),\n",
    "#             'dec_state_dict': decoder.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "\n",
    "\n",
    "#             }, 'stock_classifier_enc_dec.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f762c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 2\n",
    "d_model = 16\n",
    "model = stock_lstm(in_dim,d_model,n_layers,batch_size,t_steps1,classes)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# try:\n",
    "#     checkpoint = torch.load('stock_classifier_lstm.pt')\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# except:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "676834b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhss20/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.3419, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.4107, 0.5893]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "1 tensor(0.3537, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.2912, 0.7088]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "2 tensor(0.3332, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.3349, 0.6651]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "3 tensor(0.3459, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.3528, 0.6472]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "4 tensor(0.3470, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.3611, 0.6389]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "5 tensor(0.3435, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.3035, 0.6965]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "6 tensor(0.2222, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.0215, 0.9785]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "7 tensor(0.2134, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.9735, 0.0265]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "8 tensor(0.2381, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.0142, 0.9858]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "9 tensor(0.2336, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.6306, 0.3694]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "10 tensor(0.2093, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.8004, 0.1996]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "11 tensor(0.2620, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.0050, 0.9950]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "12 tensor(0.2287, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.0020, 0.9980]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "13 tensor(0.2453, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.0012, 0.9988]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "14 tensor(0.2074, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.0012, 0.9988]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "15 tensor(0.2171, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.9988, 0.0012]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "16 tensor(0.2021, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.9980, 0.0020]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "17 tensor(0.2299, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.0131, 0.9869]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "18 tensor(0.2076, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9915e-01, 8.4599e-04]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "19 tensor(0.1924, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.5603, 0.4397]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "20 tensor(0.2245, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[4.9631e-04, 9.9950e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "21 tensor(0.1839, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.9989, 0.0011]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "22 tensor(0.2082, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9958e-01, 4.1828e-04]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "23 tensor(0.1922, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.7311e-04, 9.9973e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "24 tensor(0.1902, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.9981, 0.0019]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "25 tensor(0.2006, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.0228e-04, 9.9980e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "26 tensor(0.2162, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.3928e-04, 9.9976e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "27 tensor(0.2219, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.8806e-04, 9.9981e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "28 tensor(0.1853, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.9901, 0.0099]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "29 tensor(0.2092, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9980e-01, 2.0159e-04]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "30 tensor(0.1924, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.8864, 0.1136]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "31 tensor(0.2198, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.7256, 0.2744]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "32 tensor(0.1804, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.0066, 0.9934]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "33 tensor(0.1992, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.9971, 0.0029]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "34 tensor(0.2181, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[8.3335e-05, 9.9992e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "35 tensor(0.2076, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9992e-01, 7.7927e-05]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "36 tensor(0.1976, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9972e-01, 2.7744e-04]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "37 tensor(0.1968, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9950e-01, 4.9533e-04]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "38 tensor(0.2244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.0017, 0.9983]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "39 tensor(0.1822, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9998e-01, 2.2215e-05]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "40 tensor(0.1754, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.4968e-04, 9.9985e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "41 tensor(0.1898, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9981e-01, 1.8527e-04]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "42 tensor(0.2010, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[3.6004e-04, 9.9964e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "43 tensor(0.2101, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9999e-01, 1.1828e-05]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "44 tensor(0.1986, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[5.8343e-05, 9.9994e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "45 tensor(0.1894, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.3661, 0.6339]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "46 tensor(0.1976, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.9795, 0.0205]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "47 tensor(0.1830, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9998e-01, 2.4490e-05]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "48 tensor(0.2097, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9997e-01, 2.6778e-05]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "49 tensor(0.2023, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[3.4883e-04, 9.9965e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "50 tensor(0.2054, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[4.8283e-05, 9.9995e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "51 tensor(0.1918, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[4.7631e-05, 9.9995e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "52 tensor(0.1964, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[6.2697e-05, 9.9994e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "53 tensor(0.1833, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[5.1475e-06, 9.9999e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "54 tensor(0.1882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[8.6021e-05, 9.9991e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "55 tensor(0.1981, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9999e-01, 7.2616e-06]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "56 tensor(0.1883, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.1612e-05, 9.9999e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "57 tensor(0.1938, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.7720e-05, 9.9998e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "58 tensor(0.1815, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.0016, 0.9984]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "59 tensor(0.1665, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.9930e-06, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "60 tensor(0.2071, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.0834e-05, 9.9998e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "61 tensor(0.1747, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.8952e-06, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "62 tensor(0.1915, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.9714, 0.0286]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "63 tensor(0.1658, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.1165e-06, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "64 tensor(0.1837, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.0346e-07]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "65 tensor(0.1729, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5806e-04, 9.9974e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "66 tensor(0.1816, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[5.7996e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "67 tensor(0.1680, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 2.4948e-08]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "68 tensor(0.1766, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[5.5136e-05, 9.9994e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "69 tensor(0.1694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[8.9612e-06, 9.9999e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "70 tensor(0.2042, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9998e-01, 2.0442e-05]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "71 tensor(0.1735, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.8842e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "72 tensor(0.1737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.9890, 0.0110]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "73 tensor(0.1756, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.6637e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "74 tensor(0.2086, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.5916e-08]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "75 tensor(0.1660, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0431e-05, 9.9999e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "76 tensor(0.1871, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.7765e-08]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "77 tensor(0.1570, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9911e-01, 8.9218e-04]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "78 tensor(0.2031, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 2.1832e-09]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "79 tensor(0.1827, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5685e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "80 tensor(0.1663, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0074e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "81 tensor(0.1714, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.2353e-06]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "82 tensor(0.1791, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.4417e-06, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "83 tensor(0.2075, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9999e-01, 6.7981e-06]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "84 tensor(0.1728, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[4.7277e-08, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "85 tensor(0.1860, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[7.2890e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "86 tensor(0.1607, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.7585e-05, 9.9997e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "87 tensor(0.2073, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 7.7326e-09]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "88 tensor(0.1706, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9998e-01, 2.0544e-05]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "89 tensor(0.1637, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.6260e-10]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "90 tensor(0.1724, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 8.6523e-09]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "91 tensor(0.1817, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[4.8633e-08, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "92 tensor(0.1575, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[5.3005e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "93 tensor(0.1944, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 8.8536e-11]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "94 tensor(0.1659, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.7731e-08, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "95 tensor(0.1775, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[3.0938e-06, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "96 tensor(0.2003, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[4.7037e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "97 tensor(0.1794, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 3.6155e-09]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "98 tensor(0.1803, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[6.4273e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "99 tensor(0.1903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[6.1375e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "100 tensor(0.1775, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 2.0903e-10]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "101 tensor(0.1798, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.6669e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "102 tensor(0.1587, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[8.1929e-06, 9.9999e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "103 tensor(0.1671, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.9389e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "104 tensor(0.1661, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.2467e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "105 tensor(0.2055, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.1907e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "106 tensor(0.1878, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.6525e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "107 tensor(0.1735, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.1081e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "108 tensor(0.1807, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9983e-01, 1.7356e-04]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "109 tensor(0.1709, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.0605e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "110 tensor(0.1893, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[0.3822, 0.6178]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "111 tensor(0.1817, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.8548e-09]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "112 tensor(0.1736, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[3.9293e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "113 tensor(0.1719, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[8.7451e-08, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "114 tensor(0.1900, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 2.2818e-11]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "115 tensor(0.1710, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.7175e-08]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "116 tensor(0.1722, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 2.3581e-12]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "117 tensor(0.1724, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[6.3809e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "118 tensor(0.1791, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.2937e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "119 tensor(0.1730, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[3.4754e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "120 tensor(0.1659, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.4433e-05, 9.9999e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "121 tensor(0.1798, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 2.4824e-07]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "122 tensor(0.1656, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9999e-01, 1.3804e-05]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "123 tensor(0.1710, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[4.7041e-08, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "124 tensor(0.1754, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[8.8093e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "125 tensor(0.1783, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[4.1464e-10, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "126 tensor(0.1650, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9996e-01, 3.8348e-05]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "127 tensor(0.1593, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9995e-01, 5.1450e-05]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "128 tensor(0.1722, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.9295e-10, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "129 tensor(0.1892, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[6.4941e-08, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "130 tensor(0.1830, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 4.7310e-12]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "131 tensor(0.1718, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 4.3691e-10]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "132 tensor(0.1641, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.0821e-10, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "133 tensor(0.1764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.4948e-04, 9.9985e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "134 tensor(0.1805, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.3338e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "135 tensor(0.1769, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.3869e-13]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "136 tensor(0.1714, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5295e-05, 9.9997e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "137 tensor(0.1981, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.6610e-06, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "138 tensor(0.1679, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.0387e-13]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "139 tensor(0.1692, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9991e-01, 9.0287e-05]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "140 tensor(0.1777, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.0796e-07]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "141 tensor(0.1692, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[7.8513e-11, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "142 tensor(0.1722, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.1810e-05, 9.9998e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "143 tensor(0.1782, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.1779e-12]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "144 tensor(0.1834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.9955e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "145 tensor(0.1666, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 5.5229e-14]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "146 tensor(0.1714, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 2.1388e-06]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "147 tensor(0.1588, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.5652e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "148 tensor(0.1656, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[8.6473e-11, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "149 tensor(0.1672, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[7.1645e-11, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "150 tensor(0.1635, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.0197e-13]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "151 tensor(0.1729, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 7.7454e-13]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "152 tensor(0.1778, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.2114e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "153 tensor(0.1731, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 3.3320e-08]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "154 tensor(0.1654, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 6.1294e-14]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "155 tensor(0.1726, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.2262e-10, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "156 tensor(0.1562, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[6.4913e-10, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "157 tensor(0.1874, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[3.8505e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "158 tensor(0.1798, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.9162e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "159 tensor(0.1574, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 3.3119e-06]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "160 tensor(0.1679, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 2.1499e-14]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "161 tensor(0.1634, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 2.7925e-10]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "162 tensor(0.1658, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.2062e-06, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "163 tensor(0.1687, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.9046e-10, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "164 tensor(0.1642, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[8.1173e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "165 tensor(0.1582, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[7.1409e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "166 tensor(0.1578, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[6.5047e-10, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "167 tensor(0.1802, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.8862e-11, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "168 tensor(0.1715, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 7.7370e-13]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "169 tensor(0.1750, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[5.8186e-05, 9.9994e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "170 tensor(0.1817, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.0350e-13]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "171 tensor(0.1679, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.1480e-12]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "172 tensor(0.1663, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 9.3313e-15]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "173 tensor(0.1847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.1198e-12]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "174 tensor(0.1876, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[3.6964e-11, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "175 tensor(0.1665, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 5.5779e-10]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "176 tensor(0.1669, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 2.7398e-14]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "177 tensor(0.1891, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.9429e-11]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "178 tensor(0.1735, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 2.6689e-14]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "179 tensor(0.1679, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.9267e-11, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "180 tensor(0.1598, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[5.3113e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "181 tensor(0.1654, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[4.2016e-10, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "182 tensor(0.1727, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 4.6689e-09]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "183 tensor(0.1612, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[7.1962e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "184 tensor(0.1646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 7.2532e-15]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "185 tensor(0.1679, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.0319e-07, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "186 tensor(0.1662, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.2981e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "187 tensor(0.1642, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 4.6075e-13]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "188 tensor(0.1654, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.1863e-11]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "189 tensor(0.1658, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[4.1731e-11, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "190 tensor(0.1710, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[6.6311e-05, 9.9993e-01]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "191 tensor(0.1961, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.1365e-06, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "192 tensor(0.1806, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[9.9903e-01, 9.6537e-04]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "193 tensor(0.1675, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 4.0836e-15]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "194 tensor(0.1590, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.1664e-12]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "195 tensor(0.1546, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 1.1486e-14]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "196 tensor(0.1598, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5181e-11, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "197 tensor(0.1550, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 4.6795e-09]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "198 tensor(0.1575, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[1.0000e+00, 5.6275e-10]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "199 tensor(0.1643, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[4.2240e-06, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "train_acc =  0.9699787946428572\n",
      "0 tensor(0.5013, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[3.8254e-09, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "1 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4859e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "2 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4422e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "3 tensor(0.3043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4411e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "4 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5190e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "5 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4458e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "6 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4503e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "7 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4423e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "8 tensor(0.3931, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4411e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "9 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4686e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "10 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4823e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "11 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "12 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4402e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "13 tensor(0.3931, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4463e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "14 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4449e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "15 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5367e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "16 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4384e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "17 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "18 tensor(0.3635, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4420e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "19 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4623e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "20 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "21 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "22 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4769e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "23 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "24 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "25 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "26 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4471e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "27 tensor(0.3635, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "28 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "29 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "30 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.6039e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "31 tensor(0.3754, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4426e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "32 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4487e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "33 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4409e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "34 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4559e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "35 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "36 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4485e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "37 tensor(0.3102, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "38 tensor(0.3991, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4437e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "39 tensor(0.3102, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.6044e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "40 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "41 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4426e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "42 tensor(0.3043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "43 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4421e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "44 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4512e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "45 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "46 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "47 tensor(0.3635, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4500e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "48 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4384e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "49 tensor(0.3813, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4406e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "50 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4436e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "51 tensor(0.3754, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4414e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "52 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4473e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "53 tensor(0.3221, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4401e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "54 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4428e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "55 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "56 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4399e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "57 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "58 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4446e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "59 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "60 tensor(0.3754, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4472e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "61 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4406e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "62 tensor(0.3813, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4444e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "63 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5714e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "64 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4569e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "65 tensor(0.3043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4444e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "66 tensor(0.3635, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4395e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "67 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "68 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "69 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "70 tensor(0.3221, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "71 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4404e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "72 tensor(0.3635, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5009e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "73 tensor(0.3221, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4437e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "74 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4426e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "75 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4437e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "76 tensor(0.2806, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4559e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "77 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4436e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "78 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4451e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "79 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "80 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5022e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "81 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "82 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4474e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "83 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "84 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4517e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "85 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4407e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "86 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "87 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4414e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "88 tensor(0.3635, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4406e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "89 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "90 tensor(0.3221, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4568e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "91 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4401e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "92 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4437e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "93 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4417e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "94 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4404e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "95 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4422e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "96 tensor(0.3221, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4413e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "97 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4423e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "98 tensor(0.2747, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "99 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4416e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "100 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4405e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "101 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4424e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "102 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4385e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "103 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4574e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "104 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4660e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "105 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "106 tensor(0.3635, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "107 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "108 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4489e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "109 tensor(0.3754, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4399e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "110 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4713e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "111 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4484e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "112 tensor(0.4050, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "113 tensor(0.3754, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4413e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "114 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4392e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "115 tensor(0.3813, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4399e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "116 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5842e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "117 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4648e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "118 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4402e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "119 tensor(0.3221, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4629e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "120 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4425e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "121 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4384e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "122 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4384e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "123 tensor(0.3102, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4591e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "124 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4801e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "125 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4386e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "126 tensor(0.3635, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4392e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "127 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4426e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "128 tensor(0.3754, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4408e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "129 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5588e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "130 tensor(0.3872, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4414e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "131 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4496e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "132 tensor(0.3102, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4415e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "133 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "134 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4715e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "135 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4493e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "136 tensor(0.3102, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4406e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "137 tensor(0.3813, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4455e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "138 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4453e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "139 tensor(0.3043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4386e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "140 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "141 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4444e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "142 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "143 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "144 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "145 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "146 tensor(0.3754, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4406e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "147 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4494e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "148 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4744e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "149 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "150 tensor(0.3102, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4455e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "151 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4471e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "152 tensor(0.3102, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "153 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4404e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "154 tensor(0.3754, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4528e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "155 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4456e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "156 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "157 tensor(0.3102, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4384e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "158 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4633e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "159 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "160 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4443e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "161 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4417e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "162 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4449e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "163 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4402e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "164 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4769e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "165 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "166 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4547e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "167 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "168 tensor(0.3754, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4421e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "169 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4431e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "170 tensor(0.4109, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4386e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "171 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4681e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "172 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4426e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "173 tensor(0.3161, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "174 tensor(0.3102, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4408e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "175 tensor(0.3043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4392e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "176 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "177 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4385e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "178 tensor(0.3635, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4600e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "179 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "180 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4419e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "181 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4421e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "182 tensor(0.3280, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4395e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "183 tensor(0.3813, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4406e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "184 tensor(0.3102, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "185 tensor(0.3635, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4667e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "186 tensor(0.3458, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4395e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "187 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "188 tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4421e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "189 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4496e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "190 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4484e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "191 tensor(0.3931, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4406e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "192 tensor(0.3221, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.7668e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "193 tensor(0.3576, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4466e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "194 tensor(0.3043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4478e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "195 tensor(0.3517, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4518e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "196 tensor(0.3398, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4721e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "197 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4441e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "198 tensor(0.3102, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4446e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "199 tensor(0.3043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "train_acc =  0.5750939764492754\n",
      "0 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4509e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "1 tensor(0.3378, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4421e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "2 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4461e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "3 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "4 tensor(0.4114, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4463e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "5 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4466e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "6 tensor(0.4114, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4442e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "7 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "8 tensor(0.4382, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4435e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "9 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4471e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "10 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "11 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4799e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "12 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "13 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "14 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "15 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5352e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "16 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4519e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "17 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4408e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "18 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "19 tensor(0.3914, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4512e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "20 tensor(0.4048, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4401e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "21 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4414e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "22 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4619e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "23 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4442e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "24 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "25 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4552e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "26 tensor(0.3378, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[3.1239e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "27 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4815e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "28 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4473e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "29 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "30 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4439e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "31 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4457e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "32 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "33 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4449e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "34 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4416e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "35 tensor(0.3177, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "36 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4972e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "37 tensor(0.4048, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4423e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "38 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4616e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "39 tensor(0.3981, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "40 tensor(0.4048, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4476e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "41 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4443e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "42 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4429e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "43 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4484e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "44 tensor(0.3378, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4564e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "45 tensor(0.4181, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4461e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "46 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4587e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "47 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "48 tensor(0.3378, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4424e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "49 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4468e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "50 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "51 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4423e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "52 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "53 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4568e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "54 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4510e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "55 tensor(0.4315, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4434e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "56 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4415e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "57 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "58 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4419e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "59 tensor(0.3914, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "60 tensor(0.4449, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4401e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "61 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4446e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "62 tensor(0.3914, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4444e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "63 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4384e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "64 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4414e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "65 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4455e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "66 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4429e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "67 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "68 tensor(0.3177, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4409e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "69 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4405e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "70 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4423e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "71 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4432e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "72 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4415e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "73 tensor(0.3311, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4423e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "74 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "75 tensor(0.3914, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4429e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "76 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4723e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "77 tensor(0.3311, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4445e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "78 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4453e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "79 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "80 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "81 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4420e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "82 tensor(0.4114, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4874e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "83 tensor(0.3244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "84 tensor(0.3110, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "85 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "86 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4425e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "87 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4416e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "88 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4402e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "89 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "90 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4449e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "91 tensor(0.4048, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4444e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "92 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4435e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "93 tensor(0.3981, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4413e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "94 tensor(0.4048, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "95 tensor(0.4248, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4566e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "96 tensor(0.3378, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4732e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "97 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4463e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "98 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4495e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "99 tensor(0.3244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "100 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4407e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "101 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4484e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "102 tensor(0.4048, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "103 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4472e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "104 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4744e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "105 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4509e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "106 tensor(0.3244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4419e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "107 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "108 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5042e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "109 tensor(0.3914, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4498e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "110 tensor(0.3311, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "111 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4386e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "112 tensor(0.3110, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4501e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "113 tensor(0.3311, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4449e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "114 tensor(0.3914, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "115 tensor(0.4048, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4385e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "116 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4637e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "117 tensor(0.3311, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "118 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4395e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "119 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "120 tensor(0.3378, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4395e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "121 tensor(0.4114, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4406e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "122 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "123 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4405e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "124 tensor(0.4114, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4487e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "125 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4384e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "126 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "127 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4413e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "128 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "129 tensor(0.3244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "130 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "131 tensor(0.3914, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4445e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "132 tensor(0.4048, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4433e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "133 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4869e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "134 tensor(0.4382, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "135 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4418e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "136 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4418e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "137 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4493e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "138 tensor(0.2909, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4546e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "139 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5862e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "140 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "141 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "142 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5198e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "143 tensor(0.3914, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4399e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "144 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "145 tensor(0.3311, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4426e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "146 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4447e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "147 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4471e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "148 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4430e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "149 tensor(0.2842, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "150 tensor(0.3311, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4643e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "151 tensor(0.3311, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4464e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "152 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5652e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "153 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4520e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "154 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "155 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4439e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "156 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4405e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "157 tensor(0.3244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4415e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "158 tensor(0.3981, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4401e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "159 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4446e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "160 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4412e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "161 tensor(0.3914, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4436e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "162 tensor(0.3311, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4395e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "163 tensor(0.3378, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4538e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "164 tensor(0.3378, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4436e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "165 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4416e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "166 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4493e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "167 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4455e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "168 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.9828e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "169 tensor(0.3244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4404e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "170 tensor(0.3311, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "171 tensor(0.3914, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "172 tensor(0.3914, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4411e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "173 tensor(0.3914, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5099e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "174 tensor(0.3646, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4408e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "175 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4407e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "176 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "177 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "178 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "179 tensor(0.4114, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4726e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "180 tensor(0.3847, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4640e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "181 tensor(0.3244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "182 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4402e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "183 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4763e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "184 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4395e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "185 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "186 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "187 tensor(0.3378, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4430e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "188 tensor(0.3713, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4420e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "189 tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4401e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "190 tensor(0.3780, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4445e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "191 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4494e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "192 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4444e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "193 tensor(0.4114, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4552e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "194 tensor(0.3981, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4585e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "195 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "196 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "197 tensor(0.3512, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4415e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "198 tensor(0.3579, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.6514e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "199 tensor(0.2842, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "train_acc =  0.5439185267857143\n",
      "0 tensor(0.4113, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4384e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "1 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4406e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "2 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4402e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "3 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4384e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "4 tensor(0.4322, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4451e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "5 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4462e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "6 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "7 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4655e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "8 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5164e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "9 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "10 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4485e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "11 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4445e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "12 tensor(0.4392, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4445e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "13 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4475e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "14 tensor(0.4322, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4414e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "15 tensor(0.3205, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4438e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "16 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4408e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "17 tensor(0.4253, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4695e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "18 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4459e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "19 tensor(0.4043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4409e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "20 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4450e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "21 tensor(0.3205, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "22 tensor(0.4043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "23 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4527e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "24 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4405e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "25 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4652e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "26 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4493e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "27 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4383e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "28 tensor(0.4113, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4478e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "29 tensor(0.3205, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "30 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4818e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "31 tensor(0.3205, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "32 tensor(0.4183, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "33 tensor(0.3275, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4417e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "34 tensor(0.4043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "35 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4424e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "36 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "37 tensor(0.4322, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4537e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "38 tensor(0.4113, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4405e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "39 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4780e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "40 tensor(0.3205, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4414e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "41 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4404e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "42 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "43 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4486e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "44 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4414e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "45 tensor(0.3414, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "46 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4418e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "47 tensor(0.3484, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "48 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4498e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "49 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "50 tensor(0.3345, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4454e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "51 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4608e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "52 tensor(0.3414, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4423e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "53 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "54 tensor(0.3484, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4407e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "55 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4386e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "56 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4425e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "57 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4618e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "58 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4487e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "59 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4509e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "60 tensor(0.3345, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4419e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "61 tensor(0.4043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4432e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "62 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4535e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "63 tensor(0.3275, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4392e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "64 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4601e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "65 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4457e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "66 tensor(0.3135, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "67 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[4.0846e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "68 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "69 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4604e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "70 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4435e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "71 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "72 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4399e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "73 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4499e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "74 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4427e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "75 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4434e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "76 tensor(0.3205, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4411e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "77 tensor(0.4043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4439e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "78 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4668e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "79 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4417e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "80 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "81 tensor(0.3484, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4413e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "82 tensor(0.4113, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4386e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "83 tensor(0.4322, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4567e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "84 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "85 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4463e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "86 tensor(0.3414, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4408e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "87 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4594e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "88 tensor(0.4183, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4430e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "89 tensor(0.3484, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4442e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "90 tensor(0.4043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4468e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "91 tensor(0.4322, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "92 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4609e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "93 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4444e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "94 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "95 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "96 tensor(0.3484, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4424e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "97 tensor(0.4043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4464e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "98 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4413e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "99 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4416e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "100 tensor(0.4392, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4416e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "101 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4425e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "102 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.9754e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "103 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4411e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "104 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "105 tensor(0.4183, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4426e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "106 tensor(0.4253, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4429e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "107 tensor(0.4322, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4434e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "108 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "109 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4446e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "110 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4405e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "111 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4420e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "112 tensor(0.4113, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "113 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "114 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "115 tensor(0.3345, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4936e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "116 tensor(0.3484, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4407e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "117 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4696e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "118 tensor(0.3414, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "119 tensor(0.4183, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4630e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "120 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4416e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "121 tensor(0.3414, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4392e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "122 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4438e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "123 tensor(0.4113, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4468e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "124 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4589e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "125 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4572e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "126 tensor(0.4322, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4430e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "127 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "128 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4414e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "129 tensor(0.3275, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4395e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "130 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4449e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "131 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4386e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "132 tensor(0.3135, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4399e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "133 tensor(0.3484, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4601e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "134 tensor(0.3414, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "135 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "136 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4778e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "137 tensor(0.3484, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4399e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "138 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "139 tensor(0.3414, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "140 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4443e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "141 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "142 tensor(0.4392, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4486e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "143 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4407e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "144 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4460e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "145 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4467e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "146 tensor(0.4113, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4500e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "147 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4401e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "148 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4402e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "149 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4421e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "150 tensor(0.3903, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "151 tensor(0.4113, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "152 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4579e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "153 tensor(0.4043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4450e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "154 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4462e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "155 tensor(0.4253, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4545e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "156 tensor(0.3205, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4422e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "157 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4404e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "158 tensor(0.4183, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4412e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "159 tensor(0.3345, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "160 tensor(0.3484, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4405e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "161 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4438e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "162 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4451e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "163 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4478e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "164 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "165 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4438e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "166 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4417e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "167 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4395e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "168 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4402e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "169 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4717e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "170 tensor(0.4043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4672e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "171 tensor(0.4183, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4402e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "172 tensor(0.3414, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4441e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "173 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "174 tensor(0.4253, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4419e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "175 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4604e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "176 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4404e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "177 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4447e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "178 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4754e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "179 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4452e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "180 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4643e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "181 tensor(0.3484, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "182 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4392e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "183 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "184 tensor(0.3275, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4690e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "185 tensor(0.3764, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4545e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "186 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "187 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4392e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "188 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4624e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "189 tensor(0.3554, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "190 tensor(0.4043, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4444e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "191 tensor(0.3973, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4448e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "192 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "193 tensor(0.3624, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4451e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "194 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4416e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "195 tensor(0.4322, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4479e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "196 tensor(0.4462, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4411e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "197 tensor(0.3694, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4474e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "198 tensor(0.4322, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "199 tensor(0.3834, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4405e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "train_acc =  0.532489013671875\n",
      "0 tensor(0.4099, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "1 tensor(0.4171, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.6360e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "2 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4401e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "3 tensor(0.4461, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4421e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "4 tensor(0.3447, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "5 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "6 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "7 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4456e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "8 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "9 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4407e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "10 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4614e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "11 tensor(0.3375, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4442e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "12 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "13 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "14 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "15 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "16 tensor(0.4099, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "17 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4450e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "18 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4415e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "19 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "20 tensor(0.4026, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4451e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "21 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4600e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "22 tensor(0.3520, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "23 tensor(0.4244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "24 tensor(0.3447, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "25 tensor(0.3230, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4385e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "26 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4477e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "27 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4408e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "28 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4406e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "29 tensor(0.4244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "30 tensor(0.3520, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4399e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "31 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4413e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "32 tensor(0.4461, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4433e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "33 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4487e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "34 tensor(0.3447, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "35 tensor(0.3592, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "36 tensor(0.3447, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5218e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "37 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "38 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4437e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "39 tensor(0.4099, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4414e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "40 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4433e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "41 tensor(0.4389, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "42 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "43 tensor(0.3592, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4409e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "44 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4458e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "45 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4392e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "46 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5294e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "47 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "48 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4440e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "49 tensor(0.4533, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4386e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "50 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4844e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "51 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "52 tensor(0.4026, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4438e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "53 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4527e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "54 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4461e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "55 tensor(0.4316, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "56 tensor(0.4461, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4578e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "57 tensor(0.3592, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "58 tensor(0.3230, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4414e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "59 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "60 tensor(0.4244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4992e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "61 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "62 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4392e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "63 tensor(0.4099, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4462e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "64 tensor(0.3302, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4420e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "65 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4711e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "66 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4516e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "67 tensor(0.3302, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "68 tensor(0.4099, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4455e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "69 tensor(0.3375, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4417e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "70 tensor(0.4099, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4501e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "71 tensor(0.4171, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4409e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "72 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4701e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "73 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4453e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "74 tensor(0.4026, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "75 tensor(0.3230, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4429e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "76 tensor(0.3157, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4407e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "77 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "78 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4394e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "79 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "80 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "81 tensor(0.4171, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4392e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "82 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "83 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4404e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "84 tensor(0.4244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "85 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4386e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "86 tensor(0.4389, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4419e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "87 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4421e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "88 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4513e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "89 tensor(0.4171, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "90 tensor(0.4026, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4414e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "91 tensor(0.4171, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4456e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "92 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4420e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "93 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "94 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "95 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4678e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "96 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4418e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "97 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4602e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "98 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4482e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "99 tensor(0.4171, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4485e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "100 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "101 tensor(0.4026, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4420e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "102 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4425e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "103 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4413e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "104 tensor(0.4171, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "105 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4496e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "106 tensor(0.3592, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5167e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "107 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4444e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "108 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4401e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "109 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4419e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "110 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5009e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "111 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4454e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "112 tensor(0.4026, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4724e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "113 tensor(0.4171, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4401e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "114 tensor(0.4751, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4435e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "115 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4443e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "116 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "117 tensor(0.3592, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4423e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "118 tensor(0.4244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4472e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "119 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4402e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "120 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "121 tensor(0.3592, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4793e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "122 tensor(0.4026, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4415e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "123 tensor(0.3447, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4724e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "124 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4545e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "125 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "126 tensor(0.4171, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4453e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "127 tensor(0.4099, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4424e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "128 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4515e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "129 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "130 tensor(0.3157, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4414e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "131 tensor(0.3447, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "132 tensor(0.4461, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4391e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "133 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4553e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "134 tensor(0.3375, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4412e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "135 tensor(0.4171, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4443e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "136 tensor(0.4099, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4417e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "137 tensor(0.3520, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4396e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "138 tensor(0.4171, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4489e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "139 tensor(0.4099, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4402e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "140 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4416e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "141 tensor(0.3520, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "142 tensor(0.4026, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4402e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "143 tensor(0.3302, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4435e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "144 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4490e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "145 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4649e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "146 tensor(0.3230, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4674e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "147 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "148 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "149 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "150 tensor(0.3375, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4400e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "151 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4406e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "152 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5528e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "153 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4411e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "154 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4507e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "155 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4442e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "156 tensor(0.4461, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4405e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "157 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5862e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "158 tensor(0.4244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4607e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "159 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4410e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "160 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4413e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "161 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "162 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4456e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "163 tensor(0.3447, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4434e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "164 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4390e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "165 tensor(0.3447, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4570e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "166 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4622e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "167 tensor(0.4316, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4387e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "168 tensor(0.3882, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4406e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "169 tensor(0.3592, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4759e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "170 tensor(0.3447, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4489e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "171 tensor(0.4244, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4398e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "172 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4392e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "173 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4403e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "174 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4619e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "175 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "176 tensor(0.3592, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4392e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "177 tensor(0.4026, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4668e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "178 tensor(0.3737, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4401e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "179 tensor(0.3520, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4468e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "180 tensor(0.3447, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4490e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "181 tensor(0.3375, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4436e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "182 tensor(0.4099, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4393e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "183 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4522e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "184 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4392e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "185 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5562e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "186 tensor(0.3520, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4404e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "187 tensor(0.3592, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4443e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "188 tensor(0.3447, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4417e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "189 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4389e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "190 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4770e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "191 tensor(0.3664, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4532e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "192 tensor(0.3592, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4540e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "193 tensor(0.4171, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4780e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "194 tensor(0.3447, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4418e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "195 tensor(0.3302, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "196 tensor(0.3954, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4631e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "197 tensor(0.3592, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.5577e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[1., 0.]], dtype=torch.float64)\n",
      "198 tensor(0.3809, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4397e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "199 tensor(0.3375, dtype=torch.float64, grad_fn=<DivBackward1>) out >>>>  tensor([[2.4388e-13, 1.0000e+00]], grad_fn=<SliceBackward0>) \n",
      "label >>>>  tensor([[0., 1.]], dtype=torch.float64)\n",
      "train_acc =  0.5224467844202898\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "\n",
    "\n",
    "def train(epochs,tsk):\n",
    "    matches,length = 0,0\n",
    "    train_dataloader = tsk[0]\n",
    "    criterion = nn.CrossEntropyLoss(weight = torch.tensor(tsk[2]))\n",
    "    for epoch in range(epochs):\n",
    "        for i,train_data in enumerate(train_dataloader):\n",
    "            model.zero_grad()\n",
    "#             in_mean = torch.mean(train_data[0],dim=1,keepdim=True)\n",
    "#             in_std = torch.std(train_data[0],dim=1,keepdim=True)\n",
    "            inp,exp_out = train_data[0],train_data[1]\n",
    "#             print(exp_out)\n",
    "#             return\n",
    "            out = model(inp)\n",
    "            loss = criterion(out,exp_out)\n",
    "            matches+=batch_size-sum(np.argmax(out.detach().numpy(),axis=1)-np.argmax(exp_out.numpy(),axis=1))\n",
    "            length+=batch_size\n",
    "#             return\n",
    "            if i%8000==0:\n",
    "#                 model()\n",
    "                print(epoch,loss,\"out >>>> \",out[:1],\"\\nlabel >>>> \",train_data[1][:1])\n",
    "#                 break\n",
    "#                 return\n",
    "            \n",
    "            loss_list.append(loss.detach().numpy())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    print('train_acc = ',matches/length)\n",
    "for tsk in task_list:\n",
    "    train(200,tsk)\n",
    "# confusion_mat1()\n",
    "# torch.save({\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "\n",
    "            \n",
    "#             }, 'stock_classifier_lstm.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ae77217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x72dc3df4dea0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "682a1aaf-f7b2-47f9-b33b-e34742edcf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhss20/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6925675675675677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhss20/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7424857839155159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhss20/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7078464106844742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhss20/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6963636363636364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhss20/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.714879467996675\n"
     ]
    }
   ],
   "source": [
    "def model_test(test_dataloader,model):\n",
    "    y_pred,y_true = [],[]\n",
    "    for i,test_data in enumerate(test_dataloader):\n",
    "        src=test_data[0]\n",
    "        trg = test_data[1]\n",
    "        pred=test_data[2][:,0]\n",
    "        past=test_data[3]\n",
    "        output = model(src)\n",
    "#         if i%50==0:\n",
    "#             print(criterion1(output,trg))\n",
    "        y_pred.append(torch.argmax(output))\n",
    "        y_true.append(torch.argmax(trg))\n",
    "    print(f1_score(y_true,y_pred))\n",
    "    \n",
    "def meta_test(model,task_list,n_shots=0):\n",
    "    \n",
    "    for task in task_list:\n",
    "        criterion1 = nn.CrossEntropyLoss(weight = torch.tensor(task[2]))\n",
    "        tmodel = copy.deepcopy(model)\n",
    "        optimizer = torch.optim.Adam(tmodel.parameters())\n",
    "        test_dataloader =  task[1]\n",
    "        itr = iter(test_dataloader)\n",
    "        for _ in range(n_shots):\n",
    "            test_data = next(itr)\n",
    "            src=test_data[0]\n",
    "            trg = test_data[1]\n",
    "            pred=test_data[2][:,0]\n",
    "            past=test_data[3]\n",
    "            output = tmodel(src)\n",
    "            loss = criterion1(output,trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        tmodel.eval()\n",
    "        model_test(test_dataloader,tmodel)\n",
    "meta_test(model,task_list,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a114805",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test1(t_steps,i1=0,i2=1e9):\n",
    "    colr = ['red','green']\n",
    "#     print()\n",
    "    i2 = int(min(i2,test_len-t_steps-extra_days))\n",
    "#     plt.plot(range(i1,i2),snp['Close'][train_len+i1+t_steps:train_len+i2+t_steps],color ='black',linewidth=0.5)\n",
    "    with torch.no_grad():\n",
    "        for i,data in enumerate(test_dataloader):\n",
    "            if(i<i1 or i>i2):\n",
    "                continue\n",
    "            inp,labels,pred,past = data[0],data[1],data[2],data[3]\n",
    "#             print(pred)\n",
    "            out = model(inp.repeat(batch_size,1,1))[0]\n",
    "#             print(inp[0,:,-1][-1])\n",
    "#             past,std_past = torch.mean(inp[0,:,-1][-1]),torch.std(inp[0,:,-1])\n",
    "            idx = torch.argmax(out)\n",
    "#             print(idx)\n",
    "#             return\n",
    "            plt.scatter(i,past,color='black',s=2,alpha=0.3)\n",
    "#             plt.scatter(i,snp['swt'][i+train_len+t_steps1-1],color='blue',s=2,alpha=0.5)\n",
    "            if(idx==0):\n",
    "                plt.fill_between(\n",
    "                    range(i,i+predict_steps),\n",
    "                    past-(10*pd1)*np.sqrt(past), \n",
    "                    past, \n",
    "                    alpha=0.5,\n",
    "                    color=colr[idx],\n",
    "                    interpolate=True,\n",
    "                    label=\"+/- 1-std\",\n",
    "                )\n",
    "            if(idx==1):\n",
    "                plt.fill_between(\n",
    "                    range(i,i+predict_steps),\n",
    "                    past, \n",
    "                    past+5*pu1*np.sqrt(past), \n",
    "                    alpha=0.5,\n",
    "                    color=colr[idx],\n",
    "                    interpolate=True,\n",
    "                    label=\"+/- 1-std\",\n",
    "                )\n",
    "            if(idx==2):\n",
    "                plt.fill_between(\n",
    "                    range(i,i+predict_steps),\n",
    "                    past+pu1*std_past, \n",
    "                    past+2*pu1*std_past, \n",
    "                    alpha=0.5,\n",
    "                    color=colr[idx],\n",
    "                    interpolate=True,\n",
    "                    label=\"+/- 1-std\",\n",
    "                )\n",
    "#             if(idx==3):\n",
    "#                 plt.fill_between(\n",
    "#                     range(i,i+predict_steps),\n",
    "#                     past+par_k2*par_k*std_past, \n",
    "#                     past+(par_k2+1)*par_k*std_past, \n",
    "#                     alpha=0.5,\n",
    "#                     color=colr[idx],\n",
    "#                     interpolate=True,\n",
    "#                     label=\"+/- 1-std\",\n",
    "#                 )\n",
    "#             if(i==200):\n",
    "#                 break\n",
    "def random_confusion():\n",
    "    y_pred,y_true = [],[]\n",
    "    colr = ['red','green']\n",
    "    for i,data in enumerate(test_dataloader):\n",
    "        inp,labels,_,past = data[0],data[1],data[2],data[3]\n",
    "        # out = model(inp.repeat(batch_size,1,1))[0]\n",
    "      # input = trg[:-1]\n",
    "#         past,std_past = torch.mean(inp[0,:,-1]),torch.std(inp[0,:,-1])\n",
    "#         past,std_past = torch.mean(inp[0,:,-1][-1]),torch.std(inp[0,:,-1])\n",
    "        pred = snp['swt'][i+train_len+t_steps1-1]\n",
    "        if pred<past:\n",
    "            idx=0\n",
    "        else:\n",
    "            idx=1\n",
    "        pred_label = torch.tensor(idx)\n",
    "#         pred_label[id]=1\n",
    "        y_pred.append(pred_label)\n",
    "#             temp[torch.argmax(out)]=1\n",
    "        y_true.append(torch.argmax(labels))\n",
    "    conf_mat = confusion_matrix(y_true,y_pred,normalize='pred')\n",
    "    print(conf_mat)\n",
    "    print(np.trace(np.array(conf_mat))/classes)\n",
    "    \n",
    "def random_confusion_return():\n",
    "    y_pred,y_true = [],[]\n",
    "    for i,data in enumerate(test_dataloader):\n",
    "        inp,labels,pred = data[0],data[1],data[2]\n",
    "        # out = model(inp.repeat(batch_size,1,1))[0]\n",
    "      # input = trg[:-1]\n",
    "#         past = talib.EMA(inp[0,:,-1].numpy().astype(np.float64),timeperiod = len(inp[0,:,-1]))[-1]\n",
    "        past = torch.mean(inp[0,:,-1][-3:])\n",
    "        pred = 100*(torch.mean(inp[0,:,-1][-1])-past)/past\n",
    "        \n",
    "        if pred<pd1:\n",
    "            id=0\n",
    "        elif pred<pd2:\n",
    "            id=1\n",
    "        elif pred<=pu1:\n",
    "            id=2\n",
    "        elif pred<pu2:\n",
    "            id=3\n",
    "        elif pred<=pu3:\n",
    "            id=4\n",
    "        else:\n",
    "            id=5\n",
    "        pred_label = torch.tensor(np.random.randint(0,6,1))[0]\n",
    "#         pred_label = torch.tensor(id)\n",
    "#         print(pred_label)\n",
    "#         pred_label[id]=1\n",
    "        y_pred.append(pred_label)\n",
    "#             temp[torch.argmax(out)]=1\n",
    "        y_true.append(torch.argmax(labels))\n",
    "#         y_true,y_pred = np.array(y_true),np.array(y_pred)\n",
    "\n",
    "#     print(y_true,y_pred)\n",
    "    conf_mat = confusion_matrix(y_true,y_pred,normalize='pred')\n",
    "    print(conf_mat)\n",
    "    print(np.trace(np.array(conf_mat))/classes)\n",
    "\n",
    "def test2(t_steps,i1=0,i2=1e9):\n",
    "    colr = ['red','green']\n",
    "#     colr = ['darkred','indianred','orange','yellowgreen','lime','darkgreen']\n",
    "#     print()\n",
    "    i2 = int(min(i2,test_len-t_steps-extra_days))\n",
    "    profit = 0\n",
    "    g_cnt = 0\n",
    "    toler = 0.75\n",
    "    ry_cnt = 0\n",
    "    gy_cnt = 0\n",
    "    r_cnt = 0\n",
    "    investment = 0\n",
    "#     prev_trade = 0\n",
    "    list_trade = []\n",
    "    cnt_trade = 0\n",
    "    profit_trade = 0\n",
    "    prev = -1\n",
    "    entry = 0\n",
    "    plt.figure(figsize=(15,15))\n",
    "#     plt.plot(range(i1,i2),snp['Close'][train_len+i1+t_steps-1+extra_days:train_len+i2+t_steps-1+extra_days],color ='black',linewidth=0.5)\n",
    "    with torch.no_grad():\n",
    "        for i,data in enumerate(test_dataloader):\n",
    "            if(i<i1 or i>i2):\n",
    "                continue\n",
    "            inp,labels,pred,past = data[0],data[1],data[2],data[3]\n",
    "            encoder_outputs, (hidden, cell) = encoder(inp.repeat(batch_size,1,1))\n",
    "            out = decoder( hidden, cell, encoder_outputs)[0]\n",
    "            \n",
    "            plt.scatter(i,past,color='black',s=2,alpha=0.3)\n",
    "#             plt.scatter(i,snp['swt'][i+train_len+t_steps1-1],color='blue',s=2,alpha=0.5)\n",
    "            print(past)\n",
    "            idx = torch.argmax(out)\n",
    "\n",
    "            if(idx==0):\n",
    "                plt.fill_between(\n",
    "                    range(i,i+predict_steps),\n",
    "                    past-(10*pd1)*np.sqrt(past), \n",
    "                    past, \n",
    "                    alpha=0.5,\n",
    "                    color=colr[idx],\n",
    "                    interpolate=True,\n",
    "                    label=\"+/- 1-std\",\n",
    "                )\n",
    "            if(idx==1):\n",
    "                plt.fill_between(\n",
    "                    range(i,i+predict_steps),\n",
    "                    past, \n",
    "                    past+5*pu1*np.sqrt(past), \n",
    "                    alpha=0.5,\n",
    "                    color=colr[idx],\n",
    "                    interpolate=True,\n",
    "                    label=\"+/- 1-std\",\n",
    "                )\n",
    "            if(idx==2):\n",
    "                plt.fill_between(\n",
    "                    range(i,i+predict_steps),\n",
    "                    past+pu1*std_past, \n",
    "                    past+2*pu1*std_past, \n",
    "                    alpha=0.5,\n",
    "                    color=colr[idx],\n",
    "                    interpolate=True,\n",
    "                    label=\"+/- 1-std\",\n",
    "                )\n",
    "#             if(idx==0):\n",
    "#                 plt.fill_between(\n",
    "#                     range(i,i+predict_steps),\n",
    "#                     past*(1+2*pd1),\n",
    "#                     past*(1+pd1),\n",
    "#                     alpha=0.8,\n",
    "#                     color=colr[idx],\n",
    "#                     interpolate=True,\n",
    "#                     label=\"+/- 1-std\",\n",
    "#                 )\n",
    "#             if(idx==1):\n",
    "#                 plt.fill_between(\n",
    "#                     range(i,i+predict_steps),\n",
    "#                     past*(1+pd1),\n",
    "#                     past*(1+pd2),\n",
    "#                     alpha=0.8,\n",
    "#                     color=colr[idx],\n",
    "#                     interpolate=True,\n",
    "#                     label=\"+/- 1-std\",\n",
    "#                 )\n",
    "#             if(idx==2):\n",
    "#                 plt.fill_between(\n",
    "#                     range(i,i+predict_steps),\n",
    "#                     past*(1+pd2),\n",
    "#                     past*(1+pu1),\n",
    "#                     alpha=0.8,\n",
    "#                     color=colr[idx],\n",
    "#                     interpolate=True,\n",
    "#                     label=\"+/- 1-std\",\n",
    "#                 )\n",
    "#             if(idx==3):\n",
    "#                 plt.fill_between(\n",
    "#                     range(i,i+predict_steps),\n",
    "#                     past*(1+pu1),\n",
    "#                     past*(1+pu2),\n",
    "#                     alpha=0.8,\n",
    "#                     color=colr[idx],\n",
    "#                     interpolate=True,\n",
    "#                     label=\"+/- 1-std\",\n",
    "#                 )\n",
    "#             if(idx==4):\n",
    "#                 plt.fill_between(\n",
    "#                     range(i,i+predict_steps),\n",
    "#                     past*(1+pu2),\n",
    "#                     past*(1+pu3),\n",
    "#                     alpha=0.8,\n",
    "#                     color=colr[idx],\n",
    "#                     interpolate=True,\n",
    "#                     label=\"+/- 1-std\",\n",
    "#                 )\n",
    "#             if(idx==5):\n",
    "#                 plt.fill_between(\n",
    "#                     range(i,i+predict_steps),\n",
    "#                     past*(1+pu3),\n",
    "#                     past*(1+pu3*2),\n",
    "#                     alpha=0.8,\n",
    "#                     color=colr[idx],\n",
    "#                     interpolate=True,\n",
    "#                     label=\"+/- 1-std\",\n",
    "#                 )\n",
    "            prev=idx\n",
    "# test2(t_steps1,700,1200)\n",
    "confusion_mat2()\n",
    "# random_confusion()\n",
    "confusion_mat1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f801e887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4573290,
     "sourceId": 7808675,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
