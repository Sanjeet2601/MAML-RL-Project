{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JfJzhbPkSS2P"
   },
   "outputs": [],
   "source": [
    "\n",
    "# # IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
    "# # TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
    "# # THEN FEEL FREE TO DELETE THIS CELL.\n",
    "# # NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
    "# # ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
    "# # NOTEBOOK.\n",
    "\n",
    "# import os\n",
    "# import sys\n",
    "# from tempfile import NamedTemporaryFile\n",
    "# from urllib.request import urlopen\n",
    "# from urllib.parse import unquote, urlparse\n",
    "# from urllib.error import HTTPError\n",
    "# from zipfile import ZipFile\n",
    "# import tarfile\n",
    "# import shutil\n",
    "\n",
    "# CHUNK_SIZE = 40960\n",
    "# DATA_SOURCE_MAPPING = 'mypendu:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4827676%2F8160127%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240419%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240419T004513Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Db9450a25a4b4b9dd445df4044dc73798408b6424ea287f57b6996e9842297dffe7c22d3fd589d98a8cdb48e44dd7bad999a4c4fb14ef6163b1a3107b4db377248c7e6e5ecaa4e6a9d2f71dd4a602dded581222bebdfd06d3dc130af9a1d831d9a3be951961486c9eb5cdd35e33b3b640f539a540573b35d4a31ead8d99720f23f75f79b2c96f5c5cc5c341d1b27f62327a27f834eb22abd0733ecd029fafbe35717e63ae402a2d197dc918538193f0943a889bae00b8e6eed75b394e8b3e5a3c20ca8c7788ea48ab27df062dff0872ce69358b9706c83d4934db1162c2dcc10dac4bba8ea1fa8d2c726234c47d46bcd192e1f1b6fd0bc7d2000d75a9d4918097'\n",
    "\n",
    "# KAGGLE_INPUT_PATH='/kaggle/input'\n",
    "# KAGGLE_WORKING_PATH='/kaggle/working'\n",
    "# KAGGLE_SYMLINK='kaggle'\n",
    "\n",
    "# !umount /kaggle/input/ 2> /dev/null\n",
    "# shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
    "# os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
    "# os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
    "\n",
    "# try:\n",
    "#   os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
    "# except FileExistsError:\n",
    "#   pass\n",
    "# try:\n",
    "#   os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
    "# except FileExistsError:\n",
    "#   pass\n",
    "\n",
    "# for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
    "#     directory, download_url_encoded = data_source_mapping.split(':')\n",
    "#     download_url = unquote(download_url_encoded)\n",
    "#     filename = urlparse(download_url).path\n",
    "#     destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
    "#     try:\n",
    "#         with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
    "#             total_length = fileres.headers['content-length']\n",
    "#             print(f'Downloading {directory}, {total_length} bytes compressed')\n",
    "#             dl = 0\n",
    "#             data = fileres.read(CHUNK_SIZE)\n",
    "#             while len(data) > 0:\n",
    "#                 dl += len(data)\n",
    "#                 tfile.write(data)\n",
    "#                 done = int(50 * dl / int(total_length))\n",
    "#                 sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
    "#                 sys.stdout.flush()\n",
    "#                 data = fileres.read(CHUNK_SIZE)\n",
    "#             if filename.endswith('.zip'):\n",
    "#               with ZipFile(tfile) as zfile:\n",
    "#                 zfile.extractall(destination_path)\n",
    "#             else:\n",
    "#               with tarfile.open(tfile.name) as tarfile:\n",
    "#                 tarfile.extractall(destination_path)\n",
    "#             print(f'\\nDownloaded and uncompressed: {directory}')\n",
    "#     except HTTPError as e:\n",
    "#         print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
    "#         continue\n",
    "#     except OSError as e:\n",
    "#         print(f'Failed to load {download_url} to path {destination_path}')\n",
    "#         continue\n",
    "\n",
    "# print('Data source import complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vu2sPr1he69K"
   },
   "source": [
    "# OpenAI Gym Environments\n",
    "<a id=\"Environment\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DGsfs7tefecH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install gymnasium\n",
    "# !pip install swig\n",
    "# !pip install gymnasium[box2d]\n",
    "# !pip install gymnasium[mujoco]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8GoDm9R3iGN9"
   },
   "outputs": [],
   "source": [
    "# %env MUJOCO_GL=egl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldi276eYSS2X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bRm0PQfFe69L"
   },
   "outputs": [],
   "source": [
    "# all imports go in here\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "import random\n",
    "from itertools import count, cycle\n",
    "from collections import deque, namedtuple\n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import grad\n",
    "import mujoco_py\n",
    "import mujoco\n",
    "import mediapy as media\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1uOe1zIQSS2Y"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir('/kaggle/input/mypendu')\n",
    "import mypendu\n",
    "\n",
    "gym.envs.register(\n",
    "    id='mypendu1',\n",
    "    entry_point='mypendu:mypendu1',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKuc7N03e69M"
   },
   "source": [
    "In this assignment we will be exploring Deep RL algorithms and for this we will be using environmentd provided by OpenAI Gym. In particular we will be exploring \"Pendulum-v1\" , \"Hopper-v4\", and \"Half-Cheetah\" environments (https://gymnasium.farama.org/environments/classic_control/ ). The code to instantiate the environments are given in the cells below. Run these cells and play with the environments to learn more details about the environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IZZ8jRnDe69M"
   },
   "outputs": [],
   "source": [
    "# # Create Inverted Pendulum environment\n",
    "# #https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
    "\n",
    "# env = gym.make('Pendulum-v1', render_mode=\"rgb_array\")\n",
    "# s = env.reset(seed = 34)\n",
    "# print(\"Observation Space = \")\n",
    "# print(env.observation_space)\n",
    "# print(\"Action Space = \")\n",
    "# print(env.action_space)\n",
    "# done = False\n",
    "# for episode in range(20):\n",
    "#     print(\"In episode {}\".format(episode))\n",
    "#     for i in range(100):\n",
    "#         env.render()\n",
    "#         print(s)\n",
    "#         a = env.action_space.sample()\n",
    "#         s, r, done, truncated, _ = env.step(a)\n",
    "#         if done:\n",
    "#             print(\"Finished after {} timestep\".format(i+1))\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "-CDCgNI5e69M"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Create Hopper environment\n",
    "# # https://gymnasium.farama.org/environments/mujoco/hopper/\n",
    "\n",
    "\n",
    "# import gymnasium as gym\n",
    "# env = gym.make(\"Humanoid-v4\", render_mode = \"human\")\n",
    "# s = env.reset(seed = 34)\n",
    "# print(\"Observation Space = \")\n",
    "# print(env.observation_space)\n",
    "# print(\"Action Space = \")\n",
    "# print(env.action_space)\n",
    "# done = False\n",
    "# for episode in range(1):\n",
    "#     # print(\"In episode {}\".format(episode))\n",
    "#     for i in range(100):\n",
    "#         env.render()\n",
    "#         # print(s)\n",
    "#         a = env.action_space.sample()\n",
    "#         s, r, done, truncated, _ = env.step(a)\n",
    "#         if done:\n",
    "#             print(\"Finished after {} timestep\".format(i+1))\n",
    "# env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5p4J86xXe69M"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Create Half-Cheetah environment\n",
    "# # https://gymnasium.farama.org/environments/mujoco/hopper/\n",
    "\n",
    "\n",
    "# import gymnasium as gym\n",
    "# env = gym.make(\"HalfCheetah-v4\", render_mode = \"rgb_array\")\n",
    "# s = env.reset(seed = 34)\n",
    "# print(\"Observation Space = \")\n",
    "# print(env.observation_space)\n",
    "# print(\"Action Space = \")\n",
    "# print(env.action_space)\n",
    "# done = False\n",
    "# for episode in range(1):\n",
    "#     print(\"In episode {}\".format(episode))\n",
    "#     for i in range(100):\n",
    "#         env.render()\n",
    "#         print(s)\n",
    "#         a = env.action_space.sample()\n",
    "#         s, r, done, truncated, _ = env.step(a)\n",
    "#         if done:\n",
    "#             print(\"Finished after {} timestep\".format(i+1))\n",
    "# env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQLclV2be69M"
   },
   "source": [
    "# Hyperparameters\n",
    "<a id=\"Hyperparameters\"></a>\n",
    "\n",
    "All your hyperparameters should be stated here. We will change their value here and your code should work  accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "sobxx_zVe69N"
   },
   "outputs": [],
   "source": [
    "# mention the values of all the hyperparameters (you can add more hyper-paramters as well) to be used in the entire notebook, put the values that gave the best\n",
    "# performance and were finally used for the agent\n",
    "\n",
    "gamma = 0.99\n",
    "noiseScaleRatio = 0.8\n",
    "minSamples = 25\n",
    "epochs = 5\n",
    "tau = 0.1\n",
    "\n",
    "\n",
    "\n",
    "MAX_TRAIN_EPISODES = 120\n",
    "MAX_EVAL_EPISODES = 1\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "F9_tt8gA9svu"
   },
   "outputs": [],
   "source": [
    "seeds = [0,1,2,3,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDzb1p2ee69N"
   },
   "source": [
    "# Helper Functions\n",
    "<a id=\"helper\"></a>\n",
    "\n",
    "Write all the helper functions that will be used for value-based and policy based algorithms below. In case you want to add more helper functions, please feel free to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rbLSqS_Be69N"
   },
   "outputs": [],
   "source": [
    "#Value Network\n",
    "def createValueNetwork(inDim, outDim, action_size, hDim = [32,32], activation = F.relu):\n",
    "    #this creates a Feed Forward Neural Network class and instantiates it and returns the class\n",
    "    #the class should be derived from torch nn.Module and it should have init and forward method at the very least\n",
    "    #the forward function should return q-value for each possible action\n",
    "\n",
    "    class ValueNetwork(nn.Module):\n",
    "      def __init__(self,inDim,outDim,hDim,activation):\n",
    "        super(ValueNetwork,self).__init__()\n",
    "        self.fc1 = nn.Linear(inDim+action_size,hDim[0])\n",
    "        self.linear_layers = nn.ModuleList()\n",
    "        for i in range(len(hDim)-1):\n",
    "            # if i == 1:\n",
    "            #     self.linear_layers.append(nn.Linear(hDim[i-1]+action_size, hDim[i]))\n",
    "            # else:\n",
    "            self.linear_layers.append(nn.Linear(hDim[i], hDim[i+1]))\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hDim[-1], outDim)\n",
    "        self.act = activation\n",
    "\n",
    "      def forward(self,src,src2):\n",
    "        out = torch.cat([src,src2],1)\n",
    "        # print(src.shape)\n",
    "        out = self.fc1(out)\n",
    "        out = torch.relu(out)\n",
    "        for i,layer in enumerate(self.linear_layers):\n",
    "            out = torch.relu(layer(out))\n",
    "        out = self.output_layer(out)\n",
    "        return out\n",
    "    valueNetwork = ValueNetwork(inDim,outDim,hDim,activation)\n",
    "\n",
    "    return valueNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nlWjseCYe69N"
   },
   "outputs": [],
   "source": [
    "#Policy Network\n",
    "def createPolicyNetwork(inDim, outDim,envActionRange ,hDim = [32,32], activation = F.relu):\n",
    "    #this creates a Feed Forward Neural Network class and instantiates it and returns the class\n",
    "    #the class should be derived from torch nn.Module and it should have init and forward method at the very least\n",
    "    #the forward function should return action logit vector\n",
    "    al,ah = envActionRange\n",
    "    al,ah = torch.tensor(al),torch.tensor(ah)\n",
    "    class PolicyNetwork(nn.Module):\n",
    "      def __init__(self,inDim,outDim,hDim,activation):\n",
    "        super(PolicyNetwork,self).__init__()\n",
    "        self.linear_layers = nn.ModuleList()\n",
    "        for i in range(len(hDim)):\n",
    "            if i == 0:\n",
    "                self.linear_layers.append(nn.Linear(inDim, hDim[i]))\n",
    "            else:\n",
    "                self.linear_layers.append(nn.Linear(hDim[i-1], hDim[i]))\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hDim[-1], outDim)\n",
    "        self.act = activation\n",
    "        self.tanh =  nn.Tanh()\n",
    "      def forward(self,x):\n",
    "        for layer in self.linear_layers:\n",
    "            x = torch.relu(layer(x))\n",
    "\n",
    "        # Forward pass through the output layer\n",
    "        x = self.output_layer(x)\n",
    "        out  = torch.tanh(x)\n",
    "        out = out*ah\n",
    "        return out\n",
    "\n",
    "    policyNetwork = PolicyNetwork(inDim,outDim,hDim,activation)\n",
    "\n",
    "\n",
    "    return policyNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ghUNEBc7jkfg"
   },
   "outputs": [],
   "source": [
    "def moving_average(arr, window_size):\n",
    "    \"\"\"\n",
    "    Moving average function to smoothen the results.\n",
    "    \"\"\"\n",
    "    kernel = np.ones(window_size) / window_size\n",
    "    moving_avg = np.convolve(arr, kernel, mode='valid')\n",
    "    return moving_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4PqJW89FbJCD"
   },
   "outputs": [],
   "source": [
    "def plotQuantity(quantityListDict, descriptionList, lln=-1):\n",
    "    #this function takes in the quantityListDict and plots quantity vs episodes.\n",
    "    #quantityListListDict = {envInstanceCount: quantityList}\n",
    "    #quantityList is list of the qunatity per episode,\n",
    "    #for example it could be mean reward per episode, traintime per episode, etc.\n",
    "    #\n",
    "    #NOTE: len(quantityList) == totalEpisodeCount\n",
    "    #\n",
    "    #Since we run multiple instances of the environment, there will be variance across environments\n",
    "    #so in the plot, you will plot per episode maximum, minimum and average value across all env instances\n",
    "    #Basically, you need to envelop (e.g., via color) the quantity between max and min with mean value in between\n",
    "    #\n",
    "    #use the descriptionList parameter to put legends, title, etc.\n",
    "    #For each of the plot, create the legend on the left/right side so that it doesn't overlay on the plot lines/envelop.\n",
    "    #\n",
    "    #this is a generic function and can be used to plot any of the quantity of interest\n",
    "    #In particular we will be using this function to plot:\n",
    "    #        mean train rewards vs episodes\n",
    "    #        mean evaluation rewards vs episodes\n",
    "    #        total steps vs episode\n",
    "    #        train time vs episode\n",
    "    #        wall clock time vs episode\n",
    "    #\n",
    "    #\n",
    "    #this function doesn't return anything\n",
    "    plt.rcParams['figure.figsize'] = (15, 10)\n",
    "    for i,des in enumerate(descriptionList):\n",
    "        # print(des)\n",
    "        for j,agent in enumerate(quantityListDict):\n",
    "            lists = agent\n",
    "#             print(lists)\n",
    "#             lists = np.array(lists[:4])\n",
    "            agent_dis =[]\n",
    "            for k in range(len(lists)):\n",
    "                # print(\"k\",k)\n",
    "                # print(\"lk\",len(lists[k]))\n",
    "                # print(\"lki\",len(lists[k][i]))\n",
    "                agent_dis.append(lists[k][i][:lln])\n",
    "            lists = np.array(agent_dis)\n",
    "            # print(lists.shape)\n",
    "            plts = moving_average(np.mean(lists,axis=0),2)\n",
    "            low = moving_average(np.min(lists,axis=0),2)\n",
    "            high = moving_average(np.max(lists,axis=0),2)\n",
    "            plt.plot(plts,label='{}'.format(j))\n",
    "            plt.xlabel(des)\n",
    "            plt.fill_between(np.arange(0,len(plts)), low, high, where=(high > low), alpha=0.2)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4zCW_BJe69N"
   },
   "source": [
    "## ReplayBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVbSqpd4e69N"
   },
   "source": [
    "In next few cells, you will implement replaybuffer class.\n",
    "\n",
    "This class creates a buffer for storing and retrieving experiences. This is a generic class and can be used\n",
    "for different agents like NFQ, DQN, DDQN, PER_DDQN, etc.\n",
    "Following are the methods for this class which are implemented in subsequent cells\n",
    "\n",
    "```\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, bufferSize, batch_size, seed)\n",
    "    def store(self, state, action, reward, next_state, done)\n",
    "    def sample(self, batchSize)\n",
    "    def length(self)\n",
    "```   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "wmfzgfjye69O"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, buffer_size, batch_size, seed):\n",
    "        # this function creates the relevant data-structures, and intializes all relevant variables\n",
    "        self.exp = []\n",
    "        self.buffer_size = buffer_size\n",
    "        self.seed = seed\n",
    "        self.batch_size = batch_size\n",
    "        # self.f=0\n",
    "#         if \"PER\" in self.buffertype:\n",
    "#           self.f=1\n",
    "#         for key,value in kwargs.items():\n",
    "#           setattr(self,key,value)\n",
    "# #         print(self.f,bufferType)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "AzOFK6J_e69O"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(ReplayBuffer):\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        #stores the experiences, based on parameters in init\n",
    "        #\n",
    "        #this function does not return anything\n",
    "        #\n",
    "        if len(self.exp)==self.buffer_size:\n",
    "            self.exp = self.exp[1:]\n",
    "        experience = (state,action,reward,next_state,done)\n",
    "        self.exp.append(experience)\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "8PButFbGe69O"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(ReplayBuffer):\n",
    "    def sample(self, batchSize):\n",
    "        # this method returns batchSize number of experiences\n",
    "        # this function returns experiences samples\n",
    "        #\n",
    "\n",
    "        indices = np.random.choice(len(self.exp), min(batchSize,len(self.exp)), replace=False)\n",
    "        experiencesList = [self.exp[id] for id in indices]\n",
    "        return experiencesList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "soTqec0dxQme"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(ReplayBuffer):\n",
    "    def splitExperiences(self, experiences):\n",
    "        #it takes in experiences and gives the following:\n",
    "        #states, actions, rewards, nextStates, dones\n",
    "        #\n",
    "        # print(experiences[1][0])\n",
    "        # print(experiences[2])\n",
    "        l = len(experiences)\n",
    "        states = np.array([experiences[i][0] for i in range(l)])\n",
    "        actions = np.array([experiences[i][1] for i in range(l)])\n",
    "        rewards = np.array([experiences[i][2] for i in range(l)])\n",
    "        nextStates = np.array([experiences[i][3] for i in range(l)])\n",
    "        dones = np.array([experiences[i][4] for i in range(l)])\n",
    "        return states, actions, rewards, nextStates, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "3-hD18lee69O"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(ReplayBuffer):\n",
    "    def length(self):\n",
    "        #tells the number of experiences stored in the internal buffer\n",
    "        #\n",
    "        return len(self.exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "AHzUPhcMSS2b"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(ReplayBuffer):\n",
    "        def clean(self):\n",
    "            self.exp.clear()\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jVXlGOye69O"
   },
   "source": [
    "## Deep Deterministic Policy Gradient (DDPG) ##\n",
    "<a id=\"ddpg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0Ju4nQMe69O"
   },
   "source": [
    "Implement the Deep Deterministic Policy Gradient (DDPG) agent. We have studied about DDPG agent in the Lecture. Use the function definitions (given below).\n",
    "\n",
    "This class implements the DDPG agent, you are required to implement the various methods of this class\n",
    "as outlined below. Note this class is generic and should work with any permissible Gym environment\n",
    "\n",
    "```\n",
    "class DDPG():\n",
    "    def init(self, env, seed, gamma, tau, bufferSize, batch_size, updateFrequency,\n",
    "             policyOptimizerFn, valueOptimizerFn,\n",
    "             policyOptimizerLR,valueOptimizerLR,\n",
    "             MAX_TRAIN_EPISODES,MAX_EVAL_EPISODE,\n",
    "             optimizerFn)\n",
    "    \n",
    "    def runDDPG(self)\n",
    "    def trainAgent(self)\n",
    "    def gaussianStrategy(self, net , s , envActionRange , noiseScaleRatio,\n",
    "        explorationMax = True)\n",
    "    def greedyStrategy(self, net , s , envActionRange)\n",
    "    def trainNetworks(self, experiences)\n",
    "    def updateNetworks(self, onlineNet, targetNet, tau)\n",
    "    def evaluateAgent(self)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "qYntSx4me69P"
   },
   "outputs": [],
   "source": [
    "class DDPG():\n",
    "    def __init__(self, env, seed, gamma, tau, bufferSize, batch_size,hdim, updateFrequency,\n",
    "             policyOptimizerFn, valueOptimizerFn,\n",
    "             policyOptimizerLR,valueOptimizerLR,\n",
    "             MAX_TRAIN_EPISODES,MAX_EVAL_EPISODE,\n",
    "             optimizerFn):\n",
    "        #this DDPG method\n",
    "        # 1. creates and initializes (with seed) the environment, train/eval episodes, gamma, etc.\n",
    "        # 2. creates and intializes all the variables required for book-keeping values via the initBookKeeping method\n",
    "        # 3. creates targetValueNetwork , targetPolicyNetwork\n",
    "        # 4. creates and initializes (with network params) the optimizer function\n",
    "        # 5. creates onlineValueNetwork, onlinePolicyNetwork\n",
    "        # 6. Creates the replayBuffer\n",
    "\n",
    "        self.env = env\n",
    "        self.env.reset(seed=seed)\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.buffer_size = bufferSize\n",
    "        self.batch_size = batch_size\n",
    "        self.updateFrequency = updateFrequency\n",
    "        self.tep = MAX_TRAIN_EPISODES\n",
    "        self.eep = MAX_EVAL_EPISODE\n",
    "\n",
    "\n",
    "        indim,outdim = self.env.observation_space.shape[0],self.env.action_space.shape[0]\n",
    "        envActionRange = (self.env.action_space.low[0],self.env.action_space.high[0])\n",
    "        self.tvnet = createValueNetwork(indim,1,outdim,hdim)\n",
    "        self.tpnet = createPolicyNetwork(indim,outdim,envActionRange,hdim)\n",
    "        self.ovnet = createValueNetwork(indim,1,outdim,hdim)\n",
    "        self.opnet = createPolicyNetwork(indim,outdim,envActionRange,hdim)\n",
    "        self.tvnet.load_state_dict(self.ovnet.state_dict())\n",
    "        self.tpnet.load_state_dict(self.opnet.state_dict())\n",
    "        if policyOptimizerFn=='adam':\n",
    "          self.poptim = torch.optim.Adam(self.opnet.parameters(),lr = policyOptimizerLR)\n",
    "        elif policyOptimizerFn=='sgd':\n",
    "          self.poptim = torch.optim.SGD(self.opnet.parameters(),lr = policyOptimizerLR)\n",
    "        else:\n",
    "          self.poptim = torch.optim.RMSprop(self.opnet.parameters(),lr = policyOptimizerLR)\n",
    "\n",
    "        if valueOptimizerFn=='adam':\n",
    "          self.voptim = torch.optim.Adam(self.ovnet.parameters(),lr = valueOptimizerLR)\n",
    "        elif valueOptimizerFn=='sgd':\n",
    "          self.voptim = torch.optim.SGD(self.ovnet.parameters(),lr = valueOptimizerLR)\n",
    "        else:\n",
    "          self.voptim = torch.optim.RMSprop(self.ovnet.parameters(),lr = valueOptimizerLR)\n",
    "        self.initBookKeeping()\n",
    "        self.rbuffer = ReplayBuffer(self.buffer_size,self.batch_size,seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "5aSw0QHFkzXe"
   },
   "outputs": [],
   "source": [
    "class DDPG(DDPG):\n",
    "    def initBookKeeping(self):\n",
    "        #this method creates and intializes all the variables required for book-keeping values and it is called\n",
    "        #init method\n",
    "      self.trainRewardsList = []\n",
    "      self.trainTimeList = []\n",
    "      self.evalRewardsList = []\n",
    "      self.wallClockTimeList = []\n",
    "      self.steps = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "xnXAVAZYD_J8"
   },
   "outputs": [],
   "source": [
    "class DDPG(DDPG):\n",
    "    def performBookKeeping(self,rewards, train = True):\n",
    "        #this method updates relevant variables for the bookKeeping, this can be called\n",
    "        #multiple times during training\n",
    "        #if you want you can print information using this, so it may help to monitor progress and also help to debug\n",
    "        #\n",
    "        if train:\n",
    "          self.trainRewardsList.append(np.sum(rewards))\n",
    "          # self.trainTimeList.append(self.traintime)\n",
    "          # self.wallClockTimeList.append(self.st_time-self.end_time)\n",
    "        else:\n",
    "          self.evalRewardsList.append(np.sum(rewards))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "DRwyutrRe69P"
   },
   "outputs": [],
   "source": [
    "class DDPG(DDPG):\n",
    "    def updateNetworks(self, onlineNet, targetNet, tau):\n",
    "        #this function updates the onlineNetwork with the target network\n",
    "        #\n",
    "        targetNet.load_state_dict(onlineNet.state_dict())\n",
    "        new_state_dict = {name:param for name,param in targetNet.named_parameters()}\n",
    "        for p1,p2 in zip(targetNet.named_parameters(),onlineNet.named_parameters()):\n",
    "          new_state_dict[p1[0]] = tau*p2[1]+(1-tau)*p1[1]\n",
    "        targetNet.load_state_dict(new_state_dict)\n",
    "        return onlineNet,targetNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "jqdbg-VMe69P"
   },
   "outputs": [],
   "source": [
    "class DDPG(DDPG):\n",
    "    def gaussianStrategy (self, net , s , envActionRange , noiseScaleRatio ,\n",
    "        explorationMax = True ):\n",
    "        #this function sets the scale of exploration then add the noise of this scale to the greedy action\n",
    "        #and clips it within the range\n",
    "\n",
    "        actionlowval,actionhighval = envActionRange\n",
    "        if explorationMax:\n",
    "            scale = actionhighval\n",
    "        else:\n",
    "            scale = noiseScaleRatio*actionhighval\n",
    "        # print(s.shape)\n",
    "        s = torch.tensor(s).float()\n",
    "        greedyaction = net(s.view(1,-1))\n",
    "        noise = scale*torch.randn(self.env.action_space.shape[0])\n",
    "\n",
    "        action = greedyaction+noise\n",
    "\n",
    "        # action = np.clip(action,actionlowval,actionhighval)\n",
    "        # print(greedyaction,noise,action)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "nJqajfXve69P"
   },
   "outputs": [],
   "source": [
    "class DDPG(DDPG):\n",
    "    def greedyStrategy (self, net , s , envActionRange ):\n",
    "        #this function selects the greedy action\n",
    "        #and clips it within the range\n",
    "\n",
    "        actionlowval,actionhighval = envActionRange\n",
    "        s = torch.tensor(s).float()\n",
    "        action = net(s.view(1,-1))\n",
    "        # action = torch.clamp(action,min=actionlowval,max=actionhighval)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "-30L67-me69P"
   },
   "outputs": [],
   "source": [
    "class DDPG(DDPG):\n",
    "    def runDDPG (self):\n",
    "        #this is the main method, it trains the agent, performs bookkeeping while training and finally evaluates\n",
    "        #the agent and returns the following quantities:\n",
    "        #1. episode wise mean train rewards\n",
    "        #2. epsiode wise mean eval rewards\n",
    "        #2. episode wise trainTime (in seconds): time elapsed during training since the start of the first episode\n",
    "        #3. episode wise wallClockTime (in seconds): actual time elapsed since the start of training,\n",
    "        #                               note this will include time for BookKeeping and evaluation\n",
    "        # Note both trainTime and wallClockTime get accumulated as episodes proceed.\n",
    "        #\n",
    "        result_train = self.trainAgent()\n",
    "        result_eval = self.evaluateAgent()\n",
    "        # self.finalEvalReward = result_eval[-1]\n",
    "        # plot\n",
    "\n",
    "        return self.trainRewardsList, self.trainTimeList, self.evalRewardsList, self.wallClockTimeList,self.steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Nk035a9me69P"
   },
   "outputs": [],
   "source": [
    "class DDPG(DDPG):\n",
    "    def trainAgent(self):\n",
    "        #this method collects experiences and trains the agent and does BookKeeping while training.\n",
    "        #this calls the trainNetwork() method internally, it also evaluates the agent per episode\n",
    "        #it trains the agent for MAX_TRAIN_EPISODES\n",
    "        #\n",
    "        # self.ovnet,self.tvnet = self.updateNetworks(self.ovnet,self.tvnet,1)\n",
    "        # self.opnet,self.tpnet = self.updateNetworks(self.opnet,self.tpnet,1)\n",
    "        w_time = 0\n",
    "        t_time=0\n",
    "        envActionRange = (self.env.action_space.low[0],self.env.action_space.high[0])\n",
    "        for e in tqdm(range(self.tep)):\n",
    "            stps = 0\n",
    "            self.st_time = time.time()\n",
    "            rew=0\n",
    "            s,_ = self.env.reset()\n",
    "            done = False\n",
    "            trunc = False\n",
    "            self.traintime = 0\n",
    "            while not (done or trunc):\n",
    "                a = self.gaussianStrategy(self.opnet,s,envActionRange,noiseScaleRatio,explorationMax=False)\n",
    "                # a = self.greedyStrategy(self.opnet,s,envActionRange)\n",
    "                a = a.data.numpy().flatten()\n",
    "                # print(env.step(a))\n",
    "                stps+=1\n",
    "                s_next,r,done,trunc,_ = self.env.step(a)\n",
    "                rew+=r\n",
    "                self.rbuffer.store(s,a,r,s_next,done)\n",
    "                # print(self.rbuffer.length(),done)\n",
    "                # if self.rbuffer.length()>1:\n",
    "                #     print('in',self.rbuffer.length(),done)\n",
    "                # print(done)\n",
    "                s = s_next\n",
    "                if self.rbuffer.length()>minSamples:\n",
    "                    experiences = self.rbuffer.sample(self.batch_size)\n",
    "                    self.traintime = timeit.timeit(lambda :self.trainNetwork(experiences),number=1)\n",
    "                # if e%self.updateFrequency==0:\n",
    "                #     self.ovnet,self.tvnet = self.updateNetworks(self.ovnet,self.tvnet,self.tau)\n",
    "                #     self.opnet,self.tpnet = self.updateNetworks(self.opnet,self.tpnet,self.tau)\n",
    "            # print(rew)\n",
    "            self.steps.append(stps)\n",
    "            self.performBookKeeping(rew)\n",
    "            self.evaluateAgent()\n",
    "            self.end_time = time.time()\n",
    "            w_time+=self.end_time-self.st_time\n",
    "            t_time+=self.traintime\n",
    "            self.wallClockTimeList.append(w_time)\n",
    "            self.trainTimeList.append(t_time)\n",
    "        return self.trainRewardsList, self.trainTimeList, self.evalRewardsList, self.wallClockTimeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "RPWXcf-je69R"
   },
   "outputs": [],
   "source": [
    "\n",
    "class DDPG(DDPG):\n",
    "    def trainNetwork(self, experiences):\n",
    "        # this method trains the value network epoch number of times and is called by the trainAgent function\n",
    "        # it essentially uses the experiences to calculate target, using the targets it calculates the error, which\n",
    "        # is further used for calulating the loss. It then uses the optimizer over the loss\n",
    "        # to update the params of the network by backpropagating through the network\n",
    "        # this function does not return anything\n",
    "        # you can try out other loss functions other than MSE like Huber loss, MAE, etc.\n",
    "        #\n",
    "        states,actions,rewards,nextstates,dones= self.rbuffer.splitExperiences(experiences)\n",
    "        states = torch.tensor(states).float()\n",
    "        actions = torch.tensor(actions).float()\n",
    "        rewards = torch.tensor(rewards).float()\n",
    "        nextstates = torch.tensor(nextstates).float()\n",
    "        dones = torch.tensor(dones).float()\n",
    "        # print(states.shape,actions.shape,rewards.shape,nextstates.shape,dones.shape)\n",
    "        for ep in range(epochs):\n",
    "\n",
    "            # argmax_a_qs_v = self.tpnet(nextstates)\n",
    "            # print(self.tvnet(torch.tensor(np.array(nextstates),dtype = torch.float32),argmax_a_qs_v),argmax_a_qs_v)\n",
    "            target_q = self.tvnet(nextstates,self.tpnet(nextstates))\n",
    "            target_q = rewards.view(-1,1)+(1-dones.view(-1,1))*self.gamma*target_q.detach()\n",
    "            qs = self.ovnet(states,actions)\n",
    "            vloss = loss_fn(qs,target_q)\n",
    "\n",
    "            self.voptim.zero_grad()\n",
    "            vloss.backward()\n",
    "            # nn.utils.clip_grad_norm_(self.ovnet.parameters(), 10.0)\n",
    "            self.voptim.step()\n",
    "\n",
    "            # argmax_a_qs_p = self.opnet(nextstates)\n",
    "            # max_a_qs_p = self.ovnet(nextstates,self.opnet(nextstates)).mean()\n",
    "            ploss = -self.ovnet(nextstates,self.opnet(nextstates)).mean()\n",
    "            self.poptim.zero_grad()\n",
    "            # print(grad(ploss,self.opnet.parameters(),retain_graph = True,allow_unused = True))\n",
    "            ploss.backward()\n",
    "\n",
    "            # nn.utils.clip_grad_norm_(self.opnet.parameters(), 10.0)\n",
    "            self.poptim.step()\n",
    "        # self.ovnet,self.tvnet = self.updateNetworks(self.ovnet,self.tvnet,self.tau)\n",
    "        # self.opnet,self.tpnet = self.updateNetworks(self.opnet,self.tpnet,self.tau)\n",
    "        for param, target_param in zip(self.ovnet.parameters(), self.tvnet.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "        for param, target_param in zip(self.opnet.parameters(), self.tpnet.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "kh1rx5Xee69R"
   },
   "outputs": [],
   "source": [
    "class DDPG(DDPG):\n",
    "    def evaluateAgent(self):\n",
    "        #this function evaluates the agent using the value network, it evaluates agent for MAX_EVAL_EPISODES\n",
    "        #typcially MAX_EVAL_EPISODES = 1\n",
    "        #\n",
    "        rew = []\n",
    "        envActionRange = (self.env.action_space.low[0],self.env.action_space.high[0])\n",
    "        for e in range(self.eep):\n",
    "            rs= 0\n",
    "            s,_ = self.env.reset()\n",
    "            done = False\n",
    "            trunc = False\n",
    "            for c in count():\n",
    "                a = self.greedyStrategy(self.opnet,s,envActionRange)\n",
    "                a = a.view(-1).detach().numpy()\n",
    "                s,r,done,trunc,_ = self.env.step(a)\n",
    "                rs+=r\n",
    "                if done or trunc:\n",
    "                    rew.append(rs)\n",
    "                    break\n",
    "        print(np.sum(rew))\n",
    "        self.performBookKeeping(rew,train=False)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "YNe_TCi98V7C"
   },
   "outputs": [],
   "source": [
    "# # env = gym.make('CartPole-v0')\n",
    "# # env = gym.make(\"Hopper-v4\")\n",
    "# env = gym.make(\"Pendulum-v1\")\n",
    "# # env = gym.make(\"HalfCheetah-v4\")\n",
    "# # print(env.action_space)\n",
    "# minSamples = 128\n",
    "# epochs = 1\n",
    "# noiseScaleRatio = 0.1\n",
    "# # loss_fn = nn.SmoothL1Loss()\n",
    "# loss_fn = nn.MSELoss()\n",
    "# ddpg = DDPG(env,seeds[0],0.99,0.005,100000,128,[512,256],1,'adam','adam',0.001,0.001,1000,1,_)\n",
    "# plt_list = ddpg.runDDPG()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "j8BXKggQIWmn"
   },
   "outputs": [],
   "source": [
    "# plt.plot(ddpg.trainRewardsList)\n",
    "# plt.show()\n",
    "# plt.plot(ddpg.evalRewardsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "3bZ0EpbI-446"
   },
   "outputs": [],
   "source": [
    "# print(env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzpF-ABVe69R"
   },
   "source": [
    "# Twin-Delayed Deep Deterministic Policy Gradient (TD3)\n",
    "<a id=\"td3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndZTyP1ie69R"
   },
   "source": [
    "Implement the Twin-delayed deep deterministic policy gradient (TD3) agent. We have studied about TD3 agent in the Lecture. Use the function definitions (given below).\n",
    "\n",
    "This class implements the TD3 agent, you are required to implement the various methods of this class\n",
    "as outlined below. Note this class is generic and should work with any permissible Gym environment\n",
    "\n",
    "```\n",
    "class DDPG():\n",
    "    def init(env, gamma, tau,\n",
    "    bufferSize ,\n",
    "    updateFrequencyPolicy ,\n",
    "    updateFrequencyValue ,\n",
    "    trainPolicyFrequency ,\n",
    "    policyOptimizerFn ,\n",
    "    valueOptimizerFn ,\n",
    "    policyOptimizerLR ,\n",
    "    valueOptimizerLR ,\n",
    "    MAX TRAIN EPISODES,\n",
    "    MAX EVAL EPISODE,\n",
    "    optimizerFn )\n",
    "    \n",
    "    def runTD3 (self)\n",
    "    def trainAgent (self)\n",
    "    def gaussianStrategy (self, net , s , envActionRange , noiseScaleRatio ,\n",
    "        explorationMax = True)\n",
    "    def greedyStrategy (self, net , s , envActionRange)\n",
    "    def trainNetworks (self,experiences , envActionRange)\n",
    "    def updateValueNetwork(self, onlineNet, targetNet, tau)\n",
    "    def updatePolicyNetwork(self, onlineNet, targetNet, tau)\n",
    "    def evaluateAgent (self)\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "qERrGcz-e69S"
   },
   "outputs": [],
   "source": [
    "class TD3():\n",
    "    def __init__(self,env,seed, gamma, tau,\n",
    "    bufferSize ,\n",
    "    batch_size,\n",
    "    hdim,\n",
    "    updateFrequencyPolicy ,\n",
    "    updateFrequencyValue ,\n",
    "    trainPolicyFrequency ,\n",
    "    policyOptimizerFn ,\n",
    "    valueOptimizerFn ,\n",
    "    policyOptimizerLR ,\n",
    "    valueOptimizerLR ,\n",
    "    MAX_TRAIN_EPISODES,\n",
    "    MAX_EVAL_EPISODE,\n",
    "    MAX_META_EPISODES,\n",
    "    optimizerFn,\n",
    "    MAX_ALLOWED_STEPS):\n",
    "        #this TD3 method\n",
    "        # 1. creates and initializes (with seed) the environment, train/eval episodes, gamma, etc.\n",
    "        # 2. creates and intializes all the variables required for book-keeping values via the initBookKeeping method\n",
    "        # 3. creates targetValueNetwork , targetPolicyNetwork\n",
    "        # 4. creates and initializes (with network params) the optimizer function\n",
    "        # 5. creates onlineValueNetwork, onlinePolicyNetwork\n",
    "        # 6. Creates the replayBuffer\n",
    "\n",
    "        self.env = env\n",
    "        self.env.reset(seed=seed)\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.buffer_size = bufferSize\n",
    "        self.batch_size = batch_size\n",
    "        self.updateFrequencyPolicy = updateFrequencyPolicy\n",
    "        self.updateFrequencyValue = updateFrequencyValue\n",
    "        self.trainPolicyFrequency = trainPolicyFrequency\n",
    "        self.tep = MAX_TRAIN_EPISODES\n",
    "        self.eep = MAX_EVAL_EPISODE\n",
    "        self.mep = MAX_META_EPISODES\n",
    "        self.mx_stps = MAX_ALLOWED_STEPS\n",
    "\n",
    "        indim,outdim = self.env.observation_space.shape[0],self.env.action_space.shape[0]\n",
    "        self.indim,self.outdim,self.hdim = indim,outdim,hdim\n",
    "        envActionRange = (self.env.action_space.low[0],self.env.action_space.high[0])\n",
    "        self.tvnet1 = createValueNetwork(indim,1,outdim,hdim)\n",
    "        self.tvnet2 = createValueNetwork(indim,1,outdim,hdim)\n",
    "        self.tpnet = createPolicyNetwork(indim,outdim,envActionRange,hdim)\n",
    "\n",
    "        self.ovnet1 = createValueNetwork(indim,1,outdim,hdim)\n",
    "        self.ovnet2 = createValueNetwork(indim,1,outdim,hdim)\n",
    "        self.opnet = createPolicyNetwork(indim,outdim,envActionRange,hdim)\n",
    "\n",
    "        self.tvnet1.load_state_dict(self.ovnet1.state_dict())\n",
    "        self.tvnet2.load_state_dict(self.ovnet2.state_dict())\n",
    "        self.tpnet.load_state_dict(self.opnet.state_dict())\n",
    "        if policyOptimizerFn=='adam':\n",
    "          self.poptim = torch.optim.Adam(self.opnet.parameters(),lr = policyOptimizerLR)\n",
    "        elif policyOptimizerFn=='sgd':\n",
    "          self.poptim = torch.optim.SGD(self.opnet.parameters(),lr = policyOptimizerLR)\n",
    "        else:\n",
    "          self.poptim = torch.optim.RMSprop(self.opnet.parameters(),lr = policyOptimizerLR)\n",
    "\n",
    "        if valueOptimizerFn=='adam':\n",
    "          self.voptim = torch.optim.Adam(list(self.ovnet1.parameters())+list(self.ovnet2.parameters()),lr = valueOptimizerLR)\n",
    "        elif valueOptimizerFn=='sgd':\n",
    "          self.voptim = torch.optim.SGD(list(self.ovnet1.parameters())+list(self.ovnet2.parameters()),lr = valueOptimizerLR)\n",
    "        else:\n",
    "          self.voptim = torch.optim.RMSprop(list(self.ovnet1.parameters())+list(self.ovnet2.parameters()),lr = valueOptimizerLR)\n",
    "        self.initBookKeeping()\n",
    "        self.rbuffer = ReplayBuffer(self.buffer_size,self.batch_size,seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "OEsiDUd6Gf5A"
   },
   "outputs": [],
   "source": [
    "class TD3(TD3):\n",
    "    def initBookKeeping(self):\n",
    "        #this method creates and intializes all the variables required for book-keeping values and it is called\n",
    "        #init method\n",
    "      self.trainRewardsList = []\n",
    "      self.trainTimeList = []\n",
    "      self.evalRewardsList = []\n",
    "      self.wallClockTimeList = []\n",
    "      self.steps = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "pgL1MjD3Gapf"
   },
   "outputs": [],
   "source": [
    "class TD3(TD3):\n",
    "    def performBookKeeping(self,rewards, train = True):\n",
    "        #this method updates relevant variables for the bookKeeping, this can be called\n",
    "        #multiple times during training\n",
    "        #if you want you can print information using this, so it may help to monitor progress and also help to debug\n",
    "        #\n",
    "        if train:\n",
    "          self.trainRewardsList.append(np.sum(rewards))\n",
    "          # self.trainTimeList.append(self.traintime)\n",
    "          # self.wallClockTimeList.append(self.st_time-self.end_time)\n",
    "        else:\n",
    "          self.evalRewardsList.append(np.sum(rewards))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Tns-xXITe69S"
   },
   "outputs": [],
   "source": [
    "class TD3(TD3):\n",
    "    def updateValueNetwork(self, onlineNet, targetNet, tau):\n",
    "        #this function updates the onlineNetwork with the target network\n",
    "        #\n",
    "        targetNet.load_state_dict(onlineNet.state_dict())\n",
    "        new_state_dict = {name:param for name,param in targetNet.named_parameters()}\n",
    "        for p1,p2 in zip(targetNet.named_parameters(),onlineNet.named_parameters()):\n",
    "          new_state_dict[p1[0]] = self.tau*p2[1]+(1-self.tau)*p1[1]\n",
    "        targetNet.load_state_dict(new_state_dict)\n",
    "        return onlineNet,targetNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "mX0t855be69S"
   },
   "outputs": [],
   "source": [
    "class TD3(TD3):\n",
    "    def updatePolicyNetwork(self, onlineNet, targetNet, tau):\n",
    "        #this function updates the onlineNetwork with the target network\n",
    "        #\n",
    "        targetNet.load_state_dict(onlineNet.state_dict())\n",
    "        new_state_dict = {name:param for name,param in targetNet.named_parameters()}\n",
    "        for p1,p2 in zip(targetNet.named_parameters(),onlineNet.named_parameters()):\n",
    "          new_state_dict[p1[0]] = self.tau*p2[1]+(1-self.tau)*p1[1]\n",
    "        targetNet.load_state_dict(new_state_dict)\n",
    "        return onlineNet,targetNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "btMjZm14e69S"
   },
   "outputs": [],
   "source": [
    "class TD3(TD3):\n",
    "    def gaussianStrategy (self, net , s , envActionRange , noiseScaleRatio ,\n",
    "        explorationMax = True ):\n",
    "        #this function sets the scale of exploration then add the noise of this scale to the greedy action\n",
    "        #and clips it within the range\n",
    "\n",
    "        actionlowval,actionhighval = envActionRange\n",
    "        if explorationMax:\n",
    "            scale = actionhighval\n",
    "        else:\n",
    "            scale = noiseScaleRatio*actionhighval\n",
    "        # print(s.shape)\n",
    "        s = torch.tensor(s).float()\n",
    "        greedyaction = net(s.view(1,-1))\n",
    "        noise = scale*torch.randn(self.env.action_space.shape[0])\n",
    "\n",
    "        action = greedyaction+noise\n",
    "\n",
    "        # action = np.clip(action,actionlowval,actionhighval)\n",
    "        # print(greedyaction,noise,action)\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "_bJhJj6Ze69b"
   },
   "outputs": [],
   "source": [
    "class TD3(TD3):\n",
    "    def greedyStrategy (self, net , s , envActionRange ):\n",
    "        #this function selects the greedy action\n",
    "        #and clips it within the range\n",
    "\n",
    "        actionlowval,actionhighval = envActionRange\n",
    "        s = torch.tensor(s).float()\n",
    "        action = net(s.view(1,-1))\n",
    "        # action = torch.clamp(action,min=actionlowval,max=actionhighval)\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "hI552Qdie69b"
   },
   "outputs": [],
   "source": [
    "class TD3(TD3):\n",
    "    def runTD3 (self):\n",
    "        #this is the main method, it trains the agent, performs bookkeeping while training and finally evaluates\n",
    "        #the agent and returns the following quantities:\n",
    "        #1. episode wise mean train rewards\n",
    "        #2. epsiode wise mean eval rewards\n",
    "        #2. episode wise trainTime (in seconds): time elapsed during training since the start of the first episode\n",
    "        #3. episode wise wallClockTime (in seconds): actual time elapsed since the start of training,\n",
    "        #                               note this will include time for BookKeeping and evaluation\n",
    "        # Note both trainTime and wallClockTime get accumulated as episodes proceed.\n",
    "        #\n",
    "        result_train = self.trainAgent()\n",
    "        result_eval = self.evaluateAgent()\n",
    "        # self.finalEvalReward = result_eval[-1]\n",
    "        # plot\n",
    "\n",
    "        return self.trainRewardsList, self.trainTimeList, self.evalRewardsList, self.wallClockTimeList,self.steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "ANjAnB_re69c"
   },
   "outputs": [],
   "source": [
    "class TD3(TD3):\n",
    "    def trainAgent(self):\n",
    "        #this method collects experiences and trains the agent and does BookKeeping while training.\n",
    "        #this calls the trainNetwork() method internally, it also evaluates the agent per episode\n",
    "        #it trains the agent for MAX_TRAIN_EPISODES\n",
    "        #\n",
    "        # self.ovnet,self.tvnet = self.updateValueNetwork(self.ovnet,self.tvnet,self.tau)\n",
    "        # self.opnet,self.tpnet = self.updatePolicyNetwork(self.opnet,self.tpnet,self.tau)\n",
    "        w_time = 0\n",
    "        t_time=0\n",
    "        envActionRange = (self.env.action_space.low[0],self.env.action_space.high[0])\n",
    "        for e in tqdm(range(self.tep)):\n",
    "            stps = 0\n",
    "            self.st_time = time.time()\n",
    "            s,_ = self.env.reset()\n",
    "\n",
    "            rew=0\n",
    "            done = False\n",
    "            trunc = False\n",
    "            self.traintime = 0\n",
    "            while not (done or stps>self.mx_stps or trunc):\n",
    "                a = self.gaussianStrategy(self.opnet,s,envActionRange,noiseScaleRatio)\n",
    "                a = a.data.numpy().flatten()\n",
    "                # print(env.step(a))\n",
    "                stps+=1\n",
    "                s_next,r,done,trunc,_ = self.env.step(a)\n",
    "\n",
    "                rew+=r\n",
    "                self.rbuffer.store(s,a,r,s_next,done)\n",
    "                # print(self.rbuffer.length(),done)\n",
    "                # if self.rbuffer.length()>1:\n",
    "                #     print('in',self.rbuffer.length(),done)\n",
    "                if self.rbuffer.length()>minSamples:\n",
    "                    experiences = self.rbuffer.sample(self.batch_size)\n",
    "                    self.traintime = timeit.timeit(lambda :self.trainNetwork(experiences,envActionRange,e),number=1)\n",
    "                s = s_next\n",
    "                # print(s)\n",
    "            self.steps.append(stps)\n",
    "            self.performBookKeeping(rew)\n",
    "            self.evaluateAgent()\n",
    "            self.end_time = time.time()\n",
    "            w_time+=self.end_time-self.st_time\n",
    "            t_time+=self.traintime\n",
    "            self.wallClockTimeList.append(w_time)\n",
    "            self.trainTimeList.append(t_time)\n",
    "            # print(np.sum(rew),len(self.trainRewardsList),len(self.trainTimeList),len(self.evalRewardsList))\n",
    "        return self.trainRewardsList, self.trainTimeList, self.evalRewardsList, self.wallClockTimeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "0j_vGiEse69c"
   },
   "outputs": [],
   "source": [
    "class TD3(TD3):\n",
    "    def trainNetwork(self,experiences , envActionRange,episode):\n",
    "        # this method trains the value network epoch number of times and is called by the trainAgent function\n",
    "        # it essentially uses the experiences to calculate target, using the targets it calculates the error, which\n",
    "        # is further used for calulating the loss. It then uses the optimizer over the loss\n",
    "        # to update the params of the network by backpropagating through the network\n",
    "        # this function does not return anything\n",
    "        # you can try out other loss functions other than MSE like Huber loss, MAE, etc.\n",
    "        #\n",
    "        actionlowval,actionhighval = envActionRange\n",
    "        states,actions,rewards,nextstates,dones= self.rbuffer.splitExperiences(experiences)\n",
    "        states = torch.tensor(states).float()\n",
    "        actions = torch.tensor(actions).float()\n",
    "        rewards = torch.tensor(rewards).float()\n",
    "        nextstates = torch.tensor(nextstates).float()\n",
    "        dones = torch.tensor(dones).float()\n",
    "        for ep in range(epochs):\n",
    "            as_noise = (actionhighval-actionlowval)*torch.randn(actions.shape)\n",
    "            as_noise = torch.clamp(as_noise,min=actionlowval,max=actionhighval)\n",
    "            argmax_a_qs_v = self.tpnet(nextstates)\n",
    "            noisy_argmax_a_qs_v = argmax_a_qs_v+as_noise\n",
    "            noisy_argmax_a_qs_v = torch.clamp(noisy_argmax_a_qs_v,min = actionlowval,max = actionhighval)\n",
    "\n",
    "            max1_a_qs_v = self.tvnet1(nextstates,noisy_argmax_a_qs_v)\n",
    "            max2_a_qs_v = self.tvnet2(nextstates,noisy_argmax_a_qs_v)\n",
    "            max_a_qs_v = torch.min(max1_a_qs_v,max2_a_qs_v)\n",
    "\n",
    "            target_qs = rewards.view(-1,1)+self.gamma*max_a_qs_v.detach()*(1-dones.view(-1,1))\n",
    "            # print(self.tvnet(torch.tensor(np.array(nextstates),dtype = torch.float32),argmax_a_qs_v),argmax_a_qs_v)\n",
    "            qs1 = self.ovnet1(states,actions)\n",
    "            qs2 = self.ovnet2(states,actions)\n",
    "            # print(tderr,qs)\n",
    "            vloss = loss_fn(target_qs,qs1)+loss_fn(target_qs,qs2)\n",
    "\n",
    "            self.voptim.zero_grad()\n",
    "            vloss.backward()\n",
    "            # nn.utils.clip_grad_norm_(self.ovnet1.parameters(), 1.0)\n",
    "            # nn.utils.clip_grad_norm_(self.ovnet2.parameters(), 1.0)\n",
    "            self.voptim.step()\n",
    "\n",
    "            if episode%self.trainPolicyFrequency==0:\n",
    "                argmax_a_qs_p = self.opnet(nextstates)\n",
    "                max_a_qs_p = self.ovnet1(nextstates,argmax_a_qs_p)\n",
    "                # max2_a_qs_p = self.ovnet2(nextstates,argmax_a_qs_p)\n",
    "                ploss = -torch.mean(max_a_qs_p)\n",
    "                self.poptim.zero_grad()\n",
    "                # print(grad(ploss,self.opnet.parameters(),retain_graph = True,allow_unused = True))\n",
    "                ploss.backward()\n",
    "\n",
    "                self.poptim.step()\n",
    "        for param, target_param in zip(self.ovnet1.parameters(), self.tvnet1.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "        for param, target_param in zip(self.ovnet2.parameters(), self.tvnet2.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "        for param, target_param in zip(self.opnet.parameters(), self.tpnet.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Np1uywWiSS2i"
   },
   "outputs": [],
   "source": [
    "class TD3(TD3):\n",
    "    def runTD3maml(self,epochs,ntasks,nshots=1,g=0):\n",
    "        ends = np.random.uniform(0, 2*np.pi, ntasks)\n",
    "        print(ends)\n",
    "        envs = [gym.make(\"mypendu1\", render_mode=None,end = end,g=g) for end in ends]\n",
    "        # envs_train = envs[:int(0.8*len(envs))]\n",
    "        # envs_test = envs[int(0.8*len(envs)):]\n",
    "        inner_lr = 0.001\n",
    "        meta_lr = 0.0001\n",
    "        shs = None\n",
    "        sha = None\n",
    "        indim,outdim,hdim = self.indim,self.outdim,self.hdim\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            vgrads1 = []\n",
    "            vgrads2 = []\n",
    "            pgrads = []\n",
    "            for env in envs:\n",
    "                totploss = 0\n",
    "                totvloss = 0\n",
    "                envActionRange = (self.env.action_space.low[0],self.env.action_space.high[0])\n",
    "                self.rbuffer.clean()\n",
    "                cctvnet1 = createValueNetwork(indim,1,outdim,hdim)\n",
    "                cctvnet2 = createValueNetwork(indim,1,outdim,hdim)\n",
    "                cctpnet = createPolicyNetwork(indim,outdim,envActionRange,hdim)\n",
    "\n",
    "                ccovnet1 = createValueNetwork(indim,1,outdim,hdim)\n",
    "                ccovnet2 = createValueNetwork(indim,1,outdim,hdim)\n",
    "                ccopnet = createPolicyNetwork(indim,outdim,envActionRange,hdim)\n",
    "\n",
    "                ccopnet.load_state_dict(self.opnet.state_dict())\n",
    "                ccovnet1.load_state_dict(self.ovnet1.state_dict())\n",
    "                ccovnet2.load_state_dict(self.ovnet2.state_dict())\n",
    "                cctvnet1.load_state_dict(ccovnet1.state_dict())\n",
    "                cctvnet2.load_state_dict(ccovnet2.state_dict())\n",
    "                cctpnet.load_state_dict(ccopnet.state_dict())\n",
    "                fast_v1 = dict((name,param) for (name,param) in ccovnet1.named_parameters())\n",
    "                fast_v2 = dict((name,param) for (name,param) in ccovnet2.named_parameters())\n",
    "                fast_p = dict((name,param) for (name,param) in ccopnet.named_parameters())\n",
    "                for e in range(self.mep):\n",
    "                    stps = 0\n",
    "                    # self.st_time = time.time()\n",
    "                    s,_ = env.reset()\n",
    "\n",
    "                    rew=0\n",
    "                    done = False\n",
    "                    trunc = False\n",
    "\n",
    "                    # self.traintime = 0\n",
    "                    while not (done or stps>self.mx_stps or trunc):\n",
    "                        a = self.gaussianStrategy(ccopnet,s,envActionRange,noiseScaleRatio)\n",
    "                        a = a.data.numpy().flatten()\n",
    "                        # print(env.step(a))\n",
    "                        stps+=1\n",
    "                        s_next,r,done,trunc,_ = env.step(a)\n",
    "\n",
    "                        rew+=r\n",
    "                        self.rbuffer.store(s,a,r,s_next,done)\n",
    "                        s = s_next\n",
    "                experiences = self.rbuffer.sample(self.rbuffer.length())\n",
    "                actionlowval,actionhighval = envActionRange\n",
    "                states,actions,rewards,nextstates,dones= self.rbuffer.splitExperiences(experiences)\n",
    "                states = torch.tensor(states).float()\n",
    "                shs = states.shape\n",
    "                sha = actions.shape\n",
    "                actions = torch.tensor(actions).float()\n",
    "                rewards = torch.tensor(rewards).float()\n",
    "                nextstates = torch.tensor(nextstates).float()\n",
    "                dones = torch.tensor(dones).float()\n",
    "                for _ in range(nshots):\n",
    "                    as_noise = (actionhighval-actionlowval)*torch.randn(actions.shape)\n",
    "                    as_noise = torch.clamp(as_noise,min=actionlowval,max=actionhighval)\n",
    "                    argmax_a_qs_v = cctpnet(nextstates)\n",
    "                    noisy_argmax_a_qs_v = argmax_a_qs_v+as_noise\n",
    "                    noisy_argmax_a_qs_v = torch.clamp(noisy_argmax_a_qs_v,min = actionlowval,max = actionhighval)\n",
    "\n",
    "                    max1_a_qs_v = cctvnet1(nextstates,noisy_argmax_a_qs_v)\n",
    "                    max2_a_qs_v = cctvnet2(nextstates,noisy_argmax_a_qs_v)\n",
    "                    max_a_qs_v = torch.min(max1_a_qs_v,max2_a_qs_v)\n",
    "\n",
    "                    target_qs = rewards.view(-1,1)+self.gamma*max_a_qs_v.detach()*(1-dones.view(-1,1))\n",
    "                    # print(self.tvnet(torch.tensor(np.array(nextstates),dtype = torch.float32),argmax_a_qs_v),argmax_a_qs_v)\n",
    "                    qs1 = ccovnet1(states,actions)\n",
    "                    qs2 = ccovnet2(states,actions)\n",
    "                    # print(tderr,qs)\n",
    "                    vloss1 = loss_fn(target_qs,qs1)\n",
    "                    vloss2 = loss_fn(target_qs,qs2)\n",
    "                    gd1 = grad(vloss1,ccovnet1.parameters())\n",
    "                    gd2 = grad(vloss2,ccovnet2.parameters())\n",
    "                    fast_v1 = dict((name, param - inner_lr * g) for ((name, param), g) in zip(fast_v1.items(), gd1))\n",
    "                    fast_v2 = dict((name, param - inner_lr * g) for ((name, param), g) in zip(fast_v2.items(), gd2))\n",
    "                    ccovnet1.load_state_dict(fast_v1)\n",
    "                    ccovnet2.load_state_dict(fast_v2)\n",
    "                    # nn.utils.clip_grad_norm_(self.ovnet1.parameters(), 1.0)\n",
    "                    # nn.utils.clip_grad_norm_(self.ovnet2.parameters(), 1.0)\n",
    "\n",
    "                    argmax_a_qs_p = ccopnet(nextstates)\n",
    "                    max_a_qs_p = ccovnet1(nextstates,argmax_a_qs_p)\n",
    "                    ploss = -torch.mean(max_a_qs_p)\n",
    "                    gdp = grad(ploss,ccopnet.parameters())\n",
    "                    fast_p = dict((name, param - inner_lr * g) for ((name, param), g) in zip(fast_p.items(), gdp))\n",
    "                    for param, target_param in zip(ccovnet1.parameters(), cctvnet1.parameters()):\n",
    "                        target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "                    for param, target_param in zip(ccovnet2.parameters(), cctvnet2.parameters()):\n",
    "                        target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "                    for param, target_param in zip(ccopnet.parameters(), cctpnet.parameters()):\n",
    "                        target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "                # actionlowval,actionhighval = envActionRange\n",
    "                # states,actions,rewards,nextstates,dones= self.rbuffer.splitExperiences(experiences)\n",
    "                # states = torch.tensor(states).float()\n",
    "                # actions = torch.tensor(actions).float()\n",
    "                # rewards = torch.tensor(rewards).float()\n",
    "                # nextstates = torch.tensor(nextstates).float()\n",
    "                # dones = torch.tensor(dones).float()\n",
    "                as_noise = (actionhighval-actionlowval)*torch.randn(actions.shape)\n",
    "                as_noise = torch.clamp(as_noise,min=actionlowval,max=actionhighval)\n",
    "                argmax_a_qs_v = cctpnet(nextstates)\n",
    "                noisy_argmax_a_qs_v = argmax_a_qs_v+as_noise\n",
    "                noisy_argmax_a_qs_v = torch.clamp(noisy_argmax_a_qs_v,min = actionlowval,max = actionhighval)\n",
    "\n",
    "                max1_a_qs_v = cctvnet1(nextstates,noisy_argmax_a_qs_v)\n",
    "                max2_a_qs_v = cctvnet2(nextstates,noisy_argmax_a_qs_v)\n",
    "                max_a_qs_v = torch.min(max1_a_qs_v,max2_a_qs_v)\n",
    "\n",
    "                target_qs = rewards.view(-1,1)+self.gamma*max_a_qs_v.detach()*(1-dones.view(-1,1))\n",
    "                # print(self.tvnet(torch.tensor(np.array(nextstates),dtype = torch.float32),argmax_a_qs_v),argmax_a_qs_v)\n",
    "                qs1 = ccovnet1(states,actions)\n",
    "                qs2 = ccovnet2(states,actions)\n",
    "                # print(tderr,qs)\n",
    "                totvloss += loss_fn(target_qs,qs1)+loss_fn(target_qs,qs2)\n",
    "                # nn.utils.clip_grad_norm_(self.ovnet1.parameters(), 1.0)\n",
    "                # nn.utils.clip_grad_norm_(self.ovnet2.parameters(), 1.0)\n",
    "                # print(e)\n",
    "                argmax_a_qs_p = ccopnet(nextstates)\n",
    "                max_a_qs_p = ccovnet1(nextstates,argmax_a_qs_p)\n",
    "                # max2_a_qs_p = self.ovnet2(nextstates,argmax_a_qs_p)\n",
    "                totploss += -torch.mean(max_a_qs_p)\n",
    "                for param, target_param in zip(ccovnet1.parameters(), cctvnet1.parameters()):\n",
    "                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "                for param, target_param in zip(ccovnet2.parameters(), cctvnet2.parameters()):\n",
    "                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "                for param, target_param in zip(ccopnet.parameters(), cctpnet.parameters()):\n",
    "                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "                gv1 = torch.autograd.grad(totvloss, ccovnet1.parameters(), create_graph=True)\n",
    "                gv2  = torch.autograd.grad(totvloss, ccovnet2.parameters(), create_graph=True)\n",
    "                # print(totvloss,totploss)\n",
    "                gp = torch.autograd.grad(totploss, ccopnet.parameters(), create_graph=True)\n",
    "                vmeta_grads1 = {name:g for ((name, _), g) in zip(ccovnet1.named_parameters(), gv1)}\n",
    "                vmeta_grads2 = {name:g for ((name, _), g) in zip(ccovnet2.named_parameters(), gv2)}\n",
    "                pmeta_grads = {name:g for ((name, _), g) in zip(ccopnet.named_parameters(), gp)}\n",
    "                # print(vmeta_grads1)\n",
    "                vgrads1.append(vmeta_grads1)\n",
    "                vgrads2.append(vmeta_grads2)\n",
    "                pgrads.append(pmeta_grads)\n",
    "            output1 = self.ovnet1(torch.zeros(shs),torch.zeros(sha))\n",
    "            trg = torch.zeros(output1.shape)\n",
    "            output2 = self.ovnet2(torch.zeros(shs),torch.zeros(sha))\n",
    "            loss1 = loss_fn(output1,trg)\n",
    "            loss1.backward(retain_graph=True)\n",
    "            loss2 = loss_fn(output2,trg)\n",
    "            loss2.backward(retain_graph=True)\n",
    "            outputp = self.opnet(torch.zeros(shs))\n",
    "            max_a_qs_p = self.ovnet1(nextstates,outputp)\n",
    "            lossp = -torch.mean(max_a_qs_p)\n",
    "            lossp.backward(retain_graph=True)\n",
    "            # print(lossp)\n",
    "            gradientsv1 = {k: sum(d[k] for d in vgrads1) for k in vgrads1[0].keys()}\n",
    "            gradientsv2 = {k: sum(d[k] for d in vgrads2) for k in vgrads2[0].keys()}\n",
    "            gradientsp = {k: sum(d[k] for d in pgrads) for k in pgrads[0].keys()}\n",
    "            hooks = []\n",
    "            for(k,v) in self.opnet.named_parameters():\n",
    "                def get_closure():\n",
    "                    key = k\n",
    "                    def replace_grad(grad):\n",
    "                        return gradientsp[key]\n",
    "                    return replace_grad\n",
    "                hooks.append(v.register_hook(get_closure()))\n",
    "            torch.optim.Adam(self.opnet.parameters(), lr=meta_lr).zero_grad()\n",
    "            lossp.backward()\n",
    "\n",
    "            torch.optim.Adam(self.opnet.parameters(), lr=meta_lr).step()\n",
    "\n",
    "            # Remove the hooks before next training phase\n",
    "            for h in hooks:\n",
    "                h.remove()\n",
    "\n",
    "            hooks = []\n",
    "            for(k,v) in self.ovnet1.named_parameters():\n",
    "                def get_closure():\n",
    "                    key = k\n",
    "                    def replace_grad(grad):\n",
    "                        return gradientsv1[key]\n",
    "                    return replace_grad\n",
    "                hooks.append(v.register_hook(get_closure()))\n",
    "            torch.optim.Adam(self.ovnet1.parameters(), lr=meta_lr).zero_grad()\n",
    "            loss1.backward()\n",
    "\n",
    "            torch.optim.Adam(self.ovnet1.parameters(), lr=meta_lr).step()\n",
    "\n",
    "            # Remove the hooks before next training phase\n",
    "            for h in hooks:\n",
    "                h.remove()\n",
    "            hooks = []\n",
    "            for(k,v) in self.ovnet2.named_parameters():\n",
    "                def get_closure():\n",
    "                    key = k\n",
    "                    def replace_grad(grad):\n",
    "                        return gradientsv1[key]\n",
    "                    return replace_grad\n",
    "                hooks.append(v.register_hook(get_closure()))\n",
    "            torch.optim.Adam(self.ovnet2.parameters(), lr=meta_lr).zero_grad()\n",
    "            loss2.backward()\n",
    "\n",
    "            torch.optim.Adam(self.ovnet2.parameters(), lr=meta_lr).step()\n",
    "\n",
    "            # Remove the hooks before next training phase\n",
    "            for h in hooks:\n",
    "                h.remove()\n",
    "            # self.meta_test(envs[0],nshots=nshots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(TD3):\n",
    "    def runTD3reptile(self,epochs,ntasks,nshots=1,g=0):\n",
    "        ends = np.random.uniform(0, 2*np.pi, ntasks)\n",
    "        print(ends)\n",
    "        envs = [gym.make(\"mypendu1\", render_mode=None,end = end,g=g) for end in ends]\n",
    "        # envs_train = envs[:int(0.8*len(envs))]\n",
    "        # envs_test = envs[int(0.8*len(envs)):]\n",
    "        inner_lr = 0.0005\n",
    "        meta_lr = 0.0001\n",
    "        shs = None\n",
    "        sha = None\n",
    "        indim,outdim,hdim = self.indim,self.outdim,self.hdim\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            vgrads1 = []\n",
    "            vgrads2 = []\n",
    "            pgrads = []\n",
    "            for env in envs:\n",
    "                totploss = 0\n",
    "                totvloss = 0\n",
    "                envActionRange = (self.env.action_space.low[0],self.env.action_space.high[0])\n",
    "                self.rbuffer.clean()\n",
    "                \n",
    "                cctvnet1 = createValueNetwork(indim,1,outdim,hdim)\n",
    "                cctvnet2 = createValueNetwork(indim,1,outdim,hdim)\n",
    "                cctpnet = createPolicyNetwork(indim,outdim,envActionRange,hdim)\n",
    "\n",
    "                ccovnet1 = createValueNetwork(indim,1,outdim,hdim)\n",
    "                ccovnet2 = createValueNetwork(indim,1,outdim,hdim)\n",
    "                ccopnet = createPolicyNetwork(indim,outdim,envActionRange,hdim)\n",
    "\n",
    "                ccopnet.load_state_dict(self.opnet.state_dict())\n",
    "                ccovnet1.load_state_dict(self.ovnet1.state_dict())\n",
    "                ccovnet2.load_state_dict(self.ovnet2.state_dict())\n",
    "                cctvnet1.load_state_dict(ccovnet1.state_dict())\n",
    "                cctvnet2.load_state_dict(ccovnet2.state_dict())\n",
    "                cctpnet.load_state_dict(ccopnet.state_dict())\n",
    "                \n",
    "                optov1 = torch.optim.Adam(ccovnet1.parameters(),lr = inner_lr)\n",
    "                optov2 = torch.optim.Adam(ccovnet2.parameters(),lr = inner_lr)\n",
    "                optop = torch.optim.Adam(ccopnet.parameters(),lr = inner_lr)\n",
    "                \n",
    "                # fast_v1 = dict((name,param) for (name,param) in ccovnet1.named_parameters())\n",
    "                # fast_v2 = dict((name,param) for (name,param) in ccovnet2.named_parameters())\n",
    "                # fast_p = dict((name,param) for (name,param) in ccopnet.named_parameters())\n",
    "                \n",
    "\n",
    "                \n",
    "                for _ in range(nshots):\n",
    "                    for e in range(self.mep):\n",
    "                        stps = 0\n",
    "                        s,_ = env.reset()\n",
    "                        rew=0\n",
    "                        done = False\n",
    "                        trunc = False\n",
    "                        while not (done or stps>self.mx_stps or trunc):\n",
    "                            a = self.gaussianStrategy(ccopnet,s,envActionRange,noiseScaleRatio)\n",
    "                            a = a.data.numpy().flatten()\n",
    "                            stps+=1\n",
    "                            s_next,r,done,trunc,_ = env.step(a)\n",
    "                            rew+=r\n",
    "                            self.rbuffer.store(s,a,r,s_next,done)\n",
    "                            s = s_next\n",
    "                    experiences = self.rbuffer.sample(self.rbuffer.length())\n",
    "                    actionlowval,actionhighval = envActionRange\n",
    "                    states,actions,rewards,nextstates,dones= self.rbuffer.splitExperiences(experiences)\n",
    "                    states = torch.tensor(states).float()\n",
    "                    shs = states.shape\n",
    "                    sha = actions.shape\n",
    "                    actions = torch.tensor(actions).float()\n",
    "                    rewards = torch.tensor(rewards).float()\n",
    "                    nextstates = torch.tensor(nextstates).float()\n",
    "                    dones = torch.tensor(dones).float()\n",
    "\n",
    "\n",
    "                    \n",
    "                    as_noise = (actionhighval-actionlowval)*torch.randn(actions.shape)\n",
    "                    as_noise = torch.clamp(as_noise,min=actionlowval,max=actionhighval)\n",
    "                    argmax_a_qs_v = cctpnet(nextstates)\n",
    "                    noisy_argmax_a_qs_v = argmax_a_qs_v+as_noise\n",
    "                    noisy_argmax_a_qs_v = torch.clamp(noisy_argmax_a_qs_v,min = actionlowval,max = actionhighval)\n",
    "\n",
    "                    max1_a_qs_v = cctvnet1(nextstates,noisy_argmax_a_qs_v)\n",
    "                    max2_a_qs_v = cctvnet2(nextstates,noisy_argmax_a_qs_v)\n",
    "                    max_a_qs_v = torch.min(max1_a_qs_v,max2_a_qs_v)\n",
    "\n",
    "                    target_qs = rewards.view(-1,1)+self.gamma*max_a_qs_v.detach()*(1-dones.view(-1,1))\n",
    "                    # print(self.tvnet(torch.tensor(np.array(nextstates),dtype = torch.float32),argmax_a_qs_v),argmax_a_qs_v)\n",
    "                    qs1 = ccovnet1(states,actions)\n",
    "                    qs2 = ccovnet2(states,actions)\n",
    "                    # print(tderr,qs)\n",
    "                    vloss1 = loss_fn(target_qs,qs1)\n",
    "                    vloss2 = loss_fn(target_qs,qs2)\n",
    "\n",
    "                    vloss1.backward()\n",
    "                    vloss2.backward()\n",
    "                    optov1.step()\n",
    "                    optov2.step()\n",
    "                    \n",
    "                    argmax_a_qs_p = ccopnet(nextstates)\n",
    "                    max_a_qs_p = ccovnet1(nextstates,argmax_a_qs_p)\n",
    "                    ploss = -torch.mean(max_a_qs_p)\n",
    "                    ploss.backward()\n",
    "                    optop.step()\n",
    "                    \n",
    "                    for param, target_param in zip(ccovnet1.parameters(), cctvnet1.parameters()):\n",
    "                        target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "                    for param, target_param in zip(ccovnet2.parameters(), cctvnet2.parameters()):\n",
    "                        target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "                    for param, target_param in zip(ccopnet.parameters(), cctpnet.parameters()):\n",
    "                        target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "                # actionlowval,actionhighval = envActionRange\n",
    "                # states,actions,rewards,nextstates,dones= self.rbuffer.splitExperiences(experiences)\n",
    "                # states = torch.tensor(states).float()\n",
    "                # actions = torch.tensor(actions).float()\n",
    "                # rewards = torch.tensor(rewards).float()\n",
    "                # nextstates = torch.tensor(nextstates).float()\n",
    "                # dones = torch.tensor(dones).float()\n",
    "                \n",
    "                # vmeta_grads1 = {name:g for ((name, _), g) in zip(ccovnet1.named_parameters(), gv1)}\n",
    "                # vmeta_grads2 = {name:g for ((name, _), g) in zip(ccovnet2.named_parameters(), gv2)}\n",
    "                # pmeta_grads = {name:g for ((name, _), g) in zip(ccopnet.named_parameters(), gp)}\n",
    "                # print(vmeta_grads1)\n",
    "                vgrads1.append(\n",
    "                [param.data - model_param.data for param, model_param in zip(ccovnet1.parameters(), self.ovnet1.parameters())]\n",
    "            )\n",
    "                vgrads2.append(\n",
    "                [param.data - model_param.data for param, model_param in zip(ccovnet2.parameters(), self.ovnet2.parameters())]\n",
    "            )\n",
    "                pgrads.append(\n",
    "                [param.data - model_param.data for param, model_param in zip(ccopnet.parameters(), self.opnet.parameters())]\n",
    "            )\n",
    "            outoptv1 = torch.optim.Adam(self.ovnet1.parameters(),lr = meta_lr)\n",
    "            outoptv2 = torch.optim.Adam(self.ovnet2.parameters(),lr = meta_lr)\n",
    "            outoptp = torch.optim.Adam(self.opnet.parameters(),lr = meta_lr)\n",
    "            \n",
    "            meta_gv1 = torch.stack(vgrads1).mean(dim=0)\n",
    "            meta_gv2 = torch.stack(vgrads2).mean(dim=0)\n",
    "            meta_gp = torch.stack(pgrads).mean(dim=0)\n",
    "\n",
    "            for param, grad in zip(self.ovnet1.parameters(),meta_gv1 ):\n",
    "                param.grad = grad.clone()\n",
    "            outoptv1.step()\n",
    "            for param, grad in zip(self.ovnet2.parameters(),meta_gv2 ):\n",
    "                param.grad = grad.clone()\n",
    "            outoptv2.step()\n",
    "            for param, grad in zip(self.opnet.parameters(),meta_gp ):\n",
    "                param.grad = grad.clone()\n",
    "            outoptp.step()\n",
    "\n",
    "            # self.meta_test(envs[0],nshots=nshots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "foUkWInkSS2i"
   },
   "outputs": [],
   "source": [
    "class TD3(TD3):\n",
    "    def meta_test(self,env,nshots=1):\n",
    "        tccovnet1 = copy.deepcopy(self.ovnet1)\n",
    "        tccovnet2 = copy.deepcopy(self.ovnet2)\n",
    "        tccopnet = copy.deepcopy(self.opnet)\n",
    "        tcctvnet1 = copy.deepcopy(self.tvnet1)\n",
    "        tcctvnet2 = copy.deepcopy(self.tvnet2)\n",
    "        tcctpnet = copy.deepcopy(self.tpnet)\n",
    "        opt1 = torch.optim.Adam(tccovnet1.parameters())\n",
    "        opt2 = torch.optim.Adam(tccovnet2.parameters())\n",
    "        opt3 = torch.optim.Adam(tccopnet.parameters())\n",
    "        self.rbuffer.clean()\n",
    "        envActionRange = (env.action_space.low[0],env.action_space.high[0])\n",
    "        # for e in range(128):\n",
    "\n",
    "        for _ in range(nshots):\n",
    "            # for e in range(self.mep):\n",
    "            stps = 0\n",
    "            s,_ = env.reset()\n",
    "            rew=0\n",
    "            done = False\n",
    "            trunc = False\n",
    "            while not (done or stps>self.mx_stps or trunc):\n",
    "                a = self.gaussianStrategy(tccopnet,s,envActionRange,noiseScaleRatio)\n",
    "                a = a.data.numpy().flatten()\n",
    "                stps+=1\n",
    "                s_next,r,done,trunc,_ = env.step(a)\n",
    "                rew+=r\n",
    "                self.rbuffer.store(s,a,r,s_next,done)\n",
    "                s = s_next\n",
    "            experiences = self.rbuffer.sample(self.rbuffer.length())\n",
    "            actionlowval,actionhighval = envActionRange\n",
    "            states,actions,rewards,nextstates,dones= self.rbuffer.splitExperiences(experiences)\n",
    "            states = torch.tensor(states).float()\n",
    "            shs = states.shape\n",
    "            sha = actions.shape\n",
    "            actions = torch.tensor(actions).float()\n",
    "            rewards = torch.tensor(rewards).float()\n",
    "            nextstates = torch.tensor(nextstates).float()\n",
    "            dones = torch.tensor(dones).float()\n",
    "            \n",
    "            as_noise = (actionhighval-actionlowval)*torch.randn(actions.shape)\n",
    "            as_noise = torch.clamp(as_noise,min=actionlowval,max=actionhighval)\n",
    "            argmax_a_qs_v = tcctpnet(nextstates)\n",
    "            noisy_argmax_a_qs_v = argmax_a_qs_v+as_noise\n",
    "            noisy_argmax_a_qs_v = torch.clamp(noisy_argmax_a_qs_v,min = actionlowval,max = actionhighval)\n",
    "\n",
    "            max1_a_qs_v = tcctvnet1(nextstates,noisy_argmax_a_qs_v)\n",
    "            max2_a_qs_v = tcctvnet2(nextstates,noisy_argmax_a_qs_v)\n",
    "            max_a_qs_v = torch.min(max1_a_qs_v,max2_a_qs_v)\n",
    "\n",
    "            target_qs = rewards.view(-1,1)+self.gamma*max_a_qs_v.detach()*(1-dones.view(-1,1))\n",
    "            # print(self.tvnet(torch.tensor(np.array(nextstates),dtype = torch.float32),argmax_a_qs_v),argmax_a_qs_v)\n",
    "            qs1 = tccovnet1(states,actions)\n",
    "            qs2 = tccovnet2(states,actions)\n",
    "            # print(tderr,qs)\n",
    "            vloss1 = loss_fn(target_qs,qs1)\n",
    "            vloss2 = loss_fn(target_qs,qs2)\n",
    "            opt1.zero_grad()\n",
    "            opt2.zero_grad()\n",
    "            vloss1.backward()\n",
    "            vloss2.backward()\n",
    "            opt1.step()\n",
    "            opt2.step()\n",
    "            # nn.utils.clip_grad_norm_(self.ovnet1.parameters(), 1.0)\n",
    "            # nn.utils.clip_grad_norm_(self.ovnet2.parameters(), 1.0)\n",
    "\n",
    "            argmax_a_qs_p = tccopnet(nextstates)\n",
    "            max_a_qs_p = tccovnet1(nextstates,argmax_a_qs_p)\n",
    "            ploss = -torch.mean(max_a_qs_p)\n",
    "            opt3.zero_grad()\n",
    "            ploss.backward()\n",
    "            opt3.step()\n",
    "            for param, target_param in zip(tccovnet1.parameters(), tcctvnet1.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "            for param, target_param in zip(tccovnet2.parameters(), tcctvnet2.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "            for param, target_param in zip(tccopnet.parameters(), tcctpnet.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\n",
    "        rew = []\n",
    "        tccopnet.eval()\n",
    "\n",
    "        for e in range(self.eep):\n",
    "            stps = 0\n",
    "            rs= 0\n",
    "            s,_ = env.reset()\n",
    "\n",
    "            done = False\n",
    "            trunc = False\n",
    "            for c in count():\n",
    "                a = self.greedyStrategy(tccopnet,s,envActionRange)\n",
    "                a = a.data.numpy().flatten()\n",
    "                stps+=1\n",
    "                s,r,done,trunc,_ = env.step(a)\n",
    "                rs+=r\n",
    "                if done or stps>self.mx_stps or trunc:\n",
    "                    rew.append(rs)\n",
    "                    break\n",
    "        # print(np.sum(rew))\n",
    "        return np.sum(rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "mqFVbqC3e69d"
   },
   "outputs": [],
   "source": [
    "class TD3(TD3):\n",
    "    def evaluateAgent(self):\n",
    "        #this function evaluates the agent using the value network, it evaluates agent for MAX_EVAL_EPISODES\n",
    "        #typcially MAX_EVAL_EPISODES = 1\n",
    "        #\n",
    "        rew = []\n",
    "        self.opnet.eval()\n",
    "        envActionRange = (self.env.action_space.low[0],self.env.action_space.high[0])\n",
    "        for e in range(self.eep):\n",
    "            stps = 0\n",
    "            rs= 0\n",
    "            s,_ = self.env.reset()\n",
    "\n",
    "            done = False\n",
    "            trunc = False\n",
    "            for c in count():\n",
    "                a = self.greedyStrategy(self.opnet,s,envActionRange)\n",
    "                a = a.data.numpy().flatten()\n",
    "                stps+=1\n",
    "                s,r,done,trunc,_ = self.env.step(a)\n",
    "                rs+=r\n",
    "                if done or stps>self.mx_stps or trunc:\n",
    "                    rew.append(rs)\n",
    "                    break\n",
    "        print(np.sum(rew))\n",
    "        self.performBookKeeping(rew,train=False)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "l_fIog0ZSS2j"
   },
   "outputs": [],
   "source": [
    "class TD3(TD3):\n",
    "    def huvaluate(self,env,seed=None):\n",
    "        rew = []\n",
    "        envActionRange = (self.env.action_space.low[0],self.env.action_space.high[0])\n",
    "        self.opnet.eval()\n",
    "        for e in range(self.eep):\n",
    "            rs= 0\n",
    "            stps = 0\n",
    "            s,_ = env.reset(seed=seed)\n",
    "            done = False\n",
    "            trunc = False\n",
    "            for c in count():\n",
    "                a = self.greedyStrategy(self.opnet,s,envActionRange)\n",
    "                a = a.data.numpy().flatten()\n",
    "                env.render()\n",
    "                stps+=1\n",
    "                s,r,done,trunc,_ = env.step(a)\n",
    "                rs+=r\n",
    "                if done or stps>self.mx_stps or trunc:\n",
    "                    print(stps)\n",
    "                    rew.append(rs)\n",
    "                    break\n",
    "        print(np.sum(rew))\n",
    "        # self.performBookKeeping(rew,train=False)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "Epuq1otHSS2j"
   },
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bsZjhNhITeaa",
    "outputId": "66639a91-d00a-468b-ca88-d1ae068f6678",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# env = gym.make('CartPole-v0')\n",
    "# env = gym.make(\"Hopper-v4\")\n",
    "# env = gym.make(\"Pendulum-v1\")\n",
    "# env = gym.make(\"HalfCheetah-v4\")\n",
    "# print(env.action_space)\n",
    "target_angle = 1*np.pi/2\n",
    "g = 0.0\n",
    "env = gym.make(\"mypendu1\", render_mode=None,end = target_angle,g=g)\n",
    "minSamples = 128\n",
    "epochs = 1\n",
    "noiseScaleRatio = 1\n",
    "nshots=1\n",
    "# loss_fn = nn.SmoothL1Loss()\n",
    "loss_fn = nn.MSELoss()\n",
    "# td3 = TD3(env,0,0.99,0.005,100000,256,[256,512,256],1,1,1,'adam','adam',0.0005,0.0005,1,1,16,_,100)\n",
    "td3 = TD3(env,0,0.99,0.005,100000,256,[100,100],1,1,1,'adam','adam',0.0005,0.0005,1,1,16,_,100)\n",
    "# plt_list = td3.runTD3()\n",
    "# td3.runTD3maml(100,20,nshots=nshots,g=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "AnUNM6ONSS2k"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'meta_demo_256_512_256_15_7.pt'\n",
    "# pth = 'meta_demo_256_512_256_10.pt'\n",
    "'meta_demo_256_512_512_256_25_2.pt'\n",
    "'reptile_demo_256_512_256_15_7.pt'\n",
    "pth = 'reptile_demo_256_512_256_30_7.pt'\n",
    "'reptile_demo_256_512_512_256_25_2.pt'\n",
    "chkpt = torch.load(pth)\n",
    "td3.ovnet1.load_state_dict(chkpt[\"ovnet1\"])\n",
    "td3.ovnet2.load_state_dict(chkpt[\"ovnet2\"])\n",
    "td3.opnet.load_state_dict(chkpt[\"opnet\"])\n",
    "td3.tvnet1.load_state_dict(chkpt[\"tvnet1\"])\n",
    "td3.tvnet2.load_state_dict(chkpt[\"tvnet2\"])\n",
    "td3.tpnet.load_state_dict(chkpt[\"tpnet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rSOFbwDESS2k",
    "outputId": "4c99498c-96c6-4c4d-abbb-97945822f99f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6., 4., 6., 7., 5., 2., 0., 0., 0., 0., 0., 1., 0., 0., 2., 2., 0.,\n",
       "        3., 2., 0., 1., 5., 1., 0., 3.]),\n",
       " array([-484.27869229, -470.77455511, -457.27041793, -443.76628075,\n",
       "        -430.26214357, -416.7580064 , -403.25386922, -389.74973204,\n",
       "        -376.24559486, -362.74145768, -349.2373205 , -335.73318333,\n",
       "        -322.22904615, -308.72490897, -295.22077179, -281.71663461,\n",
       "        -268.21249743, -254.70836026, -241.20422308, -227.7000859 ,\n",
       "        -214.19594872, -200.69181154, -187.18767437, -173.68353719,\n",
       "        -160.17940001, -146.67526283]),\n",
       " <BarContainer object of 25 artists>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"mypendu1\", render_mode = None,start = [1*np.pi/4,0.01],end = np.pi/2,g=0)\n",
    "loss = []\n",
    "for _ in range(50):\n",
    "    loss.append(td3.meta_test(env,nshots=1))\n",
    "plt.hist(loss,bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {
    "id": "f3aqvkfP3g0z"
   },
   "outputs": [],
   "source": [
    "# pth = \"meta_demo_256_512_256_10.pt\"\n",
    "# torch.save({\"tvnet1\":td3.tvnet1.state_dict(),\n",
    "#             \"tvnet2\":td3.tvnet2.state_dict(),\n",
    "#             \"tpnet\":td3.tpnet.state_dict(),\n",
    "#             \"ovnet1\":td3.ovnet1.state_dict(),\n",
    "#             \"ovnet2\":td3.ovnet2.state_dict(),\n",
    "#             \"opnet\":td3.opnet.state_dict()},pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 291
    },
    "id": "_aDu_E8Q4hqj",
    "outputId": "2c087266-1b49-45e3-91ba-3db5f5bbf9a5"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Cannot find file: /content/meta_demo_256_512_256_10.pt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-033695b26243>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    223\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Cannot find file: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m   \u001b[0mcomm_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file: /content/meta_demo_256_512_256_10.pt"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download(\"/content/{}\".format(pth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 861
    },
    "id": "BKJ3xQE7Te-F",
    "outputId": "600b1225-c19f-469f-83f4-d99eac815c9f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7a8d51511c90>]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAGdCAYAAAAWp6lMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAA9hAAAPYQGoP6dpAABKNElEQVR4nO3de1yUZd4/8M9wGg7BcBQYOVuKiqWC5VhG1AoeytzcirXHZCvLVX9uok+mtavSqm2Z9WQZHTzU5nZatTKtMFMrQwXDBM8HTgpIKMyIyMzAXL8/kDtHARmYs5/36zUvnZlrZq7bW7g/c93XdX9lQggBIiIiIifmYusOEBEREVkaAw8RERE5PQYeIiIicnoMPEREROT0GHiIiIjI6THwEBERkdNj4CEiIiKnx8BDRERETs/N1h2wFwaDARUVFfD19YVMJrN1d4iIiKgThBA4f/48lEolXFzaH8dh4LmkoqICkZGRtu4GERERdUF5eTkiIiLafZ6B5xJfX18ALf9gfn5+Nu4NERERdYZGo0FkZKR0HG8PA88lraex/Pz8GHiIiIgczLWmo3DSMhERETk9Bh4iIiJyegw8RERE5PQYeIiIiMjpMfAQERGR02PgISIiIqfHwENEREROj4GHiIiInB4DDxERETk9pwo8K1asQGxsLDw9PZGYmIgff/zR1l0iIiIiO+A0geeTTz7B008/jeeeew4FBQUYPnw4Ro0ahbKyMlt3jYiIiGxMJoQQtu6EOdx2220YPHgw3nrrLemxvn37Yty4cViyZMk1X6/RaKBQKKBWq1lLi4iIyEF09vjtFMVDdTod9u7di2effdbo8dTUVPz8889tvkar1UKr1Ur3NRqNRftIRETXD/VFPf6dW4K6Bj1cXWVwc5HB1cXl0p8y4z9d23rc5bLnf3/c3bXl8UBvD0QHeV+zYCb9zikCT01NDZqbmxEaGmr0eGhoKKqqqtp8zZIlS7Bw4UJrdI+IiK4j5eca8NiaPByrrrfo58SF+GBUQhhGJYSjv9KP4ecanCLwtLpyZwsh2v0PMHfuXGRmZkr3NRoNIiMjLdo/IiJybr+W1+Hx9/NRU69FqJ8c4wb1hMEg0GQQaDYI6JsFmg0G6X6TQaC5ufX5Kx43XPZ4c8v91seq1I04+dsFvLntBN7cdgIRAV4YlRCGkQnhGBTpDxcX+wo/6ot67C09hyExgfD1dLdJH5wi8AQHB8PV1fWq0Zzq6uqrRn1ayeVyyOVya3SPyKqEEPjtvBYNumZEBnrD1c5+8RE5q5wDVZjxcQEa9Qb0DffDqowkhCu8LPJZ5xv1+P5wNb4pqsK2I9U4VXsR7/5YjHd/LEaYnyfS+odiZEI4bo0NtMnvgIq6i8grOYe8knPIL6nFkTPnIQSwKiMJd8e3fVy2NKcIPB4eHkhMTMSWLVvwxz/+UXp8y5YtuP/++23YMyLLMBgEzpxvRElNA0rPXkDx2QsorWlAydkLKD3bgIv6ZgCAt4cr4sN80V+pQH+lH/op/dA71Bee7q423gIi5yGEwKqdJfjnpoMQAkjuHYI3HxmMG+SWO8T6errj/oE9cf/Anrioa8aOo9X4uqgKWw9Vo0rTiPdzS/F+bimCfDyQein8DOsVBHdX8y/ONhgEjv9Wjz3F55Bfcg55JbU4XXfxqnZxwT5o1BvM/vmd5TSrtD755BNMnDgR2dnZUKlUeOedd/Duu+/iwIEDiI6OvubruUqrhRACH+eV49P8cjyYGIn0IZF2NzR6vWg2CFSqL6L0bEuQKam5gJKzLQGn9GwDtE3t/+JwkQHuri5ttnFzkeHGHjegX3hLAOqvVKCf0g8KL9sMM1PbOjolT/aj2SCQtfEA3s8tBQA8clsUFo7tDzcLBIvO0DY1Y+fxGnxdWIUth86grkEvPefn6YY/9AvFqIRwDL8puMtffHRNBhSerkNeSS3yS84hv7TW6HMAwNVFhv5KPwyJCcSQmAAkRgcixNcyZ1U6e/x2msADtFx48KWXXkJlZSUSEhLw6quv4s477+zUaxl4gOrzjZi7rhBbD1dLjw2JCcCSBwbgxh6+NuyZ82pqNqCirvHSyExLoGkJNhdQfu4idM3thxpXFxkiA7wQE+yDmCAfRAd5S39GBLScyiquqceBCg0OVmhwoEKDAxVq1F7xi6lVRIAX+rcGoHA/9O/phzA/Tx50beBUbQP+sjoPHm4uWDi2P5JiAm3dJWrDBW0TZnxUIP3OnDsqHk/eGWc3PzP6ZgN2nzyHr4sq8e2BKtTU66TnfDxckRLfA6MSwnFXnxD4dDAapWnU45fSWuSX1GJPyTn8Wl531ZcpL3dXDI72R1J0IIbEBGJQlH+H72lO12Xg6Y7rPfB8U1SFeRsKce6CDh6uLvjjoJ7YuL8CDbpmeLi64K939cLUlF6Qu/FUiLlsO1KNpz/eB/XFtgMIALi7yhAZ+HuQiQ32QXSQD2KCvKH09zJ5eFoIgSpNIw6c/j0AHazU4FTt1cPPABDo49ESfpS/jwbFBvtwXpAFVWsa8eDbuSg92yA99shtUXhmZDxH4exItaYRj72fh6LTGsjdXPDqwwMxekC4rbvVrmaDQH7JOXxdVIVvD1ShUt0oPSd3c0Fy7xCMGhCGe/qG4qKuuWX+TXHL6anDVRoYrkgKQT4eSIoJuDSCE4h+Sj+LnC7rDAYeE12vged8ox4LNx7Ef/eeAgD0DffDaw8PRJ8wX5yuu4i/f16E7y99e+kV4oPFfxyA2+KCbNllp/Dz8RpkrMmDrskADzcXRAd6S0EmOrjlz5ggHyj9vawSLtQNehyoVOPgZaNBx3+rR/OVv+XQ8k0uTOEJf293+Hu5w9/bAwovd+l+gE/rfY9Lz7vD19OdIakTzl3Q4eG3c3Gsuh6RgV4YGhuEzy79bIb4yrHgvv4YPSDMbkYQrleHqzR4bHUeKtSNCPLxwLuTkjA4KsDW3eo0IQR+PaXG14WV+LqoCmXnfg/XLjJcFW4AIDrIG0nRgbg1NgBJMYGIC/axm/+HDDwmuh4Dz+6TZ5H56a84XXcRMhnw1J29MHPETUajOEIIbC6swvwvD6CmvuVCjX++NRLPjuwLhTe/bXbF3tJaTFy5Gw26Zvyhbw+seCQRHm72V+WlUd+Mo2fO/z4SVKHBocrz0oRoU8hkgJ+nOwK83aG4LAj5e/1+P8DHHT39vTEkJsBufpFak6ZRjwnv7kLRaQ3C/Dzx2RQVIgO9sfvkWczdUIiTv10AANwT3wNZ4xLQ098yq3+oYz8c/Q3T1v6C89omxIX4YE3GrYgK8rZ1t7pMCIGDlRp8U1SFr4uqcLy6Hi6yli+/raM3Q2IC0MPP09ZdbRcDj4mup8CjbWrGspyjeOfHkxCiZe7GsocG4tbY9ucJqBv0ePGbw/hoT0ttsuAb5Fgwth/GDAi/Lg9OXVV0Wo0/v7sL5xubcMeNwXhvUpJDrZhqNgiUnr2Amnodaht0UDfoUXdRh7oGPeou6qFu0KO2oeW++qIedQ06XNCZFpBujQ1E1v39ER/m3D+Hl2vQNeHRlXuQX1qLIB8PfPKUCjf2uEF6XtvUjBXbTuCt7SegazbA28MVs1L7IGNYjFOPnBkMAidrLqDwdB1KahpwW1wgVHFBNvud8/GeMjz3eRGaDQK3xgbinYmJ8Pf2sElfLKWi7iJ8Pd1sdq2crmDgMdH1EngOVWow85N9OFx1HgDwcFIk/n5fv04vn9xTfA5z1+/HiUvfNu+O74EX+G2zU46dOY+H39mFcxd0GBITgPcfuxXeHk5xZYgO6ZoMUvipu6hvCUcNukuPXQpIl57fW1qLRr0Bri4yTBwajZkjejv9vJVGfTOeeD8fPx2vgZ+nGz56cij6KxVttj1efR7z1hdhT8k5AMCAngoseWAAEnq23d6RCCFQerYB+0+rUXiqDvtPqXGgQoN6bZNRu5t63IBHVdH44+AIiy77vpzBILA05whWbD8BABg3UIl//elmzmm0Eww8JnL2wNNsEHjvx5N4JecodM0GBPl4YMkDA5DaP8zk99I2NeOt7SewYtv19W2zO0pqLuCht3NRfV6LmyMU+PCJ2+DnQN+grOV03UX886uD+Lqo5SKiwTd4YM7IeIwfHOGUl0fQNxvw1w9/wXeHzsDbwxUfPnHbNeeCGAwCn+aXY/HmQ9A0NsFFBjx+RyxmjujtMAFaCIFTtRdReFqN/afUKDxdh8JTamgam65q6+nugv5KBcIVnvj+cDUaLo0Y3iB3w/jBPTFRFWM0GmZujfpmzP7sV3y1vxIAMOOemzDzDzdxZNuOMPCYyJkDT/m5Bsz67FfsKW75VviHvqF4cfwABN/QvWsiHK8+j7nrC5FXUgsAuDlCgcV/dI5vm+Z0uu4iHsrOxem6i+gT6ouPnxyKAB/nGgY3tx+P/Yb5Xx6Q5q0MjvJH1v0JTvV/q9kg8PQn+7Dx1wrI3Vyw+i9DMKxXcKdfX32+ES98dQgbf60AAPT098I/xyUgJb6HpbrcJa0rA/efUqPwlFoawWnr8ggebi7oG+6Hm3sqMCBCgZsjFLgx5AbpmjaaRj3W7z2FD3JLcbLmgvS6228MwqOqGNwT38Os1785d0GHJz/IR35pLdxcZHhx/M34U2KE2d6fzIOBx0TOGHiEEPjv3lNYuPEg6rVN8PFwxT/u64eHkiLN9u3EYBD45NK3zfONTXB1keGJO2Lx9B96w8uDw73V5xvx8Nu7UFxzAbHBPvjkqaHo4Wu/k//sia7JgFU7i/H61mNo0DVDJmtZnj07tY/Dz5sQQmDu+kJ8nFcONxcZ3nk0scuX2992pBrPbyiSrmx7783h+Md9/Wz2/6xa03jZyE3Ln60LHi7n7ipDnzBfDOjpj5sjFBjQU4Heob6dmsBvMAjsPFGDD3JLsfXQGWlVkVLhiUeGRiN9SCSCuvmFrrjmAv6yeg9KzjbA19MNb/9PIobd2PlAStbDwGMiZws8Z+u1mLehEN8eOAMASIoOwLKHBlpsNUG1phELvzqITZeGfSMDvfDPcQOQ3DvEIp/nCGov6JD+zi4cOXMePf298NkUFZSc62SySvVFLN58WBrJCPB2xzMj4/FwkmNeBVwIgayvDmL1zhK4yIDlfx6MMTd37/otDbomvLrlKFb+VAyDaLmi7tzRfS3+b9TUbMDhqvPS1XZ/Ka1FxWXXd2nl6iLDTT1uaAk2Ef64uacCfcLMU+LkVG0D1u4uw8d7yqRRIw9XF9x7czgeHRaDgZH+Jr9nXsk5PPlBPmob9Ojp74U1fxmCm0J58VV7xcBjImcKPFsPncGcdYWoqdfC3VWGmSN646k7e1llfs3WQ2fw98+LpF964wYq8fy9/bp9+uxaLuqaoWnUo4ev3C7OrWsa9Xjk3d0oPK1GD185PpuiQnSQj6275dB+PlGD+V8cwLHqegDALREKLLw/oUsHNFt6JecIln9/HACw9MFbzHqKpOi0GnPXF6LwtBqA+a+UrmnUo6CsDnsvBZx95XXSnJpWMlnLxOKEnopLp6b80S/cz+Ijvo36Zny1vxIf5JZg/ym19PgtEQpMVMXg3pvDOxWwvvy1ArM//RW6ZgNuiVDg3UlJHJW1cww8JnKGwHNB24R/bjokLR3vHXoDXn14YLsrPizZj1dyjmLNzy3fNv293fHc6L74U2JEl8KIrsmAM5pGVNRdRKW6ERXqi6isa7lfoW5EpfqiVMdlSEwA/jluAPqE2e7b2OVLjAN9PPDJk0P57dBM9M0GvP9zCV777hjqtU2QyVpWGj4zMh6BDjAvKnvHCbz49WEAQNb9/fGoKsbsn9HUbMD7uaV4JecIGnTNcHeV4a933Yipd/UyaURFCIHycxext6yl2vXe0t8rXl/O19MNg6MCkBgdgKToANwSab2SAu3ZV16HD34uwVf7K6XyLAHe7nh4SBT+Z2gUIgKuHukWQmDF9hN4+dsjAIDUfqH4v/RBPDXvABh4TOTogWdv6TlkfvqrdDn6J+6Ixey0Pja9xsuv5XV4dn0hDlVqAACquCAsfmAAYoN/H+kwGAR+q9f+HmbqLqKiriXEVKgbUVl3Eb/Va6/6JdsRVxcZHr8jFn+75yar/+K9fImxr6cbPpo81Kkm2tqL6vONeHHzYawvOA0AUHi5Y3ZaH0y4NcpuVwr+O7cEf//iAABgzsh4/PWuXhb9vNN1F/GPz4ukOk9xl66UPrSdK6Xrmgw4UKHG3tKWcJNfWovfzl899yYq0BtJ0QFIjGkJOb17+NrtqcWz9Vp8nFeOtbtKpVFnFxlwT99QPKqKxh03BkMmk0HfbMDzG4rwSX45gJZVb/NG97Xb/0tkjIHHRI4aeHRNBry+9RhWbD8Og2iZtLf0oVtMWu1hSfpmA1b9VIxXvzuKRn1LGYW7+/TA2QtaVNQ14oymEU1tXcf8Ch5uLghXeCJc4QmlvxeUCi+E+3v+/qe/F+obm5C18SC+OdCypDlc4Yn59/VDWn/rXIpf32zAlH/vxdbD1fD2cMW/H78NidGOc7l5R5RXcg5//7xIuq5Uf6Ufsu5PsLt/9//uPYXZn/0KAJieciNmp/WxyucKIfB1UcuV0lvDy0NJEZg3ui8AGIWbtgpCurvK0F+pQFJ0AJJiAjA4OsAhT+80NRuw9XA1Psgtwc7jZ6XH40J88D+3RWPbkWr8eKwGLjJgwVjLjLyR5TDwmMgRA8+xM+cx89N9KDrdMoLywKCemD+2v11eqK3sbAOe+7wQPx6rueo5FxkQ6tcSWloDTUu48UJP/5ZAE+Tj0enQsu1wNf7xZRHKz7WsWknpE4KFYxMsevn3ZoPAjI8LsGl/JeRuLljzl1uh6sWaY9bQ1GzA2t1lWJpzBOcvXcdl/OAIPDsqHiG+lp071hmbCysx/T+/wCCAjGExmH9fP6vPM1Nf1OOlbw5j7e6W092e7i5o1Buuahfg7Y7E6JZgkxQdiJsjFA51JfDOOF59Hv/OLcW6X04bXdTQy90Vb0wYhHv6dm21HNkOA4+JHC3wXNQ1Y/hL21BTr4W/tzsW/3GAXVfqBVq+bW49VI2TNfUIV3hB6d8Sanr4ys167Qyg5dTSim3Hkb3jJHTNBsjdXDAt5UY8lRxn9qujGgwCz6zbj//uPQV3VxnemZhkd9dCuR7U1Gvx0jeH8Wl+S7FNX7kbZo7ojUdV0Wb//9VZ2w5X48l/50PfLPBQUgRefOBmm57+yS85h7nrC6WJ33EhPi2np6IDkBgdiF4h9lMQ0tLqtU3Y8MspfLirDBf1zVjxyGCefnZQDDwmcrTAc7y6Hn9YtgPeHq7YPvsuuy7sZksnfqvHP74okoaxY4N98ML9CbjjJvOc8hNCYP6XB/BBbilcZMCbEwZjlJ0HT2f3S1kt/vFFkTTyGR/mi4Vj++O2duauWEruibPIWL0H2iYD7r05HP+XPsgu5oTomgw4XKVBRIC3Q0z0JroWBh4TOVrg2Vt6DuPfykVUoDd+eCbF1t2xa0IIbNxfiRe+OijNY7j35nD8/d5+CO1GUBRC4MVvDuPtHSchkwHLHroFfxzEq7Dag2aDwMd5ZXj52yPSCr7hNwXjnvgeuDs+1OLVrQvKavE/7+3GBV0z/tC3B976n0S422iUicjZMfCYyNECz3cHz+CJD/JxS4QCX0y/w9bdcQiaRj2W5RzFB7klMIiWWjyZ3TjlsXzrMbyy5SgAYNEfE/DIbdHm7jJ1U+0FHV7OOYKP9pQZrfSLC/HB3X16ICW+B4bEBHbq6r6ddbBCg/R3cqFpbMLtNwZh5aQhTjcPhsieMPCYyNECT+uqjzt7h+CDx261dXccStFpNZ7/vAj7yusAAP3C/fDPPyZcs2jj5d778ST+uekQAOD5MX3xxPA4S3SVzOTkb/XYcvAMvj9cjfzSWjRftjLQx8MVd9wUjLvje+CuPj26Nep3vLoeD7+di7MXdEiMDsAHj91q82vSEDk7Bh4TOVrgaT3gjr1Fidf/PMjW3XE4BoPAx3nl+Nc3h6G+qIdMBqQPicKckdeu0/Sf3WWYt6EQADDzD73xtz/cZI0uk5moL+rx07EabDtSje1HqlFTrzN6vr/SDymXRn8GRvp3et5N+bkGPJidiypNI/or/fCfyUPtcsUkkbNh4DGRowWe1svTP6qKRtb9CbbujsM6W6/Fkq8P4797W1b2BPp44NlR8fjT4Ig2V9NsKDiFzE9/hRDAU8lxeHZk/HWzqsUZGQwCRRVqfH+4GtuO/Ib9p+qMTn0FeLsjuXcIUuJ7ILl3SLth+IymEQ9m56LsXANu7HEDPnlyaLeLVxJR5zDwmMjRAs/znxfiw11lmHHPTcgc0dvW3XF4e4pbLmB35EzLBeyGxATghXEJiA/7/f/CN0WVmPafAjQbBCYOjUbW/f0ZdpxMTb0WO478hm1HqvHD0d+gafz9Oi0uMmBwVABS4nsgpU8P9A33hUwmw9l6LR5+ZxeOV9cjKtAbn01Rdeu0GBGZhoHHRI4WeKb/5xd8tb8S/7i3Hx67I9bW3XEK+mYDVu8sxmvfHUODrtmoREVeyTlM/qDleirjB0fg5T/Z9noqZHlNzQb8UlaH7w+3nPpqvZpzqzA/T6TEh+DXcjUOVmoQrvDEp0+pEBlo2RVgRGSMgcdEjhZ4Jq7cjR+P1WDZQ7fggcFcCm1OFXUXjUpUhPl5orZBB22TAWMGhOP/0gfa7EJ2ZDun6y5i+5FqbDtcjZ3Hz+Ki/vcq4cE3eOCTp1ToFXKDDXtIdH3q7PGbywccVG1Dy0TLgGtMsCXTKf29kD0xEdsOV2P+lwdQdq6lIOs98T3w6sMMO9ernv5eeOS2aDxyWzQa9c3YXXwO2w5Xo7jmAuaOjmfYIbJzDDwOqvViagpvrgKxlJT4HlD1CsLKn4pRe0GH2Wl9zHq9FnJcnu6uSO4dguTeIbbuChF1EgOPg1JfCjz+XPZqUZ7urpiWcqOtu0FERN3Er6sOSN9swPlLVX6vdc0YIiIiYuBxSOqLeunvvLAZERHRtTHwOKDW+Tt+nm52UX2ZiIjI3jHwOCD1xZYVWjydRURE1DkMPA6o9kLLCE8AV2gRERF1CgOPA6q72LoknSM8REREncHA44DqLl10kEvSiYiIOoeBxwG1rtLy5yktIiKiTmHgcUCtZSU4aZmIiKhzGHgcUB2vskxERGQSBh4HxFNaREREpmHgcUCslE5ERGQamwaemJgYyGQyo9uzzz5r1KasrAz33XcffHx8EBwcjBkzZkCn0xm1KSwsRHJyMry8vNCzZ09kZWVBCGHNTbEqVkonIiIyjc2rpWdlZWHy5MnS/RtuuEH6e3NzM8aMGYOQkBD89NNPOHv2LCZNmgQhBJYvXw4A0Gg0GDFiBFJSUpCXl4ejR48iIyMDPj4+mDVrltW3xxpYKZ2IiMg0Ng88vr6+CAsLa/O5nJwcHDx4EOXl5VAqlQCAV155BRkZGVi0aBH8/Pywdu1aNDY2Ys2aNZDL5UhISMDRo0exbNkyZGZmQiZzrlpTrJRORERkOpvP4fnXv/6FoKAgDBw4EIsWLTI6XZWbm4uEhAQp7ABAWloatFot9u7dK7VJTk6GXC43alNRUYGSkpJ2P1er1UKj0RjdHAErpRMREZnOpiM8f/vb3zB48GAEBARgz549mDt3LoqLi/Hee+8BAKqqqhAaGmr0moCAAHh4eKCqqkpqExMTY9Sm9TVVVVWIjY1t87OXLFmChQsXmnmLLI+V0omIiExn9hGeBQsWXDUR+cpbfn4+AGDmzJlITk7GzTffjCeeeALZ2dlYuXIlzp49K71fW6ekhBBGj1/ZpnXCckens+bOnQu1Wi3dysvLu7Xd1sJK6URERKYz+wjP9OnTkZ6e3mGbK0dkWg0dOhQAcPz4cQQFBSEsLAy7d+82alNbWwu9Xi+N4oSFhUmjPa2qq6sB4KrRocvJ5XKj02COgpXSiYiITGf2wBMcHIzg4OAuvbagoAAAEB4eDgBQqVRYtGgRKisrpcdycnIgl8uRmJgotZk3bx50Oh08PDykNkqlst1g5chYKZ2IiMh0Npu0nJubi1dffRX79u1DcXExPv30Uzz11FMYO3YsoqKiAACpqano168fJk6ciIKCAmzduhWzZ8/G5MmT4efnBwCYMGEC5HI5MjIyUFRUhA0bNmDx4sVOuUILYKV0IiKirrDZpGW5XI5PPvkECxcuhFarRXR0NCZPnoxnnnlGauPq6opNmzZh6tSpuP322+Hl5YUJEyZg6dKlUhuFQoEtW7Zg2rRpSEpKQkBAADIzM5GZmWmLzbI4lpUgIiIync0Cz+DBg7Fr165rtouKisJXX33VYZsBAwbghx9+MFfX7BorpRMREZnO5tfhIdOwUjoREZHpGHgcDE9pERERmY6Bx8GwUjoREZHpGHgcDCulExERmY6Bx8GwUjoREZHpGHgcCCulExERdQ0DjwNhpXQiIqKuYeBxIKyUTkRE1DUMPA6EldKJiIi6hoHHgbRWSuc1eIiIiEzDwONA6qSLDnKEh4iIyBQMPA6EldKJiIi6hoHHgbCsBBERUdcw8DgQVkonIiLqGgYeB8JK6URERF3DwONAeEqLiIioaxh4HMjvp7QYeIiIiEzBwONApFNanMNDRERkEgYeB8JK6URERF3DwOMgWCmdiIio6xh4HAQrpRMREXUdA4+DYKV0IiKirmPgcRCslE5ERNR1DDwOgpXSiYiIuo6Bx0GwUjoREVHXMfA4CFZKJyIi6joGHgfBshJERERdx8DjIFgpnYiIqOsYeBwEK6UTERF1HQOPg+ApLSIioq5j4HEQrJRORETUdQw8DoKV0omIiLqOgcdBsFI6ERFR1zHwOABWSiciIuoeBh4HwErpRERE3cPA4wBYKZ2IiKh7GHgcACulExERdQ8DjwP4fYUWT2cRERF1hUUDz6JFizBs2DB4e3vD39+/zTZlZWW477774OPjg+DgYMyYMQM6nc6oTWFhIZKTk+Hl5YWePXsiKysLQgijNjt27EBiYiI8PT0RFxeH7OxsS22W1dVySToREVG3uFnyzXU6HR588EGoVCqsXLnyquebm5sxZswYhISE4KeffsLZs2cxadIkCCGwfPlyAIBGo8GIESOQkpKCvLw8HD16FBkZGfDx8cGsWbMAAMXFxRg9ejQmT56MDz/8EDt37sTUqVMREhKC8ePHW3ITrYKV0omIiLrHooFn4cKFAIA1a9a0+XxOTg4OHjyI8vJyKJVKAMArr7yCjIwMLFq0CH5+fli7di0aGxuxZs0ayOVyJCQk4OjRo1i2bBkyMzMhk8mQnZ2NqKgovPbaawCAvn37Ij8/H0uXLnWKwMOyEkRERN1j0zk8ubm5SEhIkMIOAKSlpUGr1WLv3r1Sm+TkZMjlcqM2FRUVKCkpkdqkpqYavXdaWhry8/Oh1+vRFq1WC41GY3SzV6yUTkRE1D02DTxVVVUIDQ01eiwgIAAeHh6oqqpqt03r/Wu1aWpqQk1NTZufvWTJEigUCukWGRlplm2yBFZKJyIi6h6TA8+CBQsgk8k6vOXn53f6/WSyq68rI4QwevzKNq0Tlk1tc7m5c+dCrVZLt/Ly8k732dp4SouIiKh7TJ7DM336dKSnp3fYJiYmplPvFRYWht27dxs9VltbC71eL43YhIWFSSM5raqrqwHgmm3c3NwQFBTU5mfL5XKj02T2jMvSiYiIusfkwBMcHIzg4GCzfLhKpcKiRYtQWVmJ8PBwAC0TmeVyORITE6U28+bNg06ng4eHh9RGqVRKwUqlUmHjxo1G752Tk4OkpCS4uzt+SOAcHiIiou6x6ByesrIy7Nu3D2VlZWhubsa+ffuwb98+1NfXAwBSU1PRr18/TJw4EQUFBdi6dStmz56NyZMnw8/PDwAwYcIEyOVyZGRkoKioCBs2bMDixYulFVoAMGXKFJSWliIzMxOHDh3CqlWrsHLlSsyePduSm2c1rJRORETUTcKCJk2aJABcddu2bZvUprS0VIwZM0Z4eXmJwMBAMX36dNHY2Gj0Pvv37xfDhw8XcrlchIWFiQULFgiDwWDUZvv27WLQoEHCw8NDxMTEiLfeesukvqrVagFAqNXqLm+vJeiamkX0nK9E9JyvxNl6ra27Q0REZFc6e/yWCXHFJYuvUxqNBgqFAmq1Whpdsgc19Vok/fM7AMCJxaNZPJSIiOgynT1+s5aWnWOldCIiou5j4LFzrJRORETUfQw8do5L0omIiLqPgcfOsVI6ERFR9zHw2DlWSiciIuo+Bh47x7ISRERE3cfAY+d4lWUiIqLuY+Cxc6yUTkRE1H0MPHaOp7SIiIi6j4HHznFZOhERUfcx8Ng5zuEhIiLqPgYeO8dK6URERN3HwGPH9M0GnNc2AeAIDxERUXcw8Nix1gnLAKDgCA8REVGXMfDYMVZKJyIiMg8GHjvGSulERETmwcBjx7gknYiIyDwYeOwYK6UTERGZBwOPHWOldCIiIvNg4LFjLCtBRERkHgw8doxXWSYiIjIPBh47xkrpRERE5sHAY8d4SouIiMg8GHjsGJelExERmQcDjx3jHB4iIiLzYOCxY6yUTkREZB4MPHaKldKJiIjMh4HHTrFSOhERkfkw8NgpVkonIiIyHwYeO8VK6URERObDwGOnuCSdiIjIfBh47BQrpRMREZkPA4+dYqV0IiIi82HgsVMsK0FERGQ+DDx2ildZJiIiMh8GHjvFSulERETmw8Bjp3hKi4iIyHwYeOwUl6UTERGZj0UDz6JFizBs2DB4e3vD39+/zTYymeyqW3Z2tlGbwsJCJCcnw8vLCz179kRWVhaEEEZtduzYgcTERHh6eiIuLu6q93A0nMNDRERkPm6WfHOdTocHH3wQKpUKK1eubLfd6tWrMXLkSOm+QqGQ/q7RaDBixAikpKQgLy8PR48eRUZGBnx8fDBr1iwAQHFxMUaPHo3Jkyfjww8/xM6dOzF16lSEhIRg/PjxlttAC2KldCIiIvOxaOBZuHAhAGDNmjUdtvP390dYWFibz61duxaNjY1Ys2YN5HI5EhIScPToUSxbtgyZmZnSiFBUVBRee+01AEDfvn2Rn5+PpUuXOmTgYaV0IiIi87KLOTzTp09HcHAwhgwZguzsbBgMBum53NxcJCcnQy6XS4+lpaWhoqICJSUlUpvU1FSj90xLS0N+fj70ej3aotVqodFojG724vJK6X6eFs2kRERE1wWbB54XXngBn332Gb777jukp6dj1qxZWLx4sfR8VVUVQkNDjV7Ter+qqqrDNk1NTaipqWnzc5csWQKFQiHdIiMjzblZ3XJ5pXQ3V5vvIiIiIodn8tF0wYIFbU40vvyWn5/f6fd7/vnnoVKpMHDgQMyaNQtZWVl4+eWXjdrIZDKj+60Tli9/vDNtLjd37lyo1WrpVl5e3uk+WxorpRMREZmXyedLpk+fjvT09A7bxMTEdLU/GDp0KDQaDc6cOYPQ0FCEhYVJIzmtqqurAfw+0tNeGzc3NwQFBbX5OXK53Og0mT3hknQiIiLzMjnwBAcHIzg42BJ9AQAUFBTA09NTWsauUqkwb9486HQ6eHi0jHjk5ORAqVRKwUqlUmHjxo1G75OTk4OkpCS4uzteaGCldCIiIvOy6ASRsrIy7Nu3D2VlZWhubsa+ffuwb98+1NfXAwA2btyId999F0VFRThx4gTee+89PPfcc3jyySel0ZcJEyZALpcjIyMDRUVF2LBhAxYvXiyt0AKAKVOmoLS0FJmZmTh06BBWrVqFlStXYvbs2ZbcPIthpXQiIiLzsugSoH/84x94//33pfuDBg0CAGzbtg133XUX3N3dsWLFCmRmZsJgMCAuLg5ZWVmYNm2a9BqFQoEtW7Zg2rRpSEpKQkBAADIzM5GZmSm1iY2NxebNmzFz5ky8+eabUCqVeP311x1ySTrAshJERETmJhNXXrL4OqXRaKBQKKBWq+Hn52fTvjz/eSE+3FWGGXffiMzUPjbtCxERkT3r7PGba57tUB3n8BAREZkVA48d4iktIiIi82LgsUNclk5ERGReDDx2iJXSiYiIzIuBxw6xUjoREZF5MfDYGVZKJyIiMj8GHjvDSulERETmx8BjZ1gpnYiIyPx4RLUzrJRORERkfgw8doZL0omIiMyPgcfOsFI6ERGR+THw2BlWSiciIjI/Bh47w7ISRERE5sfAY2fqeNFBIiIis2PgsTMsK0FERGR+DDx2hqe0iIiIzI+Bx85wWToREZH5MfDYGZ7SIiIiMj8GHjvDSulERETmx8BjR1gpnYiIyDIYeOyIhpXSiYiILIKBx47UslI6ERGRRfCoakdYKZ2IiMgyGHjsCJekExERWQYDjx1hpXQiIiLLYOCxI6yUTkREZBkMPHaEZSWIiIgsg4HHjrBSOhERkWUw8NgRlpUgIiKyDAYeO8JTWkRERJbBwGNHuCydiIjIMhh47AhPaREREVkGA48dYaV0IiIiy2DgsROslE5ERGQ5DDx2gpXSiYiILIeBx06wUjoREZHl8MhqJ1gpnYiIyHIYeOwEl6QTERFZjsUCT0lJCR5//HHExsbCy8sLvXr1wvz586HT6YzalZWV4b777oOPjw+Cg4MxY8aMq9oUFhYiOTkZXl5e6NmzJ7KysiCEMGqzY8cOJCYmwtPTE3FxccjOzrbUplkEK6UTERFZjsVmxx4+fBgGgwFvv/02brzxRhQVFWHy5Mm4cOECli5dCgBobm7GmDFjEBISgp9++glnz57FpEmTIITA8uXLAQAajQYjRoxASkoK8vLycPToUWRkZMDHxwezZs0CABQXF2P06NGYPHkyPvzwQ+zcuRNTp05FSEgIxo8fb6lNNCtWSiciIrIgYUUvvfSSiI2Nle5v3rxZuLi4iNOnT0uPffTRR0Iulwu1Wi2EEGLFihVCoVCIxsZGqc2SJUuEUqkUBoNBCCHEM888I+Lj440+66mnnhJDhw7tdN/UarUAIH2utS399rCInvOV+PvnhTb5fCIiIkfU2eO3VefwqNVqBAYGSvdzc3ORkJAApVIpPZaWlgatVou9e/dKbZKTkyGXy43aVFRUoKSkRGqTmppq9FlpaWnIz8+HXq+HI2CldCIiIsuxWuA5ceIEli9fjilTpkiPVVVVITQ01KhdQEAAPDw8UFVV1W6b1vvXatPU1ISampo2+6PVaqHRaIxutsSyEkRERJZjcuBZsGABZDJZh7f8/Hyj11RUVGDkyJF48MEH8cQTTxg9J5PJrvoMIYTR41e2EZcmLJva5nJLliyBQqGQbpGRkdfadItipXQiIiLLMXnS8vTp05Gent5hm5iYGOnvFRUVSElJgUqlwjvvvGPULiwsDLt37zZ6rLa2Fnq9XhqxCQsLk0ZyWlVXVwPANdu4ubkhKCiozT7OnTsXmZmZ0n2NRmPT0MNl6URERJZjcuAJDg5GcHBwp9qePn0aKSkpSExMxOrVq+HiYjygpFKpsGjRIlRWViI8PBwAkJOTA7lcjsTERKnNvHnzoNPp4OHhIbVRKpVSsFKpVNi4caPRe+fk5CApKQnu7m0HCLlcbjQvyNZ4SouIiMhyLDaHp6KiAnfddRciIyOxdOlS/Pbbb6iqqjIaiUlNTUW/fv0wceJEFBQUYOvWrZg9ezYmT54MPz8/AMCECRMgl8uRkZGBoqIibNiwAYsXL0ZmZqZ0umrKlCkoLS1FZmYmDh06hFWrVmHlypWYPXu2pTbP7FgpnYiIyHIsdh2enJwcHD9+HMePH0dERITRc63za1xdXbFp0yZMnToVt99+O7y8vDBhwgTpOj0AoFAosGXLFkybNg1JSUkICAhAZmam0emo2NhYbN68GTNnzsSbb74JpVKJ119/3WGuwcNK6URERJYlE+KKSxZfpzQaDRQKBdRqtTS6ZC1n67VI/Od3AIDji0axeCgREVEndfb4zSOrHWCldCIiIsvi0dUOsFI6ERGRZTHw2AEuSSciIrIsBh47wErpRERElsXAYwdYKZ2IiMiyGHjsAMtKEBERWRYDjx1gpXQiIiLLYuCxAywrQUREZFkMPHaAp7SIiIgsi4HHDnBZOhERkWUx8NgBntIiIiKyLAYeO8BK6URERJbFwGNjrJRORERkeQw8Nqa5NGEZaCkeSkRERObHwGNjrJRORERkeTzC2hgrpRMREVkeA4+NcUk6ERGR5THw2FjrKS0FV2gRERFZDAOPjbVWSg/gKS0iIiKLYeCxMZaVICIisjwGHhtjpXQiIiLLY+CxMZaVICIisjwGHhvjKS0iIiLLY+CxMS5LJyIisjwGHhtrPaWl8OIpLSIiIkth4LGx1krpARzhISIishgGHhtipXQiIiLrYOCxIVZKJyIisg4GHhtipXQiIiLr4FHWhlgpnYiIyDoYeGyIS9KJiIisg4HHhupYKZ2IiMgqGHhsqJaV0omIiKyCgceGWFaCiIjIOhh4bIiV0omIiKyDgceGWCmdiIjIOhh4bIintIiIiKyDgceGuCydiIjIOhh4bKjuIiulExERWYPFAk9JSQkef/xxxMbGwsvLC7169cL8+fOh0+mM2slksqtu2dnZRm0KCwuRnJwMLy8v9OzZE1lZWRBCGLXZsWMHEhMT4enpibi4uKvewx7VXWCldCIiImuwWMXKw4cPw2Aw4O2338aNN96IoqIiTJ48GRcuXMDSpUuN2q5evRojR46U7isUCunvGo0GI0aMQEpKCvLy8nD06FFkZGTAx8cHs2bNAgAUFxdj9OjRmDx5Mj788EPs3LkTU6dORUhICMaPH2+pTewWVkonIiKyHosFnpEjRxqFmLi4OBw5cgRvvfXWVYHH398fYWFhbb7P2rVr0djYiDVr1kAulyMhIQFHjx7FsmXLkJmZKY0IRUVF4bXXXgMA9O3bF/n5+Vi6dKndBh5WSiciIrIeq87hUavVCAwMvOrx6dOnIzg4GEOGDEF2djYMBoP0XG5uLpKTkyGXy6XH0tLSUFFRgZKSEqlNamqq0XumpaUhPz8fer0ebdFqtdBoNEY3a2KldCIiIuux2pH2xIkTWL58OaZMmWL0+AsvvIDPPvsM3333HdLT0zFr1iwsXrxYer6qqgqhoaFGr2m9X1VV1WGbpqYm1NTUtNmfJUuWQKFQSLfIyMhub6MpWCmdiIjIekwOPAsWLGhzovHlt/z8fKPXVFRUYOTIkXjwwQfxxBNPGD33/PPPQ6VSYeDAgZg1axaysrLw8ssvG7WRyWRG91snLF/+eGfaXG7u3LlQq9XSrby83IR/he7jknQiIiLrMXnyyPTp05Gent5hm5iYGOnvFRUVSElJgUqlwjvvvHPN9x86dCg0Gg3OnDmD0NBQhIWFSSM5raqrqwH8PtLTXhs3NzcEBQW1+TlyudzoNJm1sVI6ERGR9ZgceIKDgxEcHNyptqdPn0ZKSgoSExOxevVquLhce0CpoKAAnp6e8Pf3BwCoVCrMmzcPOp0OHh4tp39ycnKgVCqlYKVSqbBx40aj98nJyUFSUhLc3e0zULBSOhERkfVYbA5PRUUF7rrrLkRGRmLp0qX47bffUFVVZTQSs3HjRrz77rsoKirCiRMn8N577+G5557Dk08+KY2+TJgwAXK5HBkZGSgqKsKGDRuwePFiaYUWAEyZMgWlpaXIzMzEoUOHsGrVKqxcuRKzZ8+21OZ1G8tKEBERWY/F1kPn5OTg+PHjOH78OCIiIoyea51f4+7ujhUrViAzMxMGgwFxcXHIysrCtGnTpLYKhQJbtmzBtGnTkJSUhICAAGRmZiIzM1NqExsbi82bN2PmzJl48803oVQq8frrr9vtknSAldKJiIisSSauvGTxdUqj0UChUECtVsPPz8/inzf9P7/gq/2V+Me9/fDYHbEW/zwiIiJn1NnjNy8AYyM8pUVERGQ9DDw2wmXpRERE1sPAYyOslE5ERGQ9DDw2wkrpRERE1sPAYwOslE5ERGRdDDw2wErpRERE1sXAYwOslE5ERGRdPNraACulExERWRcDjw1wSToREZF1MfDYACulExERWRcDjw2wUjoREZF1MfDYAMtKEBERWRcDjw2wUjoREZF1MfDYQOspLa7SIiIisg4GHhvgKS0iIiLrYuCxAS5LJyIisi4GHhtgpXQiIiLrYuCxAVZKJyIisi4GHitjpXQiIiLrY+CxMlZKJyIisj4GHitjpXQiIiLr4xHXylgpnYiIyPoYeKyMS9KJiIisj4HHylgpnYiIyPoYeKyMldKJiIisj4HHylhWgoiIyPoYeKyMldKJiIisj4HHylgpnYiIyPoYeKyMp7SIiIisj4HHyrgsnYiIyPoYeKyMldKJiIisj4HHylgpnYiIyPoYeKyIldKJiIhsg4HHilgpnYiIyDYYeKyIldKJiIhsg0ddK2KldCIiIttg4LEiLkknIiKyDQYeK2KldCIiItuwaOAZO3YsoqKi4OnpifDwcEycOBEVFRVGbcrKynDffffBx8cHwcHBmDFjBnQ6nVGbwsJCJCcnw8vLCz179kRWVhaEEEZtduzYgcTERHh6eiIuLg7Z2dmW3LQuYaV0IiIi27Bo4ElJScGnn36KI0eOYN26dThx4gT+9Kc/Sc83NzdjzJgxuHDhAn766Sd8/PHHWLduHWbNmiW10Wg0GDFiBJRKJfLy8rB8+XIsXboUy5Ytk9oUFxdj9OjRGD58OAoKCjBv3jzMmDED69ats+TmmYxlJYiIiGxEWNEXX3whZDKZ0Ol0QgghNm/eLFxcXMTp06elNh999JGQy+VCrVYLIYRYsWKFUCgUorGxUWqzZMkSoVQqhcFgEEII8cwzz4j4+Hijz3rqqafE0KFDO903tVotAEifawnPbygU0XO+Eq98e9hin0FERHQ96ezx22pzeM6dO4e1a9di2LBhcHdvGeHIzc1FQkIClEql1C4tLQ1arRZ79+6V2iQnJ0Mulxu1qaioQElJidQmNTXV6PPS0tKQn58PvV6Ptmi1Wmg0GqObpbFSOhERkW1YPPDMmTMHPj4+CAoKQllZGb744gvpuaqqKoSGhhq1DwgIgIeHB6qqqtpt03r/Wm2amppQU1PTZr+WLFkChUIh3SIjI7u3oZ3AU1pERES2YXLgWbBgAWQyWYe3/Px8qf3//u//oqCgADk5OXB1dcWjjz5qNOFYJpNd9RlCCKPHr2zT+npT21xu7ty5UKvV0q28vLyz/wRdxmXpREREtmFyfYPp06cjPT29wzYxMTHS34ODgxEcHIzevXujb9++iIyMxK5du6BSqRAWFobdu3cbvba2thZ6vV4asQkLC5NGclpVV1cDwDXbuLm5ISgoqM0+yuVyo9Nk1sBK6URERLZhcuBpDTBd0TrqotVqAQAqlQqLFi1CZWUlwsPDAQA5OTmQy+VITEyU2sybNw86nQ4eHh5SG6VSKQUrlUqFjRs3Gn1WTk4OkpKSpPlC9oCV0omIiGzDYnN49uzZgzfeeAP79u1DaWkptm3bhgkTJqBXr15QqVQAgNTUVPTr1w8TJ05EQUEBtm7ditmzZ2Py5Mnw8/MDAEyYMAFyuRwZGRkoKirChg0bsHjxYmRmZkqnq6ZMmYLS0lJkZmbi0KFDWLVqFVauXInZs2dbavNMxkrpREREtmOxwOPl5YX169fjnnvuQZ8+ffDYY48hISEBO3bskE4lubq6YtOmTfD09MTtt9+Ohx56COPGjcPSpUul91EoFNiyZQtOnTqFpKQkTJ06FZmZmcjMzJTaxMbGYvPmzdi+fTsGDhyIF154Aa+//jrGjx9vqc0zGSulExER2Y5MiCsuWXyd0mg0UCgUUKvV0uiSOR2vrscflu2Ar6cbChekmf39iYiIrkedPX6zlpaVtFZKZ1kJIiIi62PgsRIuSSciIrIdBh4rYaV0IiIi22HgsRJWSiciIrIdBh4rYVkJIiIi22HgsRJpDg9PaREREVkdA4+V1F0a4VHwlBYREZHVMfBYSZ00h4cjPERERNbGwGMlXJZORERkOww8VsJK6URERLbDwGMlrJRORERkOww8VsBK6URERLbFwGMFrJRORERkWww8VtC6JN3X0w1urvwnJyIisjYefa2gjmUliIiIbIqBxwq4JJ2IiMi2GHisgJXSiYiIbIuBxwpYKZ2IiMi2GHisgJXSiYiIbIuBxwpYKZ2IiMi2GHisgJXSiYiIbIuBxwpYKZ2IiMi2GHisgMvSiYiIbIuBxwpYKZ2IiMi2GHisgJXSiYiIbIuBx8JYKZ2IiMj2GHgsjJXSiYiIbI+Bx8JYKZ2IiMj2eAS2MFZKJyIisj0GHgvjknQiIiLbY+CxMFZKJyIisj0GHgtjpXQiIiLbY+CxMFZKJyIisj0GHgtjpXQiIiLbY+CxMFZKJyIisj1eCc/CRvYPQ0SAFwZF+du6K0RERNctBh4LG3NzOMbcHG7rbhAREV3XeEqLiIiInJ5FA8/YsWMRFRUFT09PhIeHY+LEiaioqDBqI5PJrrplZ2cbtSksLERycjK8vLzQs2dPZGVlQQhh1GbHjh1ITEyEp6cn4uLirnoPIiIiun5ZNPCkpKTg008/xZEjR7Bu3TqcOHECf/rTn65qt3r1alRWVkq3SZMmSc9pNBqMGDECSqUSeXl5WL58OZYuXYply5ZJbYqLizF69GgMHz4cBQUFmDdvHmbMmIF169ZZcvOIiIjIQcjElUMlFvTll19i3Lhx0Gq1cHdvWaYtk8mwYcMGjBs3rs3XvPXWW5g7dy7OnDkDuVwOAHjxxRexfPlynDp1CjKZDHPmzMGXX36JQ4cOSa+bMmUKfv31V+Tm5naqbxqNBgqFAmq1Gn5+ft3bUCIiIrKKzh6/rTaH59y5c1i7di2GDRsmhZ1W06dPR3BwMIYMGYLs7GwYDAbpudzcXCQnJ0thBwDS0tJQUVGBkpISqU1qaqrRe6alpSE/Px96vb7N/mi1Wmg0GqMbEREROSeLB545c+bAx8cHQUFBKCsrwxdffGH0/AsvvIDPPvsM3333HdLT0zFr1iwsXrxYer6qqgqhoaFGr2m9X1VV1WGbpqYm1NTUtNmvJUuWQKFQSLfIyMhubysRERHZJ5MDz4IFC9qcaHz5LT8/X2r/v//7vygoKEBOTg5cXV3x6KOPGk04fv7556FSqTBw4EDMmjULWVlZePnll40+UyaTGd1vff3lj3emzeXmzp0LtVot3crLy039pyAiIiIHYfJ1eKZPn4709PQO28TExEh/Dw4ORnBwMHr37o2+ffsiMjISu3btgkqlavO1Q4cOhUajwZkzZxAaGoqwsDBpJKdVdXU1gN9Hetpr4+bmhqCgoDY/Ry6XG50mIyIiIudlcuBpDTBd0TrqotVq221TUFAAT09P+Pv7AwBUKhXmzZsHnU4HD4+W8gw5OTlQKpVSsFKpVNi4caPR++Tk5CApKemq+UJERER0/bHYHJ49e/bgjTfewL59+1BaWopt27ZhwoQJ6NWrlzS6s3HjRrz77rsoKirCiRMn8N577+G5557Dk08+KY2+TJgwAXK5HBkZGSgqKsKGDRuwePFiZGZmSqerpkyZgtLSUmRmZuLQoUNYtWoVVq5cidmzZ1tq84iIiMiRCAvZv3+/SElJEYGBgUIul4uYmBgxZcoUcerUKanN119/LQYOHChuuOEG4e3tLRISEsRrr70m9Hr9Ve81fPhwIZfLRVhYmFiwYIEwGAxGbbZv3y4GDRokPDw8RExMjHjrrbdM6q9arRYAhFqt7vpGExERkVV19vht1evw2DNeh4eIiMjx2N11eIiIiIhshdXSL2kd6OIFCImIiBxH63H7WiesGHguOX/+PADwAoREREQO6Pz581AoFO0+zzk8lxgMBlRUVMDX17fdixV2hUajQWRkJMrLy512bpCzbyO3z/E5+zY6+/YBzr+N3L6uE0Lg/PnzUCqVcHFpf6YOR3gucXFxQUREhMXe38/Pzyn/E1/O2beR2+f4nH0bnX37AOffRm5f13Q0stOKk5aJiIjI6THwEBERkdNj4LEwuVyO+fPnO3XdLmffRm6f43P2bXT27QOcfxu5fZbHSctERETk9DjCQ0RERE6PgYeIiIicHgMPEREROT0GHiIiInJ6DDxmsGLFCsTGxsLT0xOJiYn48ccfO2y/Y8cOJCYmwtPTE3FxccjOzrZST023ZMkSDBkyBL6+vujRowfGjRuHI0eOdPia7du3QyaTXXU7fPiwlXrdeQsWLLiqn2FhYR2+xpH2HwDExMS0uT+mTZvWZnt7338//PAD7rvvPiiVSshkMnz++edGzwshsGDBAiiVSnh5eeGuu+7CgQMHrvm+69atQ79+/SCXy9GvXz9s2LDBQlvQsY62T6/XY86cORgwYAB8fHygVCrx6KOPoqKiosP3XLNmTZv7tLGx0cJb07Zr7cOMjIyr+jp06NBrvq8j7EMAbe4LmUyGl19+ud33tKd92Jnjgj3+HDLwdNMnn3yCp59+Gs899xwKCgowfPhwjBo1CmVlZW22Ly4uxujRozF8+HAUFBRg3rx5mDFjBtatW2flnnfOjh07MG3aNOzatQtbtmxBU1MTUlNTceHChWu+9siRI6isrJRuN910kxV6bLr+/fsb9bOwsLDdto62/wAgLy/PaPu2bNkCAHjwwQc7fJ297r8LFy7glltuwRtvvNHm8y+99BKWLVuGN954A3l5eQgLC8OIESOkenltyc3NxcMPP4yJEyfi119/xcSJE/HQQw9h9+7dltqMdnW0fQ0NDfjll1/w97//Hb/88gvWr1+Po0ePYuzYsdd8Xz8/P6P9WVlZCU9PT0tswjVdax8CwMiRI436unnz5g7f01H2IYCr9sOqVasgk8kwfvz4Dt/XXvZhZ44LdvlzKKhbbr31VjFlyhSjx+Lj48Wzzz7bZvtnnnlGxMfHGz321FNPiaFDh1qsj+ZUXV0tAIgdO3a022bbtm0CgKitrbVex7po/vz54pZbbul0e0fff0II8be//U306tVLGAyGNp93pP0HQGzYsEG6bzAYRFhYmHjxxRelxxobG4VCoRDZ2dntvs9DDz0kRo4cafRYWlqaSE9PN3ufTXHl9rVlz549AoAoLS1tt83q1auFQqEwb+fMpK1tnDRpkrj//vtNeh9H3of333+/uPvuuztsY8/78Mrjgr3+HHKEpxt0Oh327t2L1NRUo8dTU1Px888/t/ma3Nzcq9qnpaUhPz8fer3eYn01F7VaDQAIDAy8ZttBgwYhPDwc99xzD7Zt22bprnXZsWPHoFQqERsbi/T0dJw8ebLdto6+/3Q6HT788EM89thj1yyS6yj773LFxcWoqqoy2kdyuRzJycnt/kwC7e/Xjl5jL9RqNWQyGfz9/TtsV19fj+joaERERODee+9FQUGBdTrYRdu3b0ePHj3Qu3dvTJ48GdXV1R22d9R9eObMGWzatAmPP/74Ndva6z688rhgrz+HDDzdUFNTg+bmZoSGhho9HhoaiqqqqjZfU1VV1Wb7pqYm1NTUWKyv5iCEQGZmJu644w4kJCS02y48PBzvvPMO1q1bh/Xr16NPnz6455578MMPP1ixt51z22234YMPPsC3336Ld999F1VVVRg2bBjOnj3bZntH3n8A8Pnnn6Ourg4ZGRnttnGk/Xel1p87U34mW19n6mvsQWNjI5599llMmDChw4KM8fHxWLNmDb788kt89NFH8PT0xO23345jx45ZsbedN2rUKKxduxbff/89XnnlFeTl5eHuu++GVqtt9zWOug/ff/99+Pr64oEHHuiwnb3uw7aOC/b6c8hq6WZw5TdlIUSH357bat/W4/Zm+vTp2L9/P3766acO2/Xp0wd9+vSR7qtUKpSXl2Pp0qW48847Ld1Nk4waNUr6+4ABA6BSqdCrVy+8//77yMzMbPM1jrr/AGDlypUYNWoUlEplu20caf+1x9Sfya6+xpb0ej3S09NhMBiwYsWKDtsOHTrUaNLv7bffjsGDB2P58uV4/fXXLd1Vkz388MPS3xMSEpCUlITo6Ghs2rSpw2DgaPsQAFatWoVHHnnkmnNx7HUfdnRcsLefQ47wdENwcDBcXV2vSp/V1dVXpdRWYWFhbbZ3c3NDUFCQxfraXf/v//0/fPnll9i2bRsiIiJMfv3QoUNt/k2kM3x8fDBgwIB2++qo+w8ASktL8d133+GJJ54w+bWOsv9aV9iZ8jPZ+jpTX2NLer0eDz30EIqLi7Fly5YOR3fa4uLigiFDhjjEPgVaRh2jo6M77K+j7UMA+PHHH3HkyJEu/Uzawz5s77hgrz+HDDzd4OHhgcTERGnVS6stW7Zg2LBhbb5GpVJd1T4nJwdJSUlwd3e3WF+7SgiB6dOnY/369fj+++8RGxvbpfcpKChAeHi4mXtnflqtFocOHWq3r462/y63evVq9OjRA2PGjDH5tY6y/2JjYxEWFma0j3Q6HXbs2NHuzyTQ/n7t6DW20hp2jh07hu+++65LQVsIgX379jnEPgWAs2fPory8vMP+OtI+bLVy5UokJibilltuMfm1ttyH1zou2O3PoVmmPl/HPv74Y+Hu7i5WrlwpDh48KJ5++mnh4+MjSkpKhBBCPPvss2LixIlS+5MnTwpvb28xc+ZMcfDgQbFy5Urh7u4u/vvf/9pqEzr017/+VSgUCrF9+3ZRWVkp3RoaGqQ2V27jq6++KjZs2CCOHj0qioqKxLPPPisAiHXr1tliEzo0a9YssX37dnHy5Emxa9cuce+99wpfX1+n2X+tmpubRVRUlJgzZ85Vzzna/jt//rwoKCgQBQUFAoBYtmyZKCgokFYpvfjii0KhUIj169eLwsJC8ec//1mEh4cLjUYjvcfEiRONVlLu3LlTuLq6ihdffFEcOnRIvPjii8LNzU3s2rXLrrZPr9eLsWPHioiICLFv3z6jn0mtVtvu9i1YsEB888034sSJE6KgoED85S9/EW5ubmL37t1W3z4hOt7G8+fPi1mzZomff/5ZFBcXi23btgmVSiV69uzpFPuwlVqtFt7e3uKtt95q8z3seR925rhgjz+HDDxm8Oabb4ro6Gjh4eEhBg8ebLRke9KkSSI5Odmo/fbt28WgQYOEh4eHiImJafc/vD0A0OZt9erVUpsrt/Ff//qX6NWrl/D09BQBAQHijjvuEJs2bbJ+5zvh4YcfFuHh4cLd3V0olUrxwAMPiAMHDkjPO/r+a/Xtt98KAOLIkSNXPedo+6912fyVt0mTJgkhWpbEzp8/X4SFhQm5XC7uvPNOUVhYaPQeycnJUvtWn332mejTp49wd3cX8fHxNgt4HW1fcXFxuz+T27Ztk97jyu17+umnRVRUlPDw8BAhISEiNTVV/Pzzz9bfuEs62saGhgaRmpoqQkJChLu7u4iKihKTJk0SZWVlRu/hqPuw1dtvvy28vLxEXV1dm+9hz/uwM8cFe/w5lF3qPBEREZHT4hweIiIicnoMPEREROT0GHiIiIjI6THwEBERkdNj4CEiIiKnx8BDRERETo+Bh4iIiJweAw8RERE5PQYeIiIicnoMPEREROT0GHiIiIjI6THwEBERkdP7/2Q/Xa65qKgVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(td3.trainRewardsList)\n",
    "plt.show()\n",
    "plt.plot(td3.evalRewardsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j0KFwwh8cB47",
    "outputId": "3eec5f7d-265b-474b-cbad-ba84b458675e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "-599.5834948643387\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"mypendu1\", render_mode=\"human\",start = [-np.pi/4,0.01],end =0,g=g)\n",
    "td3.huvaluate(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDvuXJjVdA3k"
   },
   "outputs": [],
   "source": [
    "# torch.save({\"plotlist\":plotlist},'lists_assn4_hc.pt')\n",
    "# print(plotlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U7URSws5c_AF"
   },
   "outputs": [],
   "source": [
    "plotlist =torch.load('/content/lists_assn4_hc.pt')['plotlist']\n",
    "plotQuantity(plotlist,[\"trainRewardsList\", \"trainTimeList\", \"evalRewardsList\", \"wallClockTimeList\",\"steps\"],-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJW5pPTSe69d"
   },
   "source": [
    "# PPO\n",
    "<a id=\"PPO\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFUa3zm4e69d"
   },
   "source": [
    "PPO have quite a few key implementation details.\n",
    "Please Refer:\n",
    "\"Proximal Policy Optimization Algorithms\" [PPO](https://arxiv.org/abs/1707.06347) and\n",
    "\"Implementation Matters in Deep RL: A Case Study on PPO and TRPO\" [Implementation Matters](https://openreview.net/forum?id=r1etN1rtPB)\n",
    "\n",
    "Lets finish things off with an easy implementation of PPO!\n",
    "A easy way to check you implementation details is running your implementation on some easier environment first and make sure it converges. Like \"CartPole-v1\" should converge to episodic return of 500 in around 300k steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4z6qgI8Ae69d"
   },
   "outputs": [],
   "source": [
    "#All imports here\n",
    "## Feel free to add or remove\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFhNg_COe69e"
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "gym_id = \"CartPole-v1\"  #The id of the gym environment\n",
    "learning_rate = 0.001\n",
    "seed = 1\n",
    "total_timesteps =  #The total timesteps of the experiments\n",
    "torch_deterministic = True   #If toggled, `torch.backends.cudnn.deterministic=False\n",
    "cuda = True\n",
    "\n",
    "num_envs = 4  #The number of parallel game environments (Yes PPO works with vectorized environments)\n",
    "num_steps = 128 #The number of steps to run in each environment per policy rollout\n",
    "anneal_lr = True #Toggle learning rate annealing for policy and value networks\n",
    "gae = True #Use GAE for advantage computation\n",
    "gamma = 0.99\n",
    "gae_lambda =  #The lambda for the general advantage estimation\n",
    "num_minibatches = 4\n",
    "update_epochs =  #The K epochs to update the policy\n",
    "norm_adv = True  #Toggles advantages normalization\n",
    "clip_coef =  #The surrogate clipping coefficient (See what is recommended in the paper!)\n",
    "clip_vloss = True #Toggles whether or not to use a clipped loss for the value function, as per the paper\n",
    "ent_coef =  #Coefficient of the entropy\n",
    "vf_coef =  #Coefficient of the value function\n",
    "max_grad_norm = 0.5\n",
    "target_kl = None #The target KL divergence threshold\n",
    "\n",
    "\n",
    "batch_size = int(num_envs * num_steps)\n",
    "minibatch_size = int(batch_size // num_minibatches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPMUwJrde69e"
   },
   "outputs": [],
   "source": [
    "#PPO works with vectorized enviromnets lets make a function that returns a function that returns an environment.\n",
    "#Refer how to make vectorized environments in gymnasium\n",
    "def make_env(gym_id, seed):\n",
    "    return gym.vector.make(gym_id, num_envs=num_envs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3g6rcqEme69e"
   },
   "outputs": [],
   "source": [
    "#We initialize the layers in PPO , refer paper.\n",
    "#Lets initialize the layers with this function\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    #Initializes the weights and bias of the layers\n",
    "\n",
    "    return layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PK-K0mqye69e"
   },
   "outputs": [],
   "source": [
    "#Lets make the Main agent class\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super(Agent, self).__init__()\n",
    "        hidden_dim = 64\n",
    "        state_dim = envs.\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        #(Returns a single value of the observation)\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        #(Returns the logits of the actions on the observations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4FHhTvNe69e"
   },
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "        def get_value(self, x):\n",
    "            # Returns the value from the critic on the observation x\n",
    "            return self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEw08wFge69f"
   },
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        #Returns 1.the action (sampled according to the logits),\n",
    "        #2.log_prob of the action,\n",
    "        #3.Entropy,\n",
    "        #4.Value from the critic\n",
    "\n",
    "        #Your code here\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3fgwWyae69f"
   },
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and cuda else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJwQhgqke69f"
   },
   "outputs": [],
   "source": [
    "#Make the vectorized environments, use the helper function that we have declared above\n",
    "envs = make_env(gym_id,seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PG-qzko_e69f"
   },
   "outputs": [],
   "source": [
    "agent = Agent(envs).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=learning_rate, eps=) #eps is not the default that pytorch uses\n",
    "\n",
    "# ALGO Logic: Storage setup\n",
    "obs = torch.zeros((num_steps, num_envs) + envs.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((num_steps, num_envs) + envs.single_action_space.shape).to(device)\n",
    "logprobs = torch.zeros((num_steps, num_envs)).to(device)\n",
    "rewards = torch.zeros((num_steps, num_envs)).to(device)\n",
    "dones = torch.zeros((num_steps, num_envs)).to(device)\n",
    "values = torch.zeros((num_steps, num_envs)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-v7ODD-Xe69f"
   },
   "outputs": [],
   "source": [
    "# Start the game\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs, info = envs.reset()\n",
    "next_obs = torch.Tensor(next_obs).to(device)\n",
    "next_done = torch.zeros(num_envs).to(device)\n",
    "num_updates = total_timesteps // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "876dsOOie69f"
   },
   "outputs": [],
   "source": [
    "#This is the main training loop where we collect the experience ,\n",
    "#calculate the advantages, ratio , the total loss and learn the policy\n",
    "\n",
    "for update in range(1, num_updates + 1):\n",
    "\n",
    "    # Annealing the rate if instructed to do so.\n",
    "    if anneal_lr:\n",
    "        # Your code here\n",
    "        pass\n",
    "\n",
    "    for step in range(0, num_steps):\n",
    "        global_step += 1 * num_envs  # We are taking a step in each environment\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "\n",
    "        # ALGO LOGIC: action logic\n",
    "        with torch.no_grad():\n",
    "            #Get the action , logprob , _ , value from the agent.\n",
    "\n",
    "            action, logprob, _, value = # Your code here\n",
    "\n",
    "            values[step] = value.flatten()\n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob\n",
    "\n",
    "        # TRY NOT TO MODIFY: execute the game and log data.\n",
    "        next_obs, reward, done,truncated, info = envs.step(action.cpu().numpy())\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "\n",
    "        for item in info:\n",
    "            if item == \"final_info\" and info[item][0] is not None:\n",
    "                print(f\"global_step={global_step}, episodic_return={info[item][0]['episode']['r']}\")\n",
    "                break\n",
    "\n",
    "    # bootstrap value if not done\n",
    "    with torch.no_grad():\n",
    "        next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "        if gae:\n",
    "            pass\n",
    "            # Your code here\n",
    "\n",
    "            #returns = advantages + values  (yes official implementation of ppo calculates it this way)\n",
    "        else:\n",
    "\n",
    "            # Your code here\n",
    "            pass\n",
    "\n",
    "            #advantages = returns - values\n",
    "\n",
    "    # flatten the batch\n",
    "    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "    b_advantages = advantages.reshape(-1)\n",
    "    b_returns = returns.reshape(-1)\n",
    "    b_values = values.reshape(-1)\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_inds = np.arange(batch_size)\n",
    "    clipfracs = []\n",
    "    for epoch in range(update_epochs):\n",
    "        #Get a random sample of batch_size\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, batch_size, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "\n",
    "            #Your code here\n",
    "            #Calculate the ratio\n",
    "            _, newlogprob, entropy, newvalue =\n",
    "            logratio =\n",
    "            ratio =\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                # Refer the blog for calculating kl in a simpler way\n",
    "                old_approx_kl =\n",
    "                approx_kl =\n",
    "                clipfracs += [((ratio - 1.0).abs() > clip_coef).float().mean().item()]\n",
    "\n",
    "            mb_advantages = b_advantages[mb_inds]\n",
    "            if norm_adv:\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "            # Policy loss (Calculate the policy loss pg_loss)\n",
    "            # Your code here\n",
    "\n",
    "\n",
    "            # Value loss v_loss\n",
    "            newvalue = newvalue.view(-1)\n",
    "            if clip_vloss:\n",
    "                pass\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            # Entropy loss\n",
    "            entropy_loss =\n",
    "\n",
    "            # Total loss\n",
    "            loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "        if target_kl is not None:\n",
    "            if approx_kl > target_kl:\n",
    "                break\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDkCANf7e69g"
   },
   "source": [
    "# Experiments and Plots\n",
    "<a id=\"experiments\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meaDUggze69g"
   },
   "source": [
    "Run the DDPG, TD3, PPO on Pendulum, Hopper and Half Cheetah environment respectively.\n",
    "\n",
    "Plot the following for each of the environment separately. Note based on different hyper-parameters and strategies you use, you can have multiple plots for each of the below.\n",
    "\n",
    "As you are aware from your past experience, single run of the agent over the environment results in plots that have lot of variance and look very noisy. One way to overcome this is to create several different instances of the environment using different seeds and then average out the results across these and plot these. For all the plots below, you this strategy. You need to run 5 different instances of the environment for each agent. As you have seen in the lecture slides, we plot the maximum and minimum values around the mean in the plots, so this gives us the shaded plot with the mean curve in the between. In this assignment, you are required to do the same. Generate plots with envelop between maximum and minimum value\n",
    "For each of the quantity of interest, plot each of the agent within the same plot using different colors for the envelop. Choose colors such that that there is clear contrast between the plots corresponding to different agents.\n",
    "\n",
    "1. Plot mean train rewards vs episodes\n",
    "2. Plot mean evaluation rewards vs episodes\n",
    "3. Plot total steps vs episode\n",
    "4. Plot train time vs episode\n",
    "5. Plot wall clock time vs episode\n",
    "6. Based on plots what are your observations about DDPG and TD3, compare the two algorithms.\n",
    "7. What is the advatage of PPO over DDPG or TD3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rk5blvaktse"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "def runDeepAgents(env):\n",
    "    # this function will initialize 5 different instances of the env (using different seeds), run all the agents\n",
    "    # over these different instances. Collects results and generate the plots state above.\n",
    "    # generate your plots in the cells below\n",
    "    # write the answers to part 11, 12 and 13 in the cells below the plot-cells.\n",
    "    lllist = []\n",
    "    # env = gym.make(\"Hopper-v4\")\n",
    "    # env = gym.make(\"Pendulum-v1\")\n",
    "    # env = gym.make(\"HalfCheetah-v4\")\n",
    "    tep=500\n",
    "    llist = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        agent = DDPG(env,seed,0.99,0.005,100000,128,[256,128],1,'adam','adam',0.001,0.001,20,1,_)\n",
    "        infolist = agent.runDDPG()\n",
    "        llist.append(infolist)\n",
    "    lllist.append(llist)\n",
    "    llist = []\n",
    "    for seed in seeds:\n",
    "        agent = TD3(env,seed,0.99,0.005,100000,128,[256,128],1,1,2,'adam','adam',0.001,0.001,20,1,_)\n",
    "        infolist = agent.runTD3()\n",
    "        llist.append(infolist)\n",
    "    lllist.append(llist)\n",
    "    return lllist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fz0Myqa5leA0"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Hopper-v4\")\n",
    "plotlist = runDeepAgents(env)\n",
    "plotQuantity(plotlist,[\"trainRewardsList\", \"trainTimeList\", \"evalRewardsList\", \"wallClockTimeList\",\"steps\"],-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUdir22QlkX8"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "plotlist = runDeepAgents(env)\n",
    "plotQuantity(plotlist,[\"trainRewardsList\", \"trainTimeList\", \"evalRewardsList\", \"wallClockTimeList\",\"steps\"],-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rBmKf-cllszE"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"HalfCheetah-v4\")\n",
    "plotlist = runDeepAgents(env)\n",
    "plotQuantity(plotlist,[\"trainRewardsList\", \"trainTimeList\", \"evalRewardsList\", \"wallClockTimeList\",\"steps\"],-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwUl9NYdlHY6"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q6\n",
    "Based on the above plots, we can see that TD3 performs slilghtly better in terms of faster convergence, less variance and better rewards for most of the environments.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJQyHty6m7dV"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PPO algorithm can be run with vectorized environments which means it can make use of resources like GPUs and it significantly decreases the training time for the algorithm.\n",
    "Although I wasn't able to code the PPO algorithm properlt due to many errors, I have tried it using code from ChatGPT.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UzKBeTXTn7NL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Tanh()  # For bounded continuous actions\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        action_mean = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        return action_mean, value\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99, clip_ratio=0.2, value_coeff=0.5, entropy_coeff=0.01):\n",
    "        self.actor_critic = ActorCritic(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.value_coeff = value_coeff\n",
    "        self.entropy_coeff = entropy_coeff\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action_mean, _ = self.actor_critic(state)\n",
    "            action = action_mean.squeeze(0).numpy()\n",
    "        return action\n",
    "\n",
    "    def compute_advantages(self, rewards, dones, values, next_values):\n",
    "        deltas = rewards + self.gamma * next_values * (1 - dones) - values\n",
    "        print(deltas.shape)\n",
    "        advantages = torch.zeros(deltas.shape)\n",
    "        advantage = 0\n",
    "        for t in reversed(range(len(deltas))):\n",
    "            advantage = deltas[t] + self.gamma * advantage * (1 - dones[t])\n",
    "            advantages[t] = advantage.detach()\n",
    "        return advantages\n",
    "\n",
    "    def ppo_loss(self, states, actions, rewards, dones, next_states, old_action_probs, old_values):\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "\n",
    "        # Compute advantages\n",
    "        values = self.actor_critic.critic(states)\n",
    "        next_values = self.actor_critic.critic(next_states)\n",
    "        advantages = self.compute_advantages(rewards, dones, values, next_values)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # Compute critic loss\n",
    "        target_values = rewards + self.gamma * next_values * (1 - dones)\n",
    "        critic_loss = F.mse_loss(values, target_values.detach())\n",
    "\n",
    "        # Compute actor loss\n",
    "        action_mean, _ = self.actor_critic(states)\n",
    "        new_action_probs = self._gaussian_likelihood(actions, action_mean)\n",
    "        old_action_probs = torch.FloatTensor(old_action_probs)\n",
    "        ratios = new_action_probs / (old_action_probs + 1e-8)\n",
    "        print(type(advantages))\n",
    "        surr1 = ratios *advantages\n",
    "        surr2 = torch.clamp(ratios, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages\n",
    "        actor_loss = -torch.mean(torch.min(surr1, surr2))\n",
    "\n",
    "        # Total loss\n",
    "        loss = actor_loss + self.value_coeff * critic_loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train(self, states, actions, rewards, dones, next_states, old_action_probs, old_values):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.ppo_loss(states, actions, rewards, dones, next_states, old_action_probs, old_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def _gaussian_likelihood(self, action, mean):\n",
    "        std = 0.1  # Fixed standard deviation for simplicity\n",
    "        var = std**2\n",
    "        action_dim = action.size(-1)\n",
    "        log_std = torch.log(torch.tensor(std, dtype=torch.float32))\n",
    "        log_prob = -0.5 * (((action - mean) / (var + 1e-8)) ** 2 + 2 * log_std + np.log(2 * np.pi))\n",
    "        return log_prob.sum(dim=-1)\n",
    "\n",
    "# Example usage:\n",
    "# Replace envs with your own environment\n",
    "envs =gym.make(\"Hopper-v4\") # Replace None with your environment\n",
    "\n",
    "# Define hyperparameters\n",
    "state_dim = envs.observation_space.shape[0]\n",
    "action_dim = envs.action_space.shape[0]\n",
    "lr = 0.001\n",
    "gamma = 0.99\n",
    "clip_ratio = 0.2\n",
    "value_coeff = 0.5\n",
    "entropy_coeff = 0.01\n",
    "\n",
    "# Initialize PPO agent\n",
    "agent = PPOAgent(state_dim, action_dim, lr, gamma, clip_ratio, value_coeff, entropy_coeff)\n",
    "\n",
    "# Training loop\n",
    "num_updates = 100  # Number of updates\n",
    "num_steps = 100  # Number of steps per update\n",
    "batch_size = 64  # Batch size for training\n",
    "for update in tqdm(range(num_updates)):\n",
    "    # Lists to store experience\n",
    "    all_states = []\n",
    "    all_actions = []\n",
    "    all_rewards = []\n",
    "    all_dones = []\n",
    "    all_next_states = []\n",
    "    all_old_action_probs = []\n",
    "    all_old_values = []\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        # Collect experience\n",
    "        state,_ = envs.reset()\n",
    "        done = False\n",
    "        trunc = False\n",
    "        rew = 0\n",
    "        while not (done or trunc):\n",
    "            # Get action from the agent\n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            # Take a step in the environment\n",
    "            next_state, reward, done, trunc,_ = envs.step(action)\n",
    "\n",
    "            # Log data\n",
    "            rew+=reward\n",
    "            all_states.append(state)\n",
    "            all_actions.append(action)\n",
    "            all_rewards.append(reward)\n",
    "            all_dones.append(done)\n",
    "            all_next_states.append(next_state)\n",
    "\n",
    "            # Compute old action probabilities and values\n",
    "            with torch.no_grad():\n",
    "                action_mean, _ = agent.actor_critic(torch.FloatTensor(state))\n",
    "                old_action_prob = agent._gaussian_likelihood(torch.FloatTensor(action), action_mean)\n",
    "                all_old_action_probs.append(old_action_prob)\n",
    "                all_old_values.append(agent.actor_critic.critic(torch.FloatTensor(state)).item())\n",
    "\n",
    "            # Update state\n",
    "            state = next_state\n",
    "        print(rew)\n",
    "    # Perform updates\n",
    "    agent.train(all_states, all_actions, all_rewards, all_dones, all_next_states, all_old_action_probs, all_old_values)\n",
    "\n",
    "# Close the environment\n",
    "envs.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Q1g3rGqpNgK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "vu2sPr1he69K",
    "UQLclV2be69M",
    "zDzb1p2ee69N",
    "o4zCW_BJe69N",
    "_jVXlGOye69O"
   ],
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4822293,
     "sourceId": 8153103,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30684,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
